{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神經網路實做"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 拿取加權指數資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "2006-01-02 09:00:00    6548.34\n",
       "2006-01-02 09:15:00    6478.09\n",
       "2006-01-02 09:30:00    6474.88\n",
       "2006-01-02 09:45:00    6471.12\n",
       "2006-01-02 10:00:00    6480.50\n",
       "Name: 台股指數, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finlab.data import Data\n",
    "\n",
    "data = Data()\n",
    "twii = data.get(\"發行量加權股價指數\")\n",
    "\n",
    "twii = twii['台股指數'].resample(\"15T\").first().dropna()\n",
    "twii.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 製作features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import talib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sma = talib.SMA(twii, timeperiod=120)\n",
    "wma = talib.WMA(twii, timeperiod=120)\n",
    "mom = talib.MOM(twii, timeperiod=120)\n",
    "k, d = talib.STOCH  (twii, twii, twii, fastk_period=120, slowk_period=60, slowd_period=60)\n",
    "k2, d2 = talib.STOCH(twii, twii, twii, fastk_period=240, slowk_period=120, slowd_period=120)\n",
    "k3, d3 = talib.STOCH(twii, twii, twii, fastk_period=360, slowk_period=180, slowd_period=180)\n",
    "k4, d4 = talib.STOCH(twii, twii, twii, fastk_period=480, slowk_period=240, slowd_period=240)\n",
    "k5, d5 = talib.STOCH(twii, twii, twii, fastk_period=640, slowk_period=320, slowd_period=320)\n",
    "k6, d6 = talib.STOCH(twii, twii, twii, fastk_period=720, slowk_period=360, slowd_period=360)\n",
    "k7, d7 = talib.STOCH(twii, twii, twii, fastk_period=840, slowk_period=420, slowd_period=420)\n",
    "k8, d8 = talib.STOCH(twii, twii, twii, fastk_period=960, slowk_period=480, slowd_period=480)\n",
    "\n",
    "rsi = talib.RSI (twii, timeperiod=120)\n",
    "rsi2 = talib.RSI(twii, timeperiod=240)\n",
    "rsi3 = talib.RSI(twii, timeperiod=480)\n",
    "rsi4 = talib.RSI(twii, timeperiod=640)\n",
    "rsi5 = talib.RSI(twii, timeperiod=720)\n",
    "rsi6 = talib.RSI(twii, timeperiod=840)\n",
    "\n",
    "macd1, macd2, macd3 = talib.MACD(twii, fastperiod=120, slowperiod=60, signalperiod=60)\n",
    "willr = talib.WILLR(twii, twii, twii, timeperiod=120)\n",
    "cci = talib.CCI(twii, twii, twii, timeperiod=120)\n",
    "\n",
    "dataset = pd.DataFrame({\n",
    "    'RSIb': rsi / 50,\n",
    "    'RSIb2': rsi2 / 50,\n",
    "    'RSIb3': rsi3 / 50,\n",
    "    'RSIb4': rsi4 / 50,\n",
    "    'RSIb5': rsi5 / 50,\n",
    "    'RSIb6': rsi6 / 50,\n",
    "    'MOMb': mom - 0,\n",
    "    'KDb': k - d,\n",
    "    'KDb2': k2 - d2,\n",
    "    'KDb3': k3 - d3,\n",
    "    'KDb4': k4 - d4,\n",
    "    'KDb5': k5 - d5,\n",
    "    'KDb6': k6 - d6,\n",
    "    'KDb7': k7 - d7,\n",
    "    'KDb8': k8 - d8,\n",
    "    \n",
    "    'a5':   (twii.rolling(5).mean()   / twii),\n",
    "    'a10':  (twii.rolling(10).mean()  / twii),\n",
    "    'a20':  (twii.rolling(20).mean()  / twii),\n",
    "    'a40':  (twii.rolling(40).mean()  / twii),\n",
    "    'a80':  (twii.rolling(80).mean()  / twii),\n",
    "    'a160': (twii.rolling(160).mean() / twii),\n",
    "    'a320': (twii.rolling(320).mean() / twii),\n",
    "    'a640': (twii.rolling(640).mean() / twii),\n",
    "    'a720': (twii.rolling(720).mean() / twii),\n",
    "    'a840': (twii.rolling(840).mean() / twii),\n",
    "    'a960': (twii.rolling(960).mean() / twii),\n",
    "    'a1024':(twii.rolling(1024).mean() / twii),\n",
    "    'b1': twii/twii.shift(50),\n",
    "    'b2': twii/twii.shift(100),\n",
    "    'b3': twii/twii.shift(150),\n",
    "    'b4': twii/twii.shift(200),\n",
    "    'b5': twii/twii.shift(250),\n",
    "    'b6': twii/twii.shift(300),\n",
    "    'b7': twii/twii.shift(350),\n",
    "    'LINEARREG_SLOPE0': talib.LINEARREG_SLOPE(twii, 60),\n",
    "    'LINEARREG_SLOPE1': talib.LINEARREG_SLOPE(twii, 120),\n",
    "\n",
    "    'ADXR0': talib.ADXR(twii, twii, twii, 60),\n",
    "    'ADXR1': talib.ADXR(twii, twii, twii, 120),\n",
    "    'ADXR2': talib.ADXR(twii, twii, twii, 240),\n",
    "    'ADXR3': talib.ADXR(twii, twii, twii, 360),\n",
    "    'ADXR4': talib.ADXR(twii, twii, twii, 480),\n",
    "    'ADXR5': talib.ADXR(twii, twii, twii, 640),\n",
    "\n",
    "    'return': twii.shift(-10) / twii,\n",
    "})\n",
    "\n",
    "feature_names = list(dataset.columns[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 簡單處理一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before dropping NaN (79078, 43)\n",
      "after dropping NaN (77150, 43)\n"
     ]
    }
   ],
   "source": [
    "print(\"before dropping NaN\", dataset.shape)\n",
    "dataset = dataset.dropna()\n",
    "print(\"after dropping NaN\", dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(learning_rate=0.01, random_state=5)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "dataset_train = dataset[:'2020']\n",
    "\n",
    "gbm = lgb.LGBMClassifier(n_estimators=100, random_state=5, learning_rate=0.01)\n",
    "\n",
    "gbm.fit(dataset_train[feature_names], dataset_train['return'] > 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神經網路Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RSIb</th>\n",
       "      <th>RSIb2</th>\n",
       "      <th>RSIb3</th>\n",
       "      <th>RSIb4</th>\n",
       "      <th>RSIb5</th>\n",
       "      <th>RSIb6</th>\n",
       "      <th>MOMb</th>\n",
       "      <th>KDb</th>\n",
       "      <th>KDb2</th>\n",
       "      <th>KDb3</th>\n",
       "      <th>...</th>\n",
       "      <th>b7</th>\n",
       "      <th>LINEARREG_SLOPE0</th>\n",
       "      <th>LINEARREG_SLOPE1</th>\n",
       "      <th>ADXR0</th>\n",
       "      <th>ADXR1</th>\n",
       "      <th>ADXR2</th>\n",
       "      <th>ADXR3</th>\n",
       "      <th>ADXR4</th>\n",
       "      <th>ADXR5</th>\n",
       "      <th>return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7.715000e+04</td>\n",
       "      <td>7.715000e+04</td>\n",
       "      <td>7.715000e+04</td>\n",
       "      <td>7.715000e+04</td>\n",
       "      <td>7.715000e+04</td>\n",
       "      <td>7.715000e+04</td>\n",
       "      <td>7.715000e+04</td>\n",
       "      <td>7.715000e+04</td>\n",
       "      <td>7.715000e+04</td>\n",
       "      <td>7.715000e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>7.715000e+04</td>\n",
       "      <td>7.715000e+04</td>\n",
       "      <td>7.715000e+04</td>\n",
       "      <td>7.715000e+04</td>\n",
       "      <td>7.715000e+04</td>\n",
       "      <td>7.715000e+04</td>\n",
       "      <td>7.715000e+04</td>\n",
       "      <td>7.715000e+04</td>\n",
       "      <td>7.715000e+04</td>\n",
       "      <td>77150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.062826e-16</td>\n",
       "      <td>1.479787e-15</td>\n",
       "      <td>-3.589392e-15</td>\n",
       "      <td>-1.782172e-15</td>\n",
       "      <td>-3.785165e-15</td>\n",
       "      <td>-2.730420e-15</td>\n",
       "      <td>2.518904e-17</td>\n",
       "      <td>2.170367e-17</td>\n",
       "      <td>1.182175e-17</td>\n",
       "      <td>-3.735760e-18</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.329057e-15</td>\n",
       "      <td>-1.733646e-16</td>\n",
       "      <td>-5.333675e-17</td>\n",
       "      <td>-2.673227e-16</td>\n",
       "      <td>-3.543561e-16</td>\n",
       "      <td>-3.209962e-16</td>\n",
       "      <td>-7.830130e-17</td>\n",
       "      <td>3.978700e-16</td>\n",
       "      <td>-3.863490e-16</td>\n",
       "      <td>1.000135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>1.000006e+00</td>\n",
       "      <td>0.008257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.368989e+00</td>\n",
       "      <td>-4.506759e+00</td>\n",
       "      <td>-4.762963e+00</td>\n",
       "      <td>-4.738700e+00</td>\n",
       "      <td>-4.702210e+00</td>\n",
       "      <td>-4.630653e+00</td>\n",
       "      <td>-8.883580e+00</td>\n",
       "      <td>-2.586458e+00</td>\n",
       "      <td>-2.685999e+00</td>\n",
       "      <td>-2.663557e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.480993e+00</td>\n",
       "      <td>-9.322652e+00</td>\n",
       "      <td>-7.097892e+00</td>\n",
       "      <td>-2.105435e+00</td>\n",
       "      <td>-2.011292e+00</td>\n",
       "      <td>-1.696597e+00</td>\n",
       "      <td>-1.612772e+00</td>\n",
       "      <td>-1.755444e+00</td>\n",
       "      <td>-1.801731e+00</td>\n",
       "      <td>0.924507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.821675e-01</td>\n",
       "      <td>-6.833395e-01</td>\n",
       "      <td>-6.700367e-01</td>\n",
       "      <td>-6.656202e-01</td>\n",
       "      <td>-6.610806e-01</td>\n",
       "      <td>-6.519681e-01</td>\n",
       "      <td>-4.595441e-01</td>\n",
       "      <td>-6.391872e-01</td>\n",
       "      <td>-6.442800e-01</td>\n",
       "      <td>-6.282222e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.310826e-01</td>\n",
       "      <td>-4.636567e-01</td>\n",
       "      <td>-4.686256e-01</td>\n",
       "      <td>-7.436176e-01</td>\n",
       "      <td>-6.998969e-01</td>\n",
       "      <td>-7.519128e-01</td>\n",
       "      <td>-7.023951e-01</td>\n",
       "      <td>-6.870651e-01</td>\n",
       "      <td>-7.213463e-01</td>\n",
       "      <td>0.996989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.697027e-02</td>\n",
       "      <td>1.165786e-01</td>\n",
       "      <td>1.156960e-01</td>\n",
       "      <td>1.205676e-01</td>\n",
       "      <td>1.261177e-01</td>\n",
       "      <td>1.343380e-01</td>\n",
       "      <td>9.098792e-02</td>\n",
       "      <td>-1.326469e-02</td>\n",
       "      <td>-3.171791e-02</td>\n",
       "      <td>-7.717300e-03</td>\n",
       "      <td>...</td>\n",
       "      <td>1.203265e-01</td>\n",
       "      <td>7.747991e-02</td>\n",
       "      <td>8.381842e-02</td>\n",
       "      <td>-1.503141e-01</td>\n",
       "      <td>-2.273822e-01</td>\n",
       "      <td>-2.738363e-01</td>\n",
       "      <td>-2.538710e-01</td>\n",
       "      <td>-2.312040e-01</td>\n",
       "      <td>-2.118342e-01</td>\n",
       "      <td>1.000285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.223377e-01</td>\n",
       "      <td>7.145231e-01</td>\n",
       "      <td>6.887814e-01</td>\n",
       "      <td>6.946299e-01</td>\n",
       "      <td>6.937432e-01</td>\n",
       "      <td>6.916076e-01</td>\n",
       "      <td>5.701869e-01</td>\n",
       "      <td>6.560760e-01</td>\n",
       "      <td>6.728323e-01</td>\n",
       "      <td>6.015924e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>5.829542e-01</td>\n",
       "      <td>5.610455e-01</td>\n",
       "      <td>5.725418e-01</td>\n",
       "      <td>5.911812e-01</td>\n",
       "      <td>4.673996e-01</td>\n",
       "      <td>5.046496e-01</td>\n",
       "      <td>4.980228e-01</td>\n",
       "      <td>5.069936e-01</td>\n",
       "      <td>5.906690e-01</td>\n",
       "      <td>1.003707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.246771e+00</td>\n",
       "      <td>2.815948e+00</td>\n",
       "      <td>3.017904e+00</td>\n",
       "      <td>3.089250e+00</td>\n",
       "      <td>3.105732e+00</td>\n",
       "      <td>3.115908e+00</td>\n",
       "      <td>5.314910e+00</td>\n",
       "      <td>2.726409e+00</td>\n",
       "      <td>2.592381e+00</td>\n",
       "      <td>2.559921e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>4.869922e+00</td>\n",
       "      <td>4.700631e+00</td>\n",
       "      <td>4.095958e+00</td>\n",
       "      <td>4.476193e+00</td>\n",
       "      <td>4.063373e+00</td>\n",
       "      <td>3.820162e+00</td>\n",
       "      <td>3.563766e+00</td>\n",
       "      <td>3.680746e+00</td>\n",
       "      <td>3.523598e+00</td>\n",
       "      <td>1.087726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               RSIb         RSIb2         RSIb3         RSIb4         RSIb5  \\\n",
       "count  7.715000e+04  7.715000e+04  7.715000e+04  7.715000e+04  7.715000e+04   \n",
       "mean   4.062826e-16  1.479787e-15 -3.589392e-15 -1.782172e-15 -3.785165e-15   \n",
       "std    1.000006e+00  1.000006e+00  1.000006e+00  1.000006e+00  1.000006e+00   \n",
       "min   -4.368989e+00 -4.506759e+00 -4.762963e+00 -4.738700e+00 -4.702210e+00   \n",
       "25%   -6.821675e-01 -6.833395e-01 -6.700367e-01 -6.656202e-01 -6.610806e-01   \n",
       "50%    9.697027e-02  1.165786e-01  1.156960e-01  1.205676e-01  1.261177e-01   \n",
       "75%    7.223377e-01  7.145231e-01  6.887814e-01  6.946299e-01  6.937432e-01   \n",
       "max    3.246771e+00  2.815948e+00  3.017904e+00  3.089250e+00  3.105732e+00   \n",
       "\n",
       "              RSIb6          MOMb           KDb          KDb2          KDb3  \\\n",
       "count  7.715000e+04  7.715000e+04  7.715000e+04  7.715000e+04  7.715000e+04   \n",
       "mean  -2.730420e-15  2.518904e-17  2.170367e-17  1.182175e-17 -3.735760e-18   \n",
       "std    1.000006e+00  1.000006e+00  1.000006e+00  1.000006e+00  1.000006e+00   \n",
       "min   -4.630653e+00 -8.883580e+00 -2.586458e+00 -2.685999e+00 -2.663557e+00   \n",
       "25%   -6.519681e-01 -4.595441e-01 -6.391872e-01 -6.442800e-01 -6.282222e-01   \n",
       "50%    1.343380e-01  9.098792e-02 -1.326469e-02 -3.171791e-02 -7.717300e-03   \n",
       "75%    6.916076e-01  5.701869e-01  6.560760e-01  6.728323e-01  6.015924e-01   \n",
       "max    3.115908e+00  5.314910e+00  2.726409e+00  2.592381e+00  2.559921e+00   \n",
       "\n",
       "       ...            b7  LINEARREG_SLOPE0  LINEARREG_SLOPE1         ADXR0  \\\n",
       "count  ...  7.715000e+04      7.715000e+04      7.715000e+04  7.715000e+04   \n",
       "mean   ... -3.329057e-15     -1.733646e-16     -5.333675e-17 -2.673227e-16   \n",
       "std    ...  1.000006e+00      1.000006e+00      1.000006e+00  1.000006e+00   \n",
       "min    ... -5.480993e+00     -9.322652e+00     -7.097892e+00 -2.105435e+00   \n",
       "25%    ... -5.310826e-01     -4.636567e-01     -4.686256e-01 -7.436176e-01   \n",
       "50%    ...  1.203265e-01      7.747991e-02      8.381842e-02 -1.503141e-01   \n",
       "75%    ...  5.829542e-01      5.610455e-01      5.725418e-01  5.911812e-01   \n",
       "max    ...  4.869922e+00      4.700631e+00      4.095958e+00  4.476193e+00   \n",
       "\n",
       "              ADXR1         ADXR2         ADXR3         ADXR4         ADXR5  \\\n",
       "count  7.715000e+04  7.715000e+04  7.715000e+04  7.715000e+04  7.715000e+04   \n",
       "mean  -3.543561e-16 -3.209962e-16 -7.830130e-17  3.978700e-16 -3.863490e-16   \n",
       "std    1.000006e+00  1.000006e+00  1.000006e+00  1.000006e+00  1.000006e+00   \n",
       "min   -2.011292e+00 -1.696597e+00 -1.612772e+00 -1.755444e+00 -1.801731e+00   \n",
       "25%   -6.998969e-01 -7.519128e-01 -7.023951e-01 -6.870651e-01 -7.213463e-01   \n",
       "50%   -2.273822e-01 -2.738363e-01 -2.538710e-01 -2.312040e-01 -2.118342e-01   \n",
       "75%    4.673996e-01  5.046496e-01  4.980228e-01  5.069936e-01  5.906690e-01   \n",
       "max    4.063373e+00  3.820162e+00  3.563766e+00  3.680746e+00  3.523598e+00   \n",
       "\n",
       "             return  \n",
       "count  77150.000000  \n",
       "mean       1.000135  \n",
       "std        0.008257  \n",
       "min        0.924507  \n",
       "25%        0.996989  \n",
       "50%        1.000285  \n",
       "75%        1.003707  \n",
       "max        1.087726  \n",
       "\n",
       "[8 rows x 43 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "dataset_scaled = ss.fit_transform(dataset)\n",
    "dataset_scaled = pd.DataFrame(dataset_scaled, columns=dataset.columns, index=dataset.index)\n",
    "dataset_scaled['return'] = dataset['return']\n",
    "dataset_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "n = 3\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "indexes = []\n",
    "dataset_scaled_x = dataset_scaled[feature_names]\n",
    "\n",
    "for i in tqdm.tqdm_notebook(range(0, len(dataset_scaled)-n)):\n",
    "    X.append(dataset_scaled_x.iloc[i:i+n].values)\n",
    "    y.append(dataset_scaled['return'].iloc[i+n-1])\n",
    "    indexes.append(dataset_scaled.index[i+n-1])\n",
    "#dataset_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Timestamp('2006-06-06 09:15:00'), Timestamp('2006-06-06 09:30:00'),\n",
       "       Timestamp('2006-06-06 09:45:00'), ...,\n",
       "       Timestamp('2022-11-15 10:15:00'), Timestamp('2022-11-15 10:30:00'),\n",
       "       Timestamp('2022-11-15 10:45:00')], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes = np.array(indexes)\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2006-06-05 13:30:00', '2006-06-06 09:00:00',\n",
       "               '2006-06-06 09:15:00', '2006-06-06 09:30:00',\n",
       "               '2006-06-06 09:45:00', '2006-06-06 10:00:00',\n",
       "               '2006-06-06 10:15:00', '2006-06-06 10:30:00',\n",
       "               '2006-06-06 10:45:00', '2006-06-06 11:00:00',\n",
       "               ...\n",
       "               '2022-11-14 13:30:00', '2022-11-15 09:00:00',\n",
       "               '2022-11-15 09:15:00', '2022-11-15 09:30:00',\n",
       "               '2022-11-15 09:45:00', '2022-11-15 10:00:00',\n",
       "               '2022-11-15 10:15:00', '2022-11-15 10:30:00',\n",
       "               '2022-11-15 10:45:00', '2022-11-15 11:00:00'],\n",
       "              dtype='datetime64[ns]', name='date', length=77150, freq=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_scaled.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神經網路 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 3, 100)            57200     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 808       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 138,417\n",
      "Trainable params: 138,417\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "#model.add(keras.layers.Dense(100, activation=\"relu\", input_shape=(len(feature_names),)))\n",
    "model.add(layers.LSTM(100, return_sequences=True, input_shape=X[0].shape))\n",
    "model.add(layers.LSTM(100))\n",
    "model.add(layers.Dense(8))\n",
    "model.add(layers.Dense(1,kernel_initializer=\"uniform\",activation='linear'))\n",
    "\n",
    "adam = keras.optimizers.Adam(0.0006)\n",
    "\n",
    "model.compile(optimizer=adam, loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神經網路訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Timestamp('2006-06-06 09:15:00'), Timestamp('2006-06-06 09:30:00'),\n",
       "       Timestamp('2006-06-06 09:45:00'), ...,\n",
       "       Timestamp('2022-11-15 10:15:00'), Timestamp('2022-11-15 10:30:00'),\n",
       "       Timestamp('2022-11-15 10:45:00')], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "11/11 [==============================] - 4s 349ms/step - loss: 3.6262 - accuracy: 0.4787 - val_loss: 2.8150 - val_accuracy: 0.4543\n",
      "Epoch 2/300\n",
      "11/11 [==============================] - 3s 295ms/step - loss: 1.9158 - accuracy: 0.4787 - val_loss: 1.9351 - val_accuracy: 0.4543\n",
      "Epoch 3/300\n",
      "11/11 [==============================] - 3s 278ms/step - loss: 1.5633 - accuracy: 0.4787 - val_loss: 1.6003 - val_accuracy: 0.4543\n",
      "Epoch 4/300\n",
      "11/11 [==============================] - 3s 282ms/step - loss: 1.3227 - accuracy: 0.4787 - val_loss: 1.3383 - val_accuracy: 0.4543\n",
      "Epoch 5/300\n",
      "11/11 [==============================] - 4s 321ms/step - loss: 1.1098 - accuracy: 0.4787 - val_loss: 1.1165 - val_accuracy: 0.4543\n",
      "Epoch 6/300\n",
      "11/11 [==============================] - 3s 272ms/step - loss: 0.9303 - accuracy: 0.4819 - val_loss: 0.9410 - val_accuracy: 0.4628\n",
      "Epoch 7/300\n",
      "11/11 [==============================] - 5s 431ms/step - loss: 0.8148 - accuracy: 0.4924 - val_loss: 0.8422 - val_accuracy: 0.4647\n",
      "Epoch 8/300\n",
      "11/11 [==============================] - 3s 288ms/step - loss: 0.7792 - accuracy: 0.4912 - val_loss: 0.8222 - val_accuracy: 0.4749\n",
      "Epoch 9/300\n",
      "11/11 [==============================] - 3s 271ms/step - loss: 0.7528 - accuracy: 0.4947 - val_loss: 0.8061 - val_accuracy: 0.4749\n",
      "Epoch 10/300\n",
      "11/11 [==============================] - 3s 293ms/step - loss: 0.7381 - accuracy: 0.5013 - val_loss: 0.7859 - val_accuracy: 0.4719\n",
      "Epoch 11/300\n",
      "11/11 [==============================] - 3s 274ms/step - loss: 0.7273 - accuracy: 0.5087 - val_loss: 0.7714 - val_accuracy: 0.4754\n",
      "Epoch 12/300\n",
      "11/11 [==============================] - 4s 320ms/step - loss: 0.7197 - accuracy: 0.5109 - val_loss: 0.7621 - val_accuracy: 0.4813\n",
      "Epoch 13/300\n",
      "11/11 [==============================] - 3s 277ms/step - loss: 0.7138 - accuracy: 0.5113 - val_loss: 0.7548 - val_accuracy: 0.4778\n",
      "Epoch 14/300\n",
      "11/11 [==============================] - 3s 270ms/step - loss: 0.7093 - accuracy: 0.5113 - val_loss: 0.7482 - val_accuracy: 0.4735\n",
      "Epoch 15/300\n",
      "11/11 [==============================] - 3s 276ms/step - loss: 0.7059 - accuracy: 0.5131 - val_loss: 0.7413 - val_accuracy: 0.4723\n",
      "Epoch 16/300\n",
      "11/11 [==============================] - 4s 333ms/step - loss: 0.7033 - accuracy: 0.5143 - val_loss: 0.7364 - val_accuracy: 0.4742\n",
      "Epoch 17/300\n",
      "11/11 [==============================] - 3s 291ms/step - loss: 0.7012 - accuracy: 0.5149 - val_loss: 0.7318 - val_accuracy: 0.4749\n",
      "Epoch 18/300\n",
      "11/11 [==============================] - 3s 310ms/step - loss: 0.6996 - accuracy: 0.5165 - val_loss: 0.7279 - val_accuracy: 0.4747\n",
      "Epoch 19/300\n",
      "11/11 [==============================] - 3s 283ms/step - loss: 0.6982 - accuracy: 0.5197 - val_loss: 0.7244 - val_accuracy: 0.4754\n",
      "Epoch 20/300\n",
      "11/11 [==============================] - 3s 276ms/step - loss: 0.6971 - accuracy: 0.5224 - val_loss: 0.7216 - val_accuracy: 0.4764\n",
      "Epoch 21/300\n",
      "11/11 [==============================] - 3s 275ms/step - loss: 0.6962 - accuracy: 0.5248 - val_loss: 0.7183 - val_accuracy: 0.4807\n",
      "Epoch 22/300\n",
      "11/11 [==============================] - 3s 275ms/step - loss: 0.6953 - accuracy: 0.5264 - val_loss: 0.7167 - val_accuracy: 0.4777\n",
      "Epoch 23/300\n",
      "11/11 [==============================] - 3s 270ms/step - loss: 0.6945 - accuracy: 0.5267 - val_loss: 0.7149 - val_accuracy: 0.4762\n",
      "Epoch 24/300\n",
      "11/11 [==============================] - 3s 278ms/step - loss: 0.6938 - accuracy: 0.5290 - val_loss: 0.7127 - val_accuracy: 0.4797\n",
      "Epoch 25/300\n",
      "11/11 [==============================] - 3s 293ms/step - loss: 0.6932 - accuracy: 0.5302 - val_loss: 0.7117 - val_accuracy: 0.4778\n",
      "Epoch 26/300\n",
      "11/11 [==============================] - 3s 275ms/step - loss: 0.6926 - accuracy: 0.5304 - val_loss: 0.7105 - val_accuracy: 0.4804\n",
      "Epoch 27/300\n",
      "11/11 [==============================] - 3s 278ms/step - loss: 0.6920 - accuracy: 0.5319 - val_loss: 0.7090 - val_accuracy: 0.4811\n",
      "Epoch 28/300\n",
      "11/11 [==============================] - 3s 280ms/step - loss: 0.6915 - accuracy: 0.5325 - val_loss: 0.7084 - val_accuracy: 0.4821\n",
      "Epoch 29/300\n",
      "11/11 [==============================] - 3s 300ms/step - loss: 0.6910 - accuracy: 0.5341 - val_loss: 0.7074 - val_accuracy: 0.4826\n",
      "Epoch 30/300\n",
      "11/11 [==============================] - 3s 282ms/step - loss: 0.6906 - accuracy: 0.5347 - val_loss: 0.7068 - val_accuracy: 0.4847\n",
      "Epoch 31/300\n",
      "11/11 [==============================] - 3s 275ms/step - loss: 0.6901 - accuracy: 0.5345 - val_loss: 0.7061 - val_accuracy: 0.4851\n",
      "Epoch 32/300\n",
      "11/11 [==============================] - 3s 271ms/step - loss: 0.6897 - accuracy: 0.5357 - val_loss: 0.7056 - val_accuracy: 0.4867\n",
      "Epoch 33/300\n",
      "11/11 [==============================] - 3s 280ms/step - loss: 0.6893 - accuracy: 0.5370 - val_loss: 0.7051 - val_accuracy: 0.4870\n",
      "Epoch 34/300\n",
      "11/11 [==============================] - 3s 268ms/step - loss: 0.6889 - accuracy: 0.5376 - val_loss: 0.7043 - val_accuracy: 0.4879\n",
      "Epoch 35/300\n",
      "11/11 [==============================] - 3s 296ms/step - loss: 0.6886 - accuracy: 0.5389 - val_loss: 0.7040 - val_accuracy: 0.4863\n",
      "Epoch 36/300\n",
      "11/11 [==============================] - 3s 283ms/step - loss: 0.6882 - accuracy: 0.5389 - val_loss: 0.7038 - val_accuracy: 0.4878\n",
      "Epoch 37/300\n",
      "11/11 [==============================] - 4s 320ms/step - loss: 0.6879 - accuracy: 0.5402 - val_loss: 0.7033 - val_accuracy: 0.4897\n",
      "Epoch 38/300\n",
      "11/11 [==============================] - 3s 285ms/step - loss: 0.6875 - accuracy: 0.5413 - val_loss: 0.7027 - val_accuracy: 0.4908\n",
      "Epoch 39/300\n",
      "11/11 [==============================] - 3s 282ms/step - loss: 0.6872 - accuracy: 0.5421 - val_loss: 0.7025 - val_accuracy: 0.4922\n",
      "Epoch 40/300\n",
      "11/11 [==============================] - 3s 266ms/step - loss: 0.6868 - accuracy: 0.5431 - val_loss: 0.7024 - val_accuracy: 0.4930\n",
      "Epoch 41/300\n",
      "11/11 [==============================] - 3s 275ms/step - loss: 0.6865 - accuracy: 0.5451 - val_loss: 0.7020 - val_accuracy: 0.4934\n",
      "Epoch 42/300\n",
      "11/11 [==============================] - 3s 269ms/step - loss: 0.6862 - accuracy: 0.5457 - val_loss: 0.7015 - val_accuracy: 0.4917\n",
      "Epoch 43/300\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 0.6859 - accuracy: 0.5474 - val_loss: 0.7017 - val_accuracy: 0.4913\n",
      "Epoch 44/300\n",
      "11/11 [==============================] - 3s 279ms/step - loss: 0.6855 - accuracy: 0.5481 - val_loss: 0.7012 - val_accuracy: 0.4927\n",
      "Epoch 45/300\n",
      "11/11 [==============================] - 3s 263ms/step - loss: 0.6852 - accuracy: 0.5502 - val_loss: 0.7016 - val_accuracy: 0.4913\n",
      "Epoch 46/300\n",
      "11/11 [==============================] - 3s 310ms/step - loss: 0.6848 - accuracy: 0.5506 - val_loss: 0.7010 - val_accuracy: 0.4925\n",
      "Epoch 47/300\n",
      "11/11 [==============================] - 3s 300ms/step - loss: 0.6845 - accuracy: 0.5519 - val_loss: 0.7013 - val_accuracy: 0.4937\n",
      "Epoch 48/300\n",
      "11/11 [==============================] - 3s 289ms/step - loss: 0.6842 - accuracy: 0.5530 - val_loss: 0.7010 - val_accuracy: 0.4922\n",
      "Epoch 49/300\n",
      "11/11 [==============================] - 3s 262ms/step - loss: 0.6838 - accuracy: 0.5541 - val_loss: 0.7016 - val_accuracy: 0.4922\n",
      "Epoch 50/300\n",
      "11/11 [==============================] - 3s 272ms/step - loss: 0.6835 - accuracy: 0.5549 - val_loss: 0.7004 - val_accuracy: 0.4960\n",
      "Epoch 51/300\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.6831 - accuracy: 0.5560 - val_loss: 0.7016 - val_accuracy: 0.4951\n",
      "Epoch 52/300\n",
      "11/11 [==============================] - 3s 246ms/step - loss: 0.6828 - accuracy: 0.5570 - val_loss: 0.7012 - val_accuracy: 0.4947\n",
      "Epoch 53/300\n",
      "11/11 [==============================] - 3s 245ms/step - loss: 0.6824 - accuracy: 0.5587 - val_loss: 0.7013 - val_accuracy: 0.4959\n",
      "Epoch 54/300\n",
      "11/11 [==============================] - 3s 242ms/step - loss: 0.6820 - accuracy: 0.5595 - val_loss: 0.7005 - val_accuracy: 0.5003\n",
      "Epoch 55/300\n",
      "11/11 [==============================] - 3s 251ms/step - loss: 0.6816 - accuracy: 0.5611 - val_loss: 0.7024 - val_accuracy: 0.4942\n",
      "Epoch 56/300\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.6812 - accuracy: 0.5620 - val_loss: 0.7012 - val_accuracy: 0.4992\n",
      "Epoch 57/300\n",
      "11/11 [==============================] - 3s 244ms/step - loss: 0.6808 - accuracy: 0.5615 - val_loss: 0.7022 - val_accuracy: 0.4970\n",
      "Epoch 58/300\n",
      "11/11 [==============================] - 3s 246ms/step - loss: 0.6804 - accuracy: 0.5636 - val_loss: 0.7025 - val_accuracy: 0.4969\n",
      "Epoch 59/300\n",
      "11/11 [==============================] - 3s 247ms/step - loss: 0.6800 - accuracy: 0.5643 - val_loss: 0.7016 - val_accuracy: 0.4997\n",
      "Epoch 60/300\n",
      "11/11 [==============================] - 3s 243ms/step - loss: 0.6796 - accuracy: 0.5643 - val_loss: 0.7022 - val_accuracy: 0.4955\n",
      "Epoch 61/300\n",
      "11/11 [==============================] - 3s 245ms/step - loss: 0.6792 - accuracy: 0.5658 - val_loss: 0.7031 - val_accuracy: 0.4965\n",
      "Epoch 62/300\n",
      "11/11 [==============================] - 3s 244ms/step - loss: 0.6788 - accuracy: 0.5668 - val_loss: 0.7030 - val_accuracy: 0.4966\n",
      "Epoch 63/300\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.6784 - accuracy: 0.5683 - val_loss: 0.7031 - val_accuracy: 0.4989\n",
      "Epoch 64/300\n",
      "11/11 [==============================] - 3s 252ms/step - loss: 0.6779 - accuracy: 0.5684 - val_loss: 0.7041 - val_accuracy: 0.4981\n",
      "Epoch 65/300\n",
      "11/11 [==============================] - 3s 266ms/step - loss: 0.6774 - accuracy: 0.5691 - val_loss: 0.7041 - val_accuracy: 0.5019\n",
      "Epoch 66/300\n",
      "11/11 [==============================] - 3s 248ms/step - loss: 0.6770 - accuracy: 0.5696 - val_loss: 0.7045 - val_accuracy: 0.5011\n",
      "Epoch 67/300\n",
      "11/11 [==============================] - 3s 270ms/step - loss: 0.6765 - accuracy: 0.5704 - val_loss: 0.7051 - val_accuracy: 0.5019\n",
      "Epoch 68/300\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.6760 - accuracy: 0.5708 - val_loss: 0.7060 - val_accuracy: 0.4990\n",
      "Epoch 69/300\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.6756 - accuracy: 0.5731 - val_loss: 0.7056 - val_accuracy: 0.5032\n",
      "Epoch 70/300\n",
      "11/11 [==============================] - 3s 271ms/step - loss: 0.6750 - accuracy: 0.5723 - val_loss: 0.7074 - val_accuracy: 0.4990\n",
      "Epoch 71/300\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.6745 - accuracy: 0.5760 - val_loss: 0.7067 - val_accuracy: 0.5021\n",
      "Epoch 72/300\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.6739 - accuracy: 0.5745 - val_loss: 0.7087 - val_accuracy: 0.5005\n",
      "Epoch 73/300\n",
      "11/11 [==============================] - 3s 267ms/step - loss: 0.6734 - accuracy: 0.5762 - val_loss: 0.7084 - val_accuracy: 0.4968\n",
      "Epoch 74/300\n",
      "11/11 [==============================] - 3s 245ms/step - loss: 0.6730 - accuracy: 0.5765 - val_loss: 0.7080 - val_accuracy: 0.5027\n",
      "Epoch 75/300\n",
      "11/11 [==============================] - 3s 251ms/step - loss: 0.6724 - accuracy: 0.5770 - val_loss: 0.7126 - val_accuracy: 0.4923\n",
      "Epoch 76/300\n",
      "11/11 [==============================] - 3s 236ms/step - loss: 0.6718 - accuracy: 0.5785 - val_loss: 0.7121 - val_accuracy: 0.4946\n",
      "Epoch 77/300\n",
      "11/11 [==============================] - 3s 235ms/step - loss: 0.6712 - accuracy: 0.5788 - val_loss: 0.7109 - val_accuracy: 0.4973\n",
      "Epoch 78/300\n",
      "11/11 [==============================] - 3s 242ms/step - loss: 0.6707 - accuracy: 0.5805 - val_loss: 0.7181 - val_accuracy: 0.4978\n",
      "Epoch 79/300\n",
      "11/11 [==============================] - 3s 233ms/step - loss: 0.6701 - accuracy: 0.5820 - val_loss: 0.7188 - val_accuracy: 0.4871\n",
      "Epoch 80/300\n",
      "11/11 [==============================] - 3s 235ms/step - loss: 0.6690 - accuracy: 0.5844 - val_loss: 0.7169 - val_accuracy: 0.4992\n",
      "Epoch 81/300\n",
      "11/11 [==============================] - 3s 234ms/step - loss: 0.6684 - accuracy: 0.5836 - val_loss: 0.7288 - val_accuracy: 0.4898\n",
      "Epoch 82/300\n",
      "11/11 [==============================] - 3s 236ms/step - loss: 0.6676 - accuracy: 0.5840 - val_loss: 0.7382 - val_accuracy: 0.4918\n",
      "Epoch 83/300\n",
      "11/11 [==============================] - 3s 233ms/step - loss: 0.6670 - accuracy: 0.5867 - val_loss: 0.7228 - val_accuracy: 0.4920\n",
      "Epoch 84/300\n",
      "11/11 [==============================] - 3s 232ms/step - loss: 0.6662 - accuracy: 0.5855 - val_loss: 0.7447 - val_accuracy: 0.4960\n",
      "Epoch 85/300\n",
      "11/11 [==============================] - 3s 240ms/step - loss: 0.6656 - accuracy: 0.5875 - val_loss: 0.7563 - val_accuracy: 0.4883\n",
      "Epoch 86/300\n",
      "11/11 [==============================] - 3s 243ms/step - loss: 0.6646 - accuracy: 0.5890 - val_loss: 0.7598 - val_accuracy: 0.4916\n",
      "Epoch 87/300\n",
      "11/11 [==============================] - 3s 246ms/step - loss: 0.6636 - accuracy: 0.5920 - val_loss: 0.7559 - val_accuracy: 0.4919\n",
      "Epoch 88/300\n",
      "11/11 [==============================] - 3s 234ms/step - loss: 0.6629 - accuracy: 0.5907 - val_loss: 0.7626 - val_accuracy: 0.4947\n",
      "Epoch 89/300\n",
      "11/11 [==============================] - 3s 236ms/step - loss: 0.6620 - accuracy: 0.5941 - val_loss: 0.7691 - val_accuracy: 0.4927\n",
      "Epoch 90/300\n",
      "11/11 [==============================] - 3s 236ms/step - loss: 0.6614 - accuracy: 0.5945 - val_loss: 0.7671 - val_accuracy: 0.5012\n",
      "Epoch 91/300\n",
      "11/11 [==============================] - 3s 235ms/step - loss: 0.6605 - accuracy: 0.5952 - val_loss: 0.7768 - val_accuracy: 0.4931\n",
      "Epoch 92/300\n",
      "11/11 [==============================] - 3s 234ms/step - loss: 0.6596 - accuracy: 0.5971 - val_loss: 0.7740 - val_accuracy: 0.5007\n",
      "Epoch 93/300\n",
      "11/11 [==============================] - 3s 238ms/step - loss: 0.6587 - accuracy: 0.5976 - val_loss: 0.7871 - val_accuracy: 0.5019\n",
      "Epoch 94/300\n",
      "11/11 [==============================] - 3s 237ms/step - loss: 0.6577 - accuracy: 0.5987 - val_loss: 0.7927 - val_accuracy: 0.4980\n",
      "Epoch 95/300\n",
      "11/11 [==============================] - 3s 238ms/step - loss: 0.6611 - accuracy: 0.5949 - val_loss: 0.8487 - val_accuracy: 0.4884\n",
      "Epoch 96/300\n",
      "11/11 [==============================] - 3s 247ms/step - loss: 0.6867 - accuracy: 0.5686 - val_loss: 0.7264 - val_accuracy: 0.5207\n",
      "Epoch 97/300\n",
      "11/11 [==============================] - 3s 241ms/step - loss: 0.6670 - accuracy: 0.5894 - val_loss: 0.7669 - val_accuracy: 0.5049\n",
      "Epoch 98/300\n",
      "11/11 [==============================] - 3s 233ms/step - loss: 0.6630 - accuracy: 0.5893 - val_loss: 0.7632 - val_accuracy: 0.5020\n",
      "Epoch 99/300\n",
      "11/11 [==============================] - 3s 234ms/step - loss: 0.6606 - accuracy: 0.5939 - val_loss: 0.7676 - val_accuracy: 0.5014\n",
      "Epoch 100/300\n",
      "11/11 [==============================] - 3s 239ms/step - loss: 0.6584 - accuracy: 0.5975 - val_loss: 0.7774 - val_accuracy: 0.5024\n",
      "Epoch 101/300\n",
      "11/11 [==============================] - 3s 238ms/step - loss: 0.6570 - accuracy: 0.6001 - val_loss: 0.7814 - val_accuracy: 0.5061\n",
      "Epoch 102/300\n",
      "11/11 [==============================] - 3s 238ms/step - loss: 0.6558 - accuracy: 0.6026 - val_loss: 0.7868 - val_accuracy: 0.4962\n",
      "Epoch 103/300\n",
      "11/11 [==============================] - 3s 227ms/step - loss: 0.6543 - accuracy: 0.6041 - val_loss: 0.7975 - val_accuracy: 0.5030\n",
      "Epoch 104/300\n",
      "11/11 [==============================] - 2s 226ms/step - loss: 0.6533 - accuracy: 0.6053 - val_loss: 0.7999 - val_accuracy: 0.4986\n",
      "Epoch 105/300\n",
      "11/11 [==============================] - 2s 227ms/step - loss: 0.6526 - accuracy: 0.6055 - val_loss: 0.8159 - val_accuracy: 0.4934\n",
      "Epoch 106/300\n",
      "11/11 [==============================] - 3s 238ms/step - loss: 0.6523 - accuracy: 0.6054 - val_loss: 0.8068 - val_accuracy: 0.5083\n",
      "Epoch 107/300\n",
      "11/11 [==============================] - 3s 239ms/step - loss: 0.6506 - accuracy: 0.6075 - val_loss: 0.8004 - val_accuracy: 0.4954\n",
      "Epoch 108/300\n",
      "11/11 [==============================] - 3s 237ms/step - loss: 0.6495 - accuracy: 0.6101 - val_loss: 0.8137 - val_accuracy: 0.5011\n",
      "Epoch 109/300\n",
      "11/11 [==============================] - 3s 230ms/step - loss: 0.6484 - accuracy: 0.6110 - val_loss: 0.8257 - val_accuracy: 0.5011\n",
      "Epoch 110/300\n",
      "11/11 [==============================] - 3s 230ms/step - loss: 0.6476 - accuracy: 0.6112 - val_loss: 0.8216 - val_accuracy: 0.4988\n",
      "Epoch 111/300\n",
      "11/11 [==============================] - 3s 229ms/step - loss: 0.6468 - accuracy: 0.6135 - val_loss: 0.8226 - val_accuracy: 0.5005\n",
      "Epoch 112/300\n",
      "11/11 [==============================] - 3s 230ms/step - loss: 0.6479 - accuracy: 0.6143 - val_loss: 0.7882 - val_accuracy: 0.4976\n",
      "Epoch 113/300\n",
      "11/11 [==============================] - 3s 266ms/step - loss: 0.6466 - accuracy: 0.6140 - val_loss: 0.8047 - val_accuracy: 0.4986\n",
      "Epoch 114/300\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 0.6450 - accuracy: 0.6169 - val_loss: 0.8389 - val_accuracy: 0.5041\n",
      "Epoch 115/300\n",
      "11/11 [==============================] - 3s 250ms/step - loss: 0.6442 - accuracy: 0.6163 - val_loss: 0.8458 - val_accuracy: 0.4963\n",
      "Epoch 116/300\n",
      "11/11 [==============================] - 3s 248ms/step - loss: 0.6443 - accuracy: 0.6173 - val_loss: 0.8183 - val_accuracy: 0.5020\n",
      "Epoch 117/300\n",
      "11/11 [==============================] - 3s 270ms/step - loss: 0.6426 - accuracy: 0.6176 - val_loss: 0.8582 - val_accuracy: 0.5064\n",
      "Epoch 118/300\n",
      "11/11 [==============================] - 3s 271ms/step - loss: 0.6408 - accuracy: 0.6212 - val_loss: 0.8435 - val_accuracy: 0.5059\n",
      "Epoch 119/300\n",
      "11/11 [==============================] - 3s 244ms/step - loss: 0.6401 - accuracy: 0.6213 - val_loss: 0.8637 - val_accuracy: 0.5019\n",
      "Epoch 120/300\n",
      "11/11 [==============================] - 3s 245ms/step - loss: 0.6386 - accuracy: 0.6243 - val_loss: 0.8851 - val_accuracy: 0.5021\n",
      "Epoch 121/300\n",
      "11/11 [==============================] - 3s 245ms/step - loss: 0.6384 - accuracy: 0.6217 - val_loss: 0.8538 - val_accuracy: 0.5133\n",
      "Epoch 122/300\n",
      "11/11 [==============================] - 3s 235ms/step - loss: 0.6369 - accuracy: 0.6224 - val_loss: 0.9191 - val_accuracy: 0.5127\n",
      "Epoch 123/300\n",
      "11/11 [==============================] - 3s 241ms/step - loss: 0.6361 - accuracy: 0.6260 - val_loss: 0.8980 - val_accuracy: 0.5054\n",
      "Epoch 124/300\n",
      "11/11 [==============================] - 3s 238ms/step - loss: 0.6347 - accuracy: 0.6268 - val_loss: 0.9168 - val_accuracy: 0.5084\n",
      "Epoch 125/300\n",
      "11/11 [==============================] - 3s 240ms/step - loss: 0.6343 - accuracy: 0.6259 - val_loss: 0.9176 - val_accuracy: 0.5058\n",
      "Epoch 126/300\n",
      "11/11 [==============================] - 3s 235ms/step - loss: 0.6333 - accuracy: 0.6281 - val_loss: 0.9228 - val_accuracy: 0.5060\n",
      "Epoch 127/300\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.6318 - accuracy: 0.6289 - val_loss: 0.9301 - val_accuracy: 0.5079\n",
      "Epoch 128/300\n",
      "11/11 [==============================] - 3s 242ms/step - loss: 0.6306 - accuracy: 0.6297 - val_loss: 0.9229 - val_accuracy: 0.5001\n",
      "Epoch 129/300\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 0.6302 - accuracy: 0.6306 - val_loss: 0.9540 - val_accuracy: 0.5056\n",
      "Epoch 130/300\n",
      "11/11 [==============================] - 3s 232ms/step - loss: 0.6294 - accuracy: 0.6310 - val_loss: 0.9241 - val_accuracy: 0.5037\n",
      "Epoch 131/300\n",
      "11/11 [==============================] - 3s 238ms/step - loss: 0.6292 - accuracy: 0.6304 - val_loss: 0.9512 - val_accuracy: 0.5043\n",
      "Epoch 132/300\n",
      "11/11 [==============================] - 3s 230ms/step - loss: 0.6271 - accuracy: 0.6317 - val_loss: 0.9837 - val_accuracy: 0.5056\n",
      "Epoch 133/300\n",
      "11/11 [==============================] - 3s 228ms/step - loss: 0.6266 - accuracy: 0.6347 - val_loss: 0.9634 - val_accuracy: 0.5008\n",
      "Epoch 134/300\n",
      "11/11 [==============================] - 2s 225ms/step - loss: 0.6294 - accuracy: 0.6329 - val_loss: 0.9325 - val_accuracy: 0.4956\n",
      "Epoch 135/300\n",
      "11/11 [==============================] - 3s 228ms/step - loss: 0.6276 - accuracy: 0.6329 - val_loss: 0.8759 - val_accuracy: 0.4986\n",
      "Epoch 136/300\n",
      "11/11 [==============================] - 3s 242ms/step - loss: 0.6797 - accuracy: 0.6166 - val_loss: 0.8760 - val_accuracy: 0.4913\n",
      "Epoch 137/300\n",
      "11/11 [==============================] - 3s 233ms/step - loss: 0.7354 - accuracy: 0.5875 - val_loss: 0.9087 - val_accuracy: 0.4998\n",
      "Epoch 138/300\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 0.6743 - accuracy: 0.5911 - val_loss: 0.7428 - val_accuracy: 0.5081\n",
      "Epoch 139/300\n",
      "11/11 [==============================] - 3s 232ms/step - loss: 0.6581 - accuracy: 0.6058 - val_loss: 0.7245 - val_accuracy: 0.5060\n",
      "Epoch 140/300\n",
      "11/11 [==============================] - 3s 228ms/step - loss: 0.6525 - accuracy: 0.6148 - val_loss: 0.7231 - val_accuracy: 0.5077\n",
      "Epoch 141/300\n",
      "11/11 [==============================] - 3s 233ms/step - loss: 0.6495 - accuracy: 0.6163 - val_loss: 0.7256 - val_accuracy: 0.5005\n",
      "Epoch 142/300\n",
      "11/11 [==============================] - 3s 228ms/step - loss: 0.6466 - accuracy: 0.6234 - val_loss: 0.7263 - val_accuracy: 0.5038\n",
      "Epoch 143/300\n",
      "11/11 [==============================] - 3s 230ms/step - loss: 0.6440 - accuracy: 0.6249 - val_loss: 0.7287 - val_accuracy: 0.5064\n",
      "Epoch 144/300\n",
      "11/11 [==============================] - 3s 238ms/step - loss: 0.6418 - accuracy: 0.6255 - val_loss: 0.7329 - val_accuracy: 0.5027\n",
      "Epoch 145/300\n",
      "11/11 [==============================] - 3s 232ms/step - loss: 0.6398 - accuracy: 0.6286 - val_loss: 0.7420 - val_accuracy: 0.5003\n",
      "Epoch 146/300\n",
      "11/11 [==============================] - 3s 234ms/step - loss: 0.6377 - accuracy: 0.6304 - val_loss: 0.7483 - val_accuracy: 0.4994\n",
      "Epoch 147/300\n",
      "11/11 [==============================] - 3s 274ms/step - loss: 0.6360 - accuracy: 0.6323 - val_loss: 0.7612 - val_accuracy: 0.5005\n",
      "Epoch 148/300\n",
      "11/11 [==============================] - 3s 261ms/step - loss: 0.6343 - accuracy: 0.6327 - val_loss: 0.7665 - val_accuracy: 0.5043\n",
      "Epoch 149/300\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.6326 - accuracy: 0.6361 - val_loss: 0.7826 - val_accuracy: 0.5006\n",
      "Epoch 150/300\n",
      "11/11 [==============================] - 3s 261ms/step - loss: 0.6312 - accuracy: 0.6348 - val_loss: 0.7930 - val_accuracy: 0.5014\n",
      "Epoch 151/300\n",
      "11/11 [==============================] - 3s 271ms/step - loss: 0.6297 - accuracy: 0.6376 - val_loss: 0.8079 - val_accuracy: 0.5033\n",
      "Epoch 152/300\n",
      "11/11 [==============================] - 3s 267ms/step - loss: 0.6282 - accuracy: 0.6382 - val_loss: 0.8140 - val_accuracy: 0.5042\n",
      "Epoch 153/300\n",
      "11/11 [==============================] - 3s 244ms/step - loss: 0.6269 - accuracy: 0.6395 - val_loss: 0.8362 - val_accuracy: 0.5000\n",
      "Epoch 154/300\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 0.6253 - accuracy: 0.6397 - val_loss: 0.8420 - val_accuracy: 0.5055\n",
      "Epoch 155/300\n",
      "11/11 [==============================] - 3s 244ms/step - loss: 0.6241 - accuracy: 0.6407 - val_loss: 0.8601 - val_accuracy: 0.5077\n",
      "Epoch 156/300\n",
      "11/11 [==============================] - 3s 243ms/step - loss: 0.6232 - accuracy: 0.6415 - val_loss: 0.8625 - val_accuracy: 0.4980\n",
      "Epoch 157/300\n",
      "11/11 [==============================] - 3s 247ms/step - loss: 0.6220 - accuracy: 0.6422 - val_loss: 0.8881 - val_accuracy: 0.5036\n",
      "Epoch 158/300\n",
      "11/11 [==============================] - 3s 265ms/step - loss: 0.6209 - accuracy: 0.6443 - val_loss: 0.8969 - val_accuracy: 0.5127\n",
      "Epoch 159/300\n",
      "11/11 [==============================] - 3s 233ms/step - loss: 0.6194 - accuracy: 0.6444 - val_loss: 0.9000 - val_accuracy: 0.5004\n",
      "Epoch 160/300\n",
      "11/11 [==============================] - 3s 233ms/step - loss: 0.6184 - accuracy: 0.6454 - val_loss: 0.9195 - val_accuracy: 0.4976\n",
      "Epoch 161/300\n",
      "11/11 [==============================] - 3s 233ms/step - loss: 0.6180 - accuracy: 0.6435 - val_loss: 0.9471 - val_accuracy: 0.5017\n",
      "Epoch 162/300\n",
      "11/11 [==============================] - 3s 232ms/step - loss: 0.6161 - accuracy: 0.6474 - val_loss: 0.9360 - val_accuracy: 0.4995\n",
      "Epoch 163/300\n",
      "11/11 [==============================] - 2s 227ms/step - loss: 0.6148 - accuracy: 0.6483 - val_loss: 0.9760 - val_accuracy: 0.5054\n",
      "Epoch 164/300\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.6141 - accuracy: 0.6479 - val_loss: 0.9866 - val_accuracy: 0.4967\n",
      "Epoch 165/300\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.6132 - accuracy: 0.6499 - val_loss: 1.0016 - val_accuracy: 0.5026\n",
      "Epoch 166/300\n",
      "11/11 [==============================] - 3s 243ms/step - loss: 0.6118 - accuracy: 0.6506 - val_loss: 1.0133 - val_accuracy: 0.4998\n",
      "Epoch 167/300\n",
      "11/11 [==============================] - 3s 243ms/step - loss: 0.6104 - accuracy: 0.6512 - val_loss: 1.0253 - val_accuracy: 0.4995\n",
      "Epoch 168/300\n",
      "11/11 [==============================] - 3s 270ms/step - loss: 0.6098 - accuracy: 0.6512 - val_loss: 1.0372 - val_accuracy: 0.5001\n",
      "Epoch 169/300\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.6084 - accuracy: 0.6534 - val_loss: 1.0682 - val_accuracy: 0.5069\n",
      "Epoch 170/300\n",
      "11/11 [==============================] - 3s 268ms/step - loss: 0.6076 - accuracy: 0.6539 - val_loss: 1.0865 - val_accuracy: 0.4973\n",
      "Epoch 171/300\n",
      "11/11 [==============================] - 3s 243ms/step - loss: 0.6076 - accuracy: 0.6539 - val_loss: 1.0756 - val_accuracy: 0.5029\n",
      "Epoch 172/300\n",
      "11/11 [==============================] - 3s 241ms/step - loss: 0.6058 - accuracy: 0.6552 - val_loss: 1.0681 - val_accuracy: 0.4989\n",
      "Epoch 173/300\n",
      "11/11 [==============================] - 3s 242ms/step - loss: 0.6049 - accuracy: 0.6557 - val_loss: 1.0892 - val_accuracy: 0.4980\n",
      "Epoch 174/300\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 0.6026 - accuracy: 0.6594 - val_loss: 1.1106 - val_accuracy: 0.5004\n",
      "Epoch 175/300\n",
      "11/11 [==============================] - 3s 251ms/step - loss: 0.6027 - accuracy: 0.6584 - val_loss: 1.1465 - val_accuracy: 0.4966\n",
      "Epoch 176/300\n",
      "11/11 [==============================] - 3s 241ms/step - loss: 0.6021 - accuracy: 0.6594 - val_loss: 1.1598 - val_accuracy: 0.4972\n",
      "Epoch 177/300\n",
      "11/11 [==============================] - 3s 239ms/step - loss: 0.6016 - accuracy: 0.6580 - val_loss: 1.1456 - val_accuracy: 0.4942\n",
      "Epoch 178/300\n",
      "11/11 [==============================] - 3s 252ms/step - loss: 0.6014 - accuracy: 0.6572 - val_loss: 1.1556 - val_accuracy: 0.4998\n",
      "Epoch 179/300\n",
      "11/11 [==============================] - 3s 247ms/step - loss: 0.6028 - accuracy: 0.6570 - val_loss: 1.1294 - val_accuracy: 0.4968\n",
      "Epoch 180/300\n",
      "11/11 [==============================] - 3s 241ms/step - loss: 0.5989 - accuracy: 0.6626 - val_loss: 1.1135 - val_accuracy: 0.4955\n",
      "Epoch 181/300\n",
      " 8/11 [====================>.........] - ETA: 0s - loss: 0.5982 - accuracy: 0.6610"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dataset_scaled_train = dataset_scaled[:'2020']\n",
    "\n",
    "import datetime\n",
    "X_train = X[indexes < datetime.datetime(2021, 1, 1)]\n",
    "y_train = y[indexes < datetime.datetime(2021, 1, 1)]\n",
    "\n",
    "checkpoint_filepath = './checkpoint_u22'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train > 1,\n",
    "    batch_size=5000,\n",
    "    epochs=300,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ey = model.predict(X)\n",
    "ey = pd.Series(ey.swapaxes(0,1)[0], index=indexes)\n",
    "ey.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq = twii[indexes]\n",
    "returns = (eq.shift(-1) - eq)\n",
    "\n",
    "signal = (ey > ey.quantile(0.6)).rolling(10).sum() > 0\n",
    "signal = signal.shift(1).fillna(False)\n",
    "\n",
    "eq = (returns[signal]['2021':]).cumsum()\n",
    "eq.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(signal.astype(int).diff().abs().fillna(0) * 3)['2021':].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finlab",
   "language": "python",
   "name": "finlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
