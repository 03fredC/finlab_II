{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 獲取歷史資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finlab.data import Data\n",
    "\n",
    "data = Data()\n",
    "\n",
    "rev = data.get(\"當月營收\")\n",
    "PB = data.get(\"股價淨值比\")\n",
    "\n",
    "open = data.get(\"開盤價\")\n",
    "high = data.get(\"最高價\")\n",
    "low = data.get(\"最低價\")\n",
    "close = data.get(\"收盤價\")\n",
    "volume= data.get('成交股數')\n",
    "\n",
    "\n",
    "rev.index = rev.index.shift(5, \"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from finlab.data import Data\n",
    "### 計算features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import pandas as ta\n",
    "##ta.\n",
    "#\n",
    "#\n",
    "#from finlab.data import Data\n",
    "#data = Data()\n",
    "#import finlab.ml as ml\n",
    "#import pandas as pd\n",
    "#\n",
    "#twii_df = pd.DataFrame({\n",
    "#    'open' : data.get(\"開盤價\"),\n",
    "#    'high': data.get(\"最高價\"),\n",
    "#    'low': data.get(\"最低價\"),\n",
    "#    \"close\": data.get(\"收盤價\"),\n",
    "#})\n",
    "#\n",
    "#indicators = ['STOCH', 'RSI']\n",
    "#multiplier = [1,5,10,20,30]\n",
    "#dataset = ml.talib_features(twii_df,indicators, multiplier)\n",
    "#feature_names - dataset.columns\n",
    "#dataset ['return'] = twii.shift(-10) / twii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias(n):\n",
    "    return close / close.rolling(n, min_periods=1).mean()\n",
    "\n",
    "def acc(n):\n",
    "    return close.shift(n) / (close.shift(2*n) + close) * 2\n",
    "\n",
    "def rsv(n):\n",
    "    l = close.rolling(n, min_periods=1).min()\n",
    "    h = close.rolling(n, min_periods=1).max()\n",
    "    \n",
    "    return (close - l) / (h - l)\n",
    "\n",
    "def mom(n):\n",
    "    return (rev / rev.shift(1)).shift(n)\n",
    "\n",
    "def yoy(n):\n",
    "    return (rev / rev.shift(12)).shift(n)\n",
    "\n",
    "\n",
    "def PB(n):\n",
    "    return PB\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "features = {\n",
    "    'mom1': mom(1),\n",
    "    'mom2': mom(2),\n",
    "    'mom3': mom(3),\n",
    "    'mom4': mom(4),\n",
    "    'mom5': mom(5),\n",
    "    'mom6': mom(6),\n",
    "    'mom7': mom(7),\n",
    "    'mom8': mom(8),\n",
    "    'mom9': mom(9),\n",
    "    \n",
    "    'yoy': yoy(1),\n",
    "    'delta_yoy':yoy(1)-yoy(2),\n",
    "    \n",
    "    #\"PB\":PB(1),\n",
    "    \n",
    "    'bias5': bias(5),\n",
    "    'bias10': bias(10),\n",
    "    'bias20': bias(20),\n",
    "    'bias60': bias(60),\n",
    "    'bias120': bias(120),\n",
    "    'bias240': bias(240),\n",
    "    \n",
    "    'acc5': acc(5),\n",
    "    'acc10': acc(10),\n",
    "    'acc20': acc(20),\n",
    "    'acc60': acc(60),\n",
    "    'acc120': acc(120),\n",
    "    'acc240': acc(240),\n",
    "    \n",
    "    'rsv5': rsv(5),\n",
    "    'rsv10': rsv(10),\n",
    "    'rsv20': rsv(20),\n",
    "    'rsv60': rsv(60),\n",
    "    'rsv120': rsv(120),\n",
    "    'rsv240': rsv(240),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 製作dataset\n",
    "\n",
    "##### 設定買賣頻率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2005-02-15', '2005-03-15', '2005-04-15', '2005-05-15',\n",
       "               '2005-06-15', '2005-07-15', '2005-08-15', '2005-09-15',\n",
       "               '2005-10-15', '2005-11-15',\n",
       "               ...\n",
       "               '2021-11-15', '2021-12-15', '2022-01-15', '2022-02-15',\n",
       "               '2022-03-15', '2022-04-15', '2022-05-15', '2022-06-15',\n",
       "               '2022-07-15', '2022-08-15'],\n",
       "              dtype='datetime64[ns]', name='date', length=211, freq=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "every_month = rev.index\n",
    "every_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 將dataframe 組裝起來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features['bias20'].reindex(every_month, method='ffill')\n",
    "\n",
    "for name, f in features.items():\n",
    "    features[name] = f.reindex(every_month, method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name, f in features.items():\n",
    "    features[name] = f.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(dataset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 新增 label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Finlab\\AI選股策略\\finlab\\data.py:103: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead\n",
      "  all_index = (df.index | item.index).sort_values()\n",
      "D:\\Finlab\\AI選股策略\\finlab\\data.py:103: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead\n",
      "  all_index = (df.index | item.index).sort_values()\n",
      "D:\\Finlab\\AI選股策略\\finlab\\data.py:103: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead\n",
      "  all_index = (df.index | item.index).sort_values()\n",
      "D:\\Finlab\\AI選股策略\\finlab\\data.py:103: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead\n",
      "  all_index = (df.index | item.index).sort_values()\n",
      "D:\\Finlab\\AI選股策略\\finlab\\data.py:103: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead\n",
      "  all_index = (df.index | item.index).sort_values()\n",
      "D:\\Finlab\\AI選股策略\\finlab\\data.py:103: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead\n",
      "  all_index = (df.index | item.index).sort_values()\n",
      "D:\\Finlab\\AI選股策略\\finlab\\data.py:103: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead\n",
      "  all_index = (df.index | item.index).sort_values()\n",
      "D:\\Finlab\\AI選股策略\\finlab\\data.py:103: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead\n",
      "  all_index = (df.index | item.index).sort_values()\n"
     ]
    }
   ],
   "source": [
    "from finlab import ml\n",
    "\n",
    "ml.add_profit_prediction(dataset)\n",
    "ml.add_rank_prediction(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 刪除太大太小的歷史資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(428541, 31)\n",
      "(398596, 31)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "\n",
    "def drop_extreme_case(dataset, feature_names, thresh=0.01):\n",
    "    \n",
    "    extreme_cases = pd.Series(False, index=dataset.index)\n",
    "    for f in feature_names:\n",
    "        tf = dataset[f]\n",
    "        extreme_cases = extreme_cases | (tf < tf.quantile(thresh)) | (tf > tf.quantile(1-thresh))\n",
    "    dataset = dataset[~extreme_cases]\n",
    "    return dataset\n",
    "\n",
    "dataset_drop_extreme_case = drop_extreme_case(dataset, \n",
    "    ['bias60', 'bias120', 'bias240', 'mom1', 'mom2', 'mom3', 'mom4', 'mom5', 'mom6','yoy', 'delta_yoy'], thresh=0.01)\n",
    "\n",
    "print(dataset_drop_extreme_case.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dropna = dataset_drop_extreme_case.dropna(how='any')\n",
    "dataset_dropna = dataset_dropna.reset_index().set_index(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2005-02-15', '2005-03-15', '2005-04-15', '2005-05-15',\n",
       "               '2005-06-15', '2005-07-15', '2005-08-15', '2005-09-15',\n",
       "               '2005-10-15', '2005-11-15',\n",
       "               ...\n",
       "               '2021-11-15', '2021-12-15', '2022-01-15', '2022-02-15',\n",
       "               '2022-03-15', '2022-04-15', '2022-05-15', '2022-06-15',\n",
       "               '2022-07-15', '2022-08-15'],\n",
       "              dtype='datetime64[ns]', name='date', length=398596, freq=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_drop_extreme_case.index.get_level_values(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\finlab\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Value based partial slicing on non-monotonic DatetimeIndexes with non-existing keys is deprecated and will raise a KeyError in a future Version.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dataset_train = dataset_dropna.loc[:'2020']\n",
    "dataset_test = dataset_dropna.loc['2021':]\n",
    "\n",
    "train = (dataset_train[feature_names], dataset_train['return']  >1)#>1\n",
    "test = (dataset_test[feature_names], dataset_train['return']  >1) #>1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神經網路模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 100)               3000      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 13,201\n",
      "Trainable params: 13,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "start fitting\n",
      "Epoch 1/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.186 - ETA: 0s - loss: 0.153 - ETA: 0s - loss: 0.119 - ETA: 0s - loss: 0.106 - ETA: 0s - loss: 0.100 - ETA: 0s - loss: 0.095 - ETA: 0s - loss: 0.093 - ETA: 0s - loss: 0.091 - ETA: 0s - loss: 0.090 - ETA: 0s - loss: 0.088 - 1s 5ms/step - loss: 0.0888 - val_loss: 0.0791\n",
      "Epoch 2/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0799 - val_loss: 0.0791\n",
      "Epoch 3/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0798 - val_loss: 0.0791\n",
      "Epoch 4/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0798 - val_loss: 0.0790\n",
      "Epoch 5/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0797 - val_loss: 0.0790\n",
      "Epoch 6/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0797 - val_loss: 0.0790\n",
      "Epoch 7/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0796 - val_loss: 0.0790\n",
      "Epoch 8/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0796 - val_loss: 0.0790\n",
      "Epoch 9/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0796 - val_loss: 0.0789\n",
      "Epoch 10/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0796 - val_loss: 0.0789\n",
      "Epoch 11/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0795 - val_loss: 0.0788\n",
      "Epoch 12/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0795 - val_loss: 0.0789\n",
      "Epoch 13/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0795 - val_loss: 0.0788\n",
      "Epoch 14/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0795 - val_loss: 0.0788\n",
      "Epoch 15/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0795 - val_loss: 0.0788\n",
      "Epoch 16/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0794 - val_loss: 0.0787\n",
      "Epoch 17/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0794 - val_loss: 0.0787\n",
      "Epoch 18/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0794 - val_loss: 0.0786\n",
      "Epoch 19/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0794 - val_loss: 0.0787\n",
      "Epoch 20/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.084 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0794 - val_loss: 0.0787\n",
      "Epoch 21/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0794 - val_loss: 0.0786\n",
      "Epoch 22/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0794 - val_loss: 0.0786\n",
      "Epoch 23/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0794 - val_loss: 0.0785\n",
      "Epoch 24/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0793 - val_loss: 0.0786\n",
      "Epoch 25/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0793 - val_loss: 0.0786\n",
      "Epoch 26/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0793 - val_loss: 0.0785\n",
      "Epoch 27/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0793 - val_loss: 0.0785\n",
      "Epoch 28/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0792 - val_loss: 0.0786\n",
      "Epoch 29/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0793 - val_loss: 0.0785\n",
      "Epoch 30/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0792 - val_loss: 0.0786\n",
      "Epoch 31/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0792 - val_loss: 0.0785\n",
      "Epoch 32/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0792 - val_loss: 0.0785\n",
      "Epoch 33/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0792 - val_loss: 0.0785\n",
      "Epoch 34/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0792 - val_loss: 0.0785\n",
      "Epoch 35/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0792 - val_loss: 0.0785\n",
      "Epoch 36/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0792 - val_loss: 0.0784\n",
      "Epoch 37/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0791 - val_loss: 0.0784\n",
      "Epoch 38/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0791 - val_loss: 0.0784\n",
      "Epoch 39/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0791 - val_loss: 0.0784\n",
      "Epoch 40/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0791 - val_loss: 0.0784\n",
      "Epoch 41/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0791 - val_loss: 0.0784\n",
      "Epoch 42/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0791 - val_loss: 0.0783\n",
      "Epoch 43/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0790 - val_loss: 0.0783\n",
      "Epoch 44/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0790 - val_loss: 0.0784\n",
      "Epoch 45/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0791 - val_loss: 0.0783\n",
      "Epoch 46/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0790 - val_loss: 0.0783\n",
      "Epoch 47/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0790 - val_loss: 0.0783\n",
      "Epoch 48/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0790 - val_loss: 0.0783\n",
      "Epoch 49/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0790 - val_loss: 0.0783\n",
      "Epoch 50/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0790 - val_loss: 0.0783\n",
      "Epoch 51/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0790 - val_loss: 0.0782\n",
      "Epoch 52/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0789 - val_loss: 0.0783\n",
      "Epoch 53/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0789 - val_loss: 0.0783\n",
      "Epoch 54/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0789 - val_loss: 0.0782\n",
      "Epoch 55/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - 1s 4ms/step - loss: 0.0789 - val_loss: 0.0782\n",
      "Epoch 56/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0789 - val_loss: 0.0782\n",
      "Epoch 57/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0789 - val_loss: 0.0783\n",
      "Epoch 58/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0788 - val_loss: 0.0782\n",
      "Epoch 59/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0789 - val_loss: 0.0782\n",
      "Epoch 60/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0788 - val_loss: 0.0783\n",
      "Epoch 61/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0788 - val_loss: 0.0783\n",
      "Epoch 62/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0788 - val_loss: 0.0782\n",
      "Epoch 63/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0787 - val_loss: 0.0783\n",
      "Epoch 64/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0787 - val_loss: 0.0783\n",
      "Epoch 65/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0788 - val_loss: 0.0783\n",
      "Epoch 66/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0787 - val_loss: 0.0782\n",
      "Epoch 67/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0787 - val_loss: 0.0782\n",
      "Epoch 68/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0787 - val_loss: 0.0781\n",
      "Epoch 69/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0787 - val_loss: 0.0781\n",
      "Epoch 70/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0787 - val_loss: 0.0782\n",
      "Epoch 71/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0786 - val_loss: 0.0781\n",
      "Epoch 72/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0787 - val_loss: 0.0782\n",
      "Epoch 73/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0787 - val_loss: 0.0782\n",
      "Epoch 74/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0786 - val_loss: 0.0782\n",
      "Epoch 75/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.083 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0787 - val_loss: 0.0781\n",
      "Epoch 76/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0786 - val_loss: 0.0783\n",
      "Epoch 77/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0786 - val_loss: 0.0782\n",
      "Epoch 78/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0786 - val_loss: 0.0782\n",
      "Epoch 79/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0786 - val_loss: 0.0781\n",
      "Epoch 80/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0786 - val_loss: 0.0781\n",
      "Epoch 81/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0787 - val_loss: 0.0782\n",
      "Epoch 82/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0786 - val_loss: 0.0781\n",
      "Epoch 83/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0786 - val_loss: 0.0781\n",
      "Epoch 84/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0785 - val_loss: 0.0782\n",
      "Epoch 85/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0785 - val_loss: 0.0782\n",
      "Epoch 86/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0785 - val_loss: 0.0781\n",
      "Epoch 87/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0784 - val_loss: 0.0782\n",
      "Epoch 88/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0785 - val_loss: 0.0782\n",
      "Epoch 89/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0785 - val_loss: 0.0782\n",
      "Epoch 90/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0785 - val_loss: 0.0782\n",
      "Epoch 91/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0784 - val_loss: 0.0781\n",
      "Epoch 92/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0785 - val_loss: 0.0781\n",
      "Epoch 93/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0784 - val_loss: 0.0782\n",
      "Epoch 94/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0784 - val_loss: 0.0781\n",
      "Epoch 95/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0785 - val_loss: 0.0781\n",
      "Epoch 96/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0783 - val_loss: 0.0781\n",
      "Epoch 97/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0784 - val_loss: 0.0781\n",
      "Epoch 98/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0783 - val_loss: 0.0781\n",
      "Epoch 99/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0784 - val_loss: 0.0782\n",
      "Epoch 100/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0784 - val_loss: 0.0781\n",
      "Epoch 101/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0784 - val_loss: 0.0781\n",
      "Epoch 102/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0783 - val_loss: 0.0781\n",
      "Epoch 103/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0784 - val_loss: 0.0781\n",
      "Epoch 104/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0783 - val_loss: 0.0783\n",
      "Epoch 105/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0783 - val_loss: 0.0782\n",
      "Epoch 106/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0783 - val_loss: 0.0781\n",
      "Epoch 107/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0782 - val_loss: 0.0781\n",
      "Epoch 108/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0782 - val_loss: 0.0781\n",
      "Epoch 109/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0782 - val_loss: 0.0782\n",
      "Epoch 110/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0783 - val_loss: 0.0781\n",
      "Epoch 111/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0781 - val_loss: 0.0781\n",
      "Epoch 112/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0782 - val_loss: 0.0783\n",
      "Epoch 113/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0782 - val_loss: 0.0781\n",
      "Epoch 114/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0781 - val_loss: 0.0781\n",
      "Epoch 115/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0782 - val_loss: 0.0780\n",
      "Epoch 116/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0782 - val_loss: 0.0780\n",
      "Epoch 117/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0782 - val_loss: 0.0781\n",
      "Epoch 118/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0782 - val_loss: 0.0781\n",
      "Epoch 119/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0781 - val_loss: 0.0781\n",
      "Epoch 120/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0781 - val_loss: 0.0781\n",
      "Epoch 121/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0781 - val_loss: 0.0781\n",
      "Epoch 122/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0781 - val_loss: 0.0781\n",
      "Epoch 123/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0781 - val_loss: 0.0781\n",
      "Epoch 124/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0781 - val_loss: 0.0781\n",
      "Epoch 125/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0780 - val_loss: 0.0783\n",
      "Epoch 126/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0781 - val_loss: 0.0781\n",
      "Epoch 127/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0781 - val_loss: 0.0781\n",
      "Epoch 128/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0781 - val_loss: 0.0780\n",
      "Epoch 129/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0780 - val_loss: 0.0780\n",
      "Epoch 130/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0780 - val_loss: 0.0782\n",
      "Epoch 131/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0780 - val_loss: 0.0782\n",
      "Epoch 132/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0780 - val_loss: 0.0781\n",
      "Epoch 133/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0780 - val_loss: 0.0780\n",
      "Epoch 134/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0779 - val_loss: 0.0781\n",
      "Epoch 135/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0780 - val_loss: 0.0782\n",
      "Epoch 136/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0780 - val_loss: 0.0781\n",
      "Epoch 137/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0780 - val_loss: 0.0781\n",
      "Epoch 138/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0780 - val_loss: 0.0781\n",
      "Epoch 139/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0779 - val_loss: 0.0781\n",
      "Epoch 140/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0779 - val_loss: 0.0781\n",
      "Epoch 141/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0779 - val_loss: 0.0780\n",
      "Epoch 142/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0779 - val_loss: 0.0780\n",
      "Epoch 143/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0779 - val_loss: 0.0782\n",
      "Epoch 144/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0779 - val_loss: 0.0781\n",
      "Epoch 145/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0779 - val_loss: 0.0781\n",
      "Epoch 146/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0779 - val_loss: 0.0781\n",
      "Epoch 147/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - 1s 4ms/step - loss: 0.0779 - val_loss: 0.0781\n",
      "Epoch 148/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0778 - val_loss: 0.0781\n",
      "Epoch 149/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0779 - val_loss: 0.0781\n",
      "Epoch 150/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0778 - val_loss: 0.0782\n",
      "Epoch 151/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0779 - val_loss: 0.0780\n",
      "Epoch 152/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0778 - val_loss: 0.0781\n",
      "Epoch 153/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0778 - val_loss: 0.0781\n",
      "Epoch 154/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0777 - val_loss: 0.0780\n",
      "Epoch 155/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0778 - val_loss: 0.0780\n",
      "Epoch 156/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0779 - val_loss: 0.0781\n",
      "Epoch 157/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0778 - val_loss: 0.0781\n",
      "Epoch 158/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0778 - val_loss: 0.0781\n",
      "Epoch 159/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0778 - val_loss: 0.0782\n",
      "Epoch 160/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0778 - val_loss: 0.0781\n",
      "Epoch 161/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0778 - val_loss: 0.0781\n",
      "Epoch 162/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0778 - val_loss: 0.0781\n",
      "Epoch 163/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0778 - val_loss: 0.0782\n",
      "Epoch 164/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0778 - val_loss: 0.0780\n",
      "Epoch 165/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0777 - val_loss: 0.0781\n",
      "Epoch 166/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0777 - val_loss: 0.0781\n",
      "Epoch 167/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0777 - val_loss: 0.0781\n",
      "Epoch 168/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0778 - val_loss: 0.0781\n",
      "Epoch 169/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0777 - val_loss: 0.0781\n",
      "Epoch 170/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0777 - val_loss: 0.0781\n",
      "Epoch 171/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0776 - val_loss: 0.0781\n",
      "Epoch 172/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0776 - val_loss: 0.0782\n",
      "Epoch 173/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0777 - val_loss: 0.0783\n",
      "Epoch 174/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0777 - val_loss: 0.0781\n",
      "Epoch 175/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0776 - val_loss: 0.0782\n",
      "Epoch 176/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0777 - val_loss: 0.0782\n",
      "Epoch 177/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0776 - val_loss: 0.0782\n",
      "Epoch 178/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0777 - val_loss: 0.0782\n",
      "Epoch 179/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0776 - val_loss: 0.0781\n",
      "Epoch 180/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0776 - val_loss: 0.0782\n",
      "Epoch 181/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 5ms/step - loss: 0.0776 - val_loss: 0.0781\n",
      "Epoch 182/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0776 - val_loss: 0.0781\n",
      "Epoch 183/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0775 - val_loss: 0.0782\n",
      "Epoch 184/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0777 - val_loss: 0.0781\n",
      "Epoch 185/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.081 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0775 - val_loss: 0.0780\n",
      "Epoch 186/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0776 - val_loss: 0.0782\n",
      "Epoch 187/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0776 - val_loss: 0.0781\n",
      "Epoch 188/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0775 - val_loss: 0.0782\n",
      "Epoch 189/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0775 - val_loss: 0.0781\n",
      "Epoch 190/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0775 - val_loss: 0.0781\n",
      "Epoch 191/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0775 - val_loss: 0.0780\n",
      "Epoch 192/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0776 - val_loss: 0.0781\n",
      "Epoch 193/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0775 - val_loss: 0.0781\n",
      "Epoch 194/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0774 - val_loss: 0.0783\n",
      "Epoch 195/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0774 - val_loss: 0.0782\n",
      "Epoch 196/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0775 - val_loss: 0.0781\n",
      "Epoch 197/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0774 - val_loss: 0.0781\n",
      "Epoch 198/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0774 - val_loss: 0.0781\n",
      "Epoch 199/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0775 - val_loss: 0.0782\n",
      "Epoch 200/200\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.077 - 1s 4ms/step - loss: 0.0774 - val_loss: 0.0782\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Dense(100, activation='relu',\n",
    "                      input_shape=(len(feature_names),),\n",
    "                      kernel_initializer=initializers.he_normal(seed=0)))\n",
    "model.add(layers.Dense(100, activation='relu',\n",
    "                      kernel_initializer=initializers.he_normal(seed=0)))\n",
    "model.add(layers.Dropout(0.7))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=\"adam\",)\n",
    "\n",
    "print('start fitting')\n",
    "history = model.fit(dataset_train[feature_names], dataset_train['rank'],\n",
    "                    batch_size=1000, #1000\n",
    "                    epochs=200, #225\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1, )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1afe8e34248>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(history.history['val_loss'][1:])\n",
    "plt.plot(history.history['loss'][1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lightgbm Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\.conda\\envs\\finlab\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "90 fits failed out of a total of 90.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "90 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\.conda\\envs\\finlab\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\User\\.conda\\envs\\finlab\\lib\\site-packages\\lightgbm\\sklearn.py\", line 726, in fit\n",
      "    callbacks=callbacks)\n",
      "  File \"C:\\Users\\User\\.conda\\envs\\finlab\\lib\\site-packages\\lightgbm\\sklearn.py\", line 499, in fit\n",
      "    callbacks=callbacks)\n",
      "  File \"C:\\Users\\User\\.conda\\envs\\finlab\\lib\\site-packages\\lightgbm\\engine.py\", line 191, in train\n",
      "    booster.add_valid(valid_set, name_valid_set)\n",
      "  File \"C:\\Users\\User\\.conda\\envs\\finlab\\lib\\site-packages\\lightgbm\\basic.py\", line 1648, in add_valid\n",
      "    data.construct().handle))\n",
      "  File \"C:\\Users\\User\\.conda\\envs\\finlab\\lib\\site-packages\\lightgbm\\basic.py\", line 934, in construct\n",
      "    silent=self.silent, feature_name=self.feature_name, params=self.params)\n",
      "  File \"C:\\Users\\User\\.conda\\envs\\finlab\\lib\\site-packages\\lightgbm\\basic.py\", line 764, in _lazy_init\n",
      "    self.set_label(label)\n",
      "  File \"C:\\Users\\User\\.conda\\envs\\finlab\\lib\\site-packages\\lightgbm\\basic.py\", line 1244, in set_label\n",
      "    self.set_field('label', label)\n",
      "  File \"C:\\Users\\User\\.conda\\envs\\finlab\\lib\\site-packages\\lightgbm\\basic.py\", line 1098, in set_field\n",
      "    ctypes.c_int(type_data)))\n",
      "  File \"C:\\Users\\User\\.conda\\envs\\finlab\\lib\\site-packages\\lightgbm\\basic.py\", line 44, in _safe_call\n",
      "    raise LightGBMError(decode_string(_LIB.LGBM_GetLastError()))\n",
      "lightgbm.basic.LightGBMError: Length of label is not same with #data\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\User\\.conda\\envs\\finlab\\lib\\site-packages\\sklearn\\model_selection\\_search.py:972: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "  category=UserWarning,\n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "Length of label is not same with #data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2172\\1321628966.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m### 執行參數回測\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;31m### 顯示最佳參數\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\finlab\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    924\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 926\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    927\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\finlab\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    724\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 726\u001b[1;33m                                         callbacks=callbacks)\n\u001b[0m\u001b[0;32m    727\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\finlab\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    497\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\finlab\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mvalid_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_valid_set\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduced_valid_sets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_valid_sets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m             \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_valid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname_valid_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reverse_update_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\finlab\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36madd_valid\u001b[1;34m(self, data, name)\u001b[0m\n\u001b[0;32m   1646\u001b[0m         _safe_call(_LIB.LGBM_BoosterAddValidData(\n\u001b[0;32m   1647\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1648\u001b[1;33m             data.construct().handle))\n\u001b[0m\u001b[0;32m   1649\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalid_sets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1650\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_valid_sets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\finlab\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mconstruct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    932\u001b[0m                     self._lazy_init(self.data, label=self.label, reference=self.reference,\n\u001b[0;32m    933\u001b[0m                                     \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predictor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 934\u001b[1;33m                                     silent=self.silent, feature_name=self.feature_name, params=self.params)\n\u001b[0m\u001b[0;32m    935\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m                     \u001b[1;31m# construct subset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\finlab\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[0;32m    762\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot initialize Dataset from {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Label should not be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\finlab\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mset_label\u001b[1;34m(self, label)\u001b[0m\n\u001b[0;32m   1242\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist_to_1d_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_label_from_pandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1244\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_field\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1245\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\finlab\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mset_field\u001b[1;34m(self, field_name, data)\u001b[0m\n\u001b[0;32m   1096\u001b[0m             \u001b[0mptr_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m             \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m             ctypes.c_int(type_data)))\n\u001b[0m\u001b[0;32m   1099\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\finlab\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_safe_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \"\"\"\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecode_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLGBM_GetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLightGBMError\u001b[0m: Length of label is not same with #data"
     ]
    }
   ],
   "source": [
    "import lightgbm\n",
    "\n",
    "fit_params={\"early_stopping_rounds\":30, \n",
    "            \"eval_metric\" : 'auc', \n",
    "            \"eval_set\" : [test],\n",
    "            'eval_names': ['valid'],\n",
    "            'verbose': 100,\n",
    "            'categorical_feature': 'auto'}\n",
    "\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "param_test ={'num_leaves': sp_randint(6, 50), \n",
    "             'min_child_samples': sp_randint(100, 500), \n",
    "             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "             'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
    "             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
    "             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n",
    "\n",
    "#This parameter defines the number of HP points to be tested\n",
    "n_HP_points_to_test = 30\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "#n_estimators is set to a \"large value\". The actual number of trees build will depend on early stopping and 5000 define only the absolute maximum\n",
    "clf = lgb.LGBMClassifier(max_depth=-1, random_state=314, silent=True, metric='None', n_jobs=4, n_estimators=5000)\n",
    "gs = RandomizedSearchCV(\n",
    "    estimator=clf, param_distributions=param_test, \n",
    "    n_iter=n_HP_points_to_test,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    refit=True,\n",
    "    random_state=314,\n",
    "    verbose=True)\n",
    "\n",
    "### 執行參數回測\n",
    "gs.fit(*train, **fit_params)\n",
    "\n",
    "### 顯示最佳參數\n",
    "print(gs.best_estimator_)\n",
    "# 得分gs.score(*test)\n",
    "###提示音\n",
    "import winsound\n",
    "duration = 1000  # millisecond\n",
    "freq = 440  # Hz\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "cf = lgb.LGBMRegressor(n_estimators=500)\n",
    "cf.fit(dataset_train[feature_names].astype(float), dataset_train['rank'])\n",
    "cf.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "cf2 = RandomForestRegressor(n_estimators=100)\n",
    "cf2.fit(dataset_train[feature_names].astype(float), dataset_train['rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.DataFrame(zip(cf.feature_importances_, feature_names), \n",
    "                           columns=['Value','Feature']).sort_values('Value', ascending=False)\n",
    "feature_imp\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_drop = dataset.dropna(subset=feature_names+['return'])\n",
    "\n",
    "vals = model.predict(dataset_drop[feature_names].astype(float))\n",
    "dataset_drop['result1'] = pd.Series(vals.swapaxes(0,1)[0], dataset_drop.index)\n",
    "\n",
    "vals = cf.predict(dataset_drop[feature_names].astype(float))\n",
    "dataset_drop['result2'] = pd.Series(vals, dataset_drop.index)\n",
    "\n",
    "vals = cf2.predict(dataset_drop[feature_names].astype(float))\n",
    "dataset_drop['result3'] = pd.Series(vals, dataset_drop.index)\n",
    "\n",
    "dataset_drop = dataset_drop.reset_index().set_index(\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "dates = sorted(list(set(dataset_drop.index)))\n",
    "\n",
    "rs = []\n",
    "for d in dates:\n",
    "    \n",
    "    dataset_time = dataset_drop.loc[d]\n",
    "    \n",
    "    dataset_time = drop_extreme_case(dataset_time, \n",
    "        ['bias60', 'bias120', 'bias240', 'mom1', 'mom2', 'mom3', 'mom4', 'mom5', 'mom6','yoy', 'delta_yoy'], thresh=0.01)\n",
    "    \n",
    "    rank = dataset_time['result1'] + dataset_time['result2'] + dataset_time['result3'] \n",
    "    \n",
    "    condition = (rank >= rank.nlargest(20).iloc[-1]) \n",
    "    r = dataset_time['return'][condition].mean()\n",
    "\n",
    "    rs.append(r * (1-3/1000-1.425/1000*2*0.6))\n",
    "\n",
    "rs = pd.Series(rs, index=dates)['2021':].cumprod()\n",
    "\n",
    "s0050 = close['0050']['2021':]\n",
    "\n",
    "pd.DataFrame({'nn strategy return':rs.reindex(s0050.index, method='ffill'), '0050 return':s0050/s0050[0]}).plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 當月持股狀況"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.index.levels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the latest dataset\n",
    "last_date = \"2022-07-15\"#dataset.index.levels[1].max()\n",
    "is_last_date = dataset.index.get_level_values('date') == last_date\n",
    "last_dataset = dataset[is_last_date].copy()\n",
    "\n",
    "\n",
    "last_dataset = drop_extreme_case(last_dataset, \n",
    "    ['bias60', 'bias120', 'bias240', 'mom1', 'mom2', 'mom3', 'mom4', 'mom5', 'mom6','yoy', 'delta_yoy'], thresh=0.01)\n",
    "\n",
    "\n",
    "# remove NaN testcases\n",
    "last_dataset = last_dataset.dropna(subset=feature_names)\n",
    "\n",
    "# predict\n",
    "\n",
    "vals = model.predict(last_dataset[feature_names].astype(float))\n",
    "last_dataset['result1'] = pd.Series(vals.swapaxes(0,1)[0], last_dataset.index)\n",
    "\n",
    "vals = cf.predict(last_dataset[feature_names].astype(float))\n",
    "last_dataset['result2'] = pd.Series(vals, last_dataset.index)\n",
    "\n",
    "vals = cf2.predict(last_dataset[feature_names].astype(float))\n",
    "last_dataset['result3'] = pd.Series(vals, last_dataset.index)\n",
    "\n",
    "# calculate score\n",
    "\n",
    "rank = last_dataset['result1'] + last_dataset['result2'] + last_dataset['result3']\n",
    "condition = (rank >= rank.nlargest(20).iloc[-1]) \n",
    "\n",
    "# plot rank distribution\n",
    "rank.hist(bins=20)\n",
    "\n",
    "\n",
    "# show the best 20 stocks\n",
    "slist1 = rank[condition].reset_index()['stock_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 平均分配資產於股票之中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close = data.get(\"收盤價\")\n",
    "\n",
    "money = 50000*4\n",
    "stock_prices = close[rank[condition].reset_index()['stock_id']].iloc[-1]\n",
    "\n",
    "\n",
    "print(\"股票平分張數:\")\n",
    "money / len(stock_prices) / stock_prices / 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finlab",
   "language": "python",
   "name": "finlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
