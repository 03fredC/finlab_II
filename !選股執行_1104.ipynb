{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 移除不必要的警告\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 獲取歷史資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from finlab.data import Data\n",
    "from finlab.ml import fundamental_features\n",
    "fdf = fundamental_features()\n",
    "\n",
    "data = Data()\n",
    "\n",
    "close = data.get(\"收盤價\")\n",
    "open_ = data.get(\"開盤價\")\n",
    "high = data.get(\"最高價\")\n",
    "low = data.get(\"最低價\")\n",
    "vol = data.get(\"成交股數\")\n",
    "\n",
    "PB = data.get(\"股價淨值比\")\n",
    "pe = data.get(\"本益比\")\n",
    "\n",
    "#close = data.get_adj(\"收盤價\").round(2)\n",
    "\n",
    "#財務指標\n",
    "rev = data.get(\"當月營收\")\n",
    "l_rev = data.get(\"去年當月營收\")\n",
    "\n",
    "#t123 = data.get('土地')\n",
    "\n",
    "#bargin_i=data.get(\"投信買賣超股數\")\n",
    "#bargin_f=data.get(\"外資自營商買賣超股數\")\n",
    "#bargin_s=data.get(\"自營商買賣超股數(自行買賣)\")\n",
    "#\n",
    "\n",
    "rev.index = rev.index.shift(5, \"d\")         #每月頻率\n",
    "#周頻率"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "```\n",
    "https://www.twblogs.net/a/5d3f3173bd9eee517422735f\n",
    "W-WED\n",
    "https://docs.python.org/zh-tw/3/library/calendar.html\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MFI  = data.talib(\"MFI\")\n",
    "##MFI.tail()\n",
    "#ub,mb,lb =data.talib(\"BBANDS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 營收相關"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################　　　自己加入的　　　##############################################\n",
    "import pandas as pd\n",
    "from finlab.__init__ import talib_all_stock\n",
    "from talib import abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias(n):\n",
    "    return close / close.rolling(n, min_periods=1).mean()\n",
    "\n",
    "def acc(n):\n",
    "    return close.shift(n) / (close.shift(2*n) + close) * 2\n",
    "\n",
    "def rsv(n):\n",
    "    l = close.rolling(n, min_periods=1).min()\n",
    "    h = close.rolling(n, min_periods=1).max()\n",
    "    \n",
    "    return (close - l) / (h - l)\n",
    "\n",
    "def mom(n):\n",
    "    return (rev / rev.shift(1)).shift(n)\n",
    "\n",
    "def yoy(n):\n",
    "    return (rev.shift(n) / rev.shift(12+n)) -1\n",
    "\n",
    "\n",
    "features = {\n",
    "    'mom1': mom(1),\n",
    "    'mom2': mom(2),\n",
    "    'mom3': mom(3),\n",
    "    'mom4': mom(4),\n",
    "    'mom5': mom(5),\n",
    "    'mom6': mom(6),\n",
    "    'mom7': mom(7),\n",
    "    'mom8': mom(8),\n",
    "    'mom9': mom(9),\n",
    "    \n",
    "    'bias5': bias(5),\n",
    "    'bias10': bias(10),\n",
    "    'bias20': bias(20),\n",
    "    'bias60': bias(60),\n",
    "    'bias120': bias(120),\n",
    "    'bias240': bias(240),\n",
    "    \n",
    "    'acc5': acc(5),\n",
    "    'acc10': acc(10),\n",
    "    'acc20': acc(20),\n",
    "    'acc60': acc(60),\n",
    "    'acc120': acc(120),\n",
    "    'acc240': acc(240),\n",
    "    \n",
    "    'rsv5': rsv(5),\n",
    "    'rsv10': rsv(10),\n",
    "    'rsv20': rsv(20),\n",
    "    'rsv60': rsv(60),\n",
    "    'rsv120': rsv(120),\n",
    "    'rsv240': rsv(240),\n",
    "###############################################\n",
    "    'yoy': yoy(1),\n",
    "    'delta_yoy':yoy(1)-yoy(2),\n",
    "    \n",
    "    'PB':PB,\n",
    "    'PE':pe,       \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 財報指標"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "兩個feature結合\n",
    "[https://hahow.in/courses/5b9d3a6dca498a001e917383/discussions/5d18b63eac23d80020ae4ce7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finlab.ml import fundamental_features\n",
    "dataset_fundamental = fundamental_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 技術指標"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# https://zhuanlan.zhihu.com/p/342075180 talib函数功能一览表\n",
    "\n",
    "def bias(n):\n",
    "    return close / close.rolling(n, min_periods=1).mean()\n",
    "\n",
    "def acc(n):\n",
    "    return close.shift(n) / (close.shift(2*n) + close) * 2\n",
    "\n",
    "def rsv(n):\n",
    "    l = close.rolling(n, min_periods=1).min()\n",
    "    h = close.rolling(n, min_periods=1).max()\n",
    "    \n",
    "    return (close - l) / (h - l)\n",
    "\n",
    "def mom(n):\n",
    "    return (rev / rev.shift(1)).shift(n)\n",
    "\n",
    "\n",
    "def bi_(n):\n",
    "    return (bargin_i / vol.shift(1)).shift(n)\n",
    "\n",
    "def bf(n):\n",
    "    return (bargin_f / vol.shift(1)).shift(n)\n",
    "    \n",
    "def bs(n):\n",
    "    return (bargin_s / vol.shift(1)).shift(n)\n",
    "\n",
    "def rsi(n):\n",
    "    #return talib_all_stock(ndays=10000, func=abstract.RSI, timeperiod=n)\n",
    "    return data.talib(\"RSI\",timeperiod=n)\n",
    "\n",
    "def MFI(n):\n",
    "    return data.talib(\"MFI\",timeperiod=n)\n",
    "\n",
    "def obv(n):\n",
    "    return data.talib(\"OBV\",timeperiod=n)\n",
    "\n",
    "\n",
    "\n",
    "features = {\n",
    "    \n",
    "    #'ATR14':data.talib(\"ATR\",timeperiod=14),\n",
    "    #'NATR14':data.talib('NATR',timeperiod=14),\n",
    "    #'TRANGE':data.talib('TRANGE'),\n",
    "    #'Adosc3':data.talib('ADOSC',timeperiod=3),\n",
    "    \n",
    "    #\"MFI5\":MFI(5),\n",
    "    #\"MFI10\":MFI(10),\n",
    "    \n",
    "    #'rsi6': rsi(6),  #DataFrame\n",
    "    #'rsi10': rsi(10),  #DataFrame\n",
    "    #'rsi14': rsi(14),  #DataFrame\n",
    "    #'rsi20': rsi(20),  #DataFrame\n",
    "    #'rsi50': rsi(50),  #DataFrame\n",
    "   \n",
    "    'mom1': mom(1),\n",
    "    'mom2': mom(2),\n",
    "    'mom3': mom(3),\n",
    "    'mom4': mom(4),\n",
    "    'mom5': mom(5),\n",
    "    'mom6': mom(6),\n",
    "    'mom7': mom(7),\n",
    "    'mom8': mom(8),\n",
    "    'mom9': mom(9),\n",
    "    \n",
    "    'yoy': yoy(1),\n",
    "    'delta_yoy':yoy(1)-yoy(2),\n",
    "    \n",
    "#    'ff':ff,\n",
    "    'PB':PB,\n",
    "    'PE':pe,   \n",
    "#  \n",
    "    'bias5': bias(5),\n",
    "    'bias10': bias(10),\n",
    "    'bias20': bias(20),\n",
    "    'bias60': bias(60),\n",
    "    'bias120': bias(120),\n",
    "    'bias240': bias(240),\n",
    "    \n",
    "    'acc5': acc(5),\n",
    "    'acc10': acc(10),\n",
    "    'acc20': acc(20),\n",
    "    'acc60': acc(60),\n",
    "    'acc120': acc(120),\n",
    "    'acc240': acc(240),\n",
    "    \n",
    "    #'rsv5': rsv(5),\n",
    "    #'rsv10': rsv(10),\n",
    "    #'rsv20': rsv(20),\n",
    "    #'rsv60': rsv(60),\n",
    "    #'rsv120': rsv(120),\n",
    "    #'rsv240': rsv(240),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 組合dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認各指標清單"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PB',\n",
       " 'PE',\n",
       " 'acc10',\n",
       " 'acc120',\n",
       " 'acc20',\n",
       " 'acc240',\n",
       " 'acc5',\n",
       " 'acc60',\n",
       " 'bias10',\n",
       " 'bias120',\n",
       " 'bias20',\n",
       " 'bias240',\n",
       " 'bias5',\n",
       " 'bias60',\n",
       " 'delta_yoy',\n",
       " 'mom1',\n",
       " 'mom2',\n",
       " 'mom3',\n",
       " 'mom4',\n",
       " 'mom5',\n",
       " 'mom6',\n",
       " 'mom7',\n",
       " 'mom8',\n",
       " 'mom9',\n",
       " 'rsv10',\n",
       " 'rsv120',\n",
       " 'rsv20',\n",
       " 'rsv240',\n",
       " 'rsv5',\n",
       " 'rsv60',\n",
       " 'yoy']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1=sorted(features)\n",
    "list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t1 = data.talib(\"NATR\",timeperiod=14)\n",
    "#t1.to_csv('myfile.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 製作dataset\n",
    "\n",
    "##### 設定買賣頻率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2005-02-15', '2005-03-15', '2005-04-15', '2005-05-15',\n",
       "               '2005-06-15', '2005-07-15', '2005-08-15', '2005-09-15',\n",
       "               '2005-10-15', '2005-11-15',\n",
       "               ...\n",
       "               '2022-02-15', '2022-03-15', '2022-04-15', '2022-05-15',\n",
       "               '2022-06-15', '2022-07-15', '2022-08-15', '2022-09-15',\n",
       "               '2022-10-15', '2022-11-15'],\n",
       "              dtype='datetime64[ns]', name='date', length=214, freq=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rev.index = rev.index.tz_localize(\"Asia/Taipei\")\n",
    "every_month = rev.index\n",
    "every_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 將dataframe 組裝起來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features['bias20'].reindex(every_month, method='ffill')\n",
    "\n",
    "for name, f in features.items():\n",
    "    features[name] = f.reindex(every_month, method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name, f in features.items():\n",
    "    features[name] = f.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################　　　自己加入的　　　##############################################\n",
    "dataset.index = dataset.index.set_names(['stock_id','date'], level=[0,1])\n",
    "\n",
    "\n",
    "#dataset.index.levels[1].name = 'date'\n",
    "#dataset.index.levels[0].name = 'stock_id'\n",
    "\n",
    "#因為你pandas更新到新版了\n",
    "## profit.index.levels[0].name = 'year'\n",
    "## profit.index.levels[1].name = 'month'\n",
    "#這兩行的語法被棄用，請改成\n",
    "#profit.index=profit.index.set_names('year', level=0)\n",
    "#profit.index=profit.index.set_names('month', level=1)\n",
    "#or profit.index=profit.index.set_names(['year','month'], level=[0,1])\n",
    "#直接一行\n",
    "#就可以了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 新增 label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finlab import ml\n",
    "\n",
    "ml.add_profit_prediction(dataset)\n",
    "ml.add_rank_prediction(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 刪除太大太小的歷史資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(436988, 33)\n",
      "(378067, 33)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "\n",
    "def drop_extreme_case(dataset, feature_names, thresh=0.01):\n",
    "    \n",
    "    extreme_cases = pd.Series(False, index=dataset.index)\n",
    "    for f in feature_names:\n",
    "        tf = dataset[f]\n",
    "        extreme_cases = extreme_cases | (tf < tf.quantile(thresh)) | (tf > tf.quantile(1-thresh))\n",
    "    dataset = dataset[~extreme_cases]\n",
    "    return dataset\n",
    "\n",
    "dataset_drop_extreme_case = drop_extreme_case(dataset , list1 , thresh=0.01)\n",
    "\n",
    "print(dataset_drop_extreme_case.shape)\n",
    "\n",
    "##(436560, 27)\n",
    "##(377538, 27)\n",
    "\n",
    "##(505602, 75)\n",
    "##(446580, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dropna = dataset_drop_extreme_case.dropna(how='any')\n",
    "dataset_dropna = dataset_dropna.reset_index().set_index(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2005-02-15', '2005-03-15', '2005-04-15', '2005-05-15',\n",
       "               '2005-06-15', '2005-07-15', '2005-08-15', '2005-09-15',\n",
       "               '2005-10-15', '2005-11-15',\n",
       "               ...\n",
       "               '2021-11-15', '2021-12-15', '2022-02-15', '2022-03-15',\n",
       "               '2022-04-15', '2022-06-15', '2022-08-15', '2022-09-15',\n",
       "               '2022-10-15', '2022-11-15'],\n",
       "              dtype='datetime64[ns]', name='date', length=378067, freq=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_drop_extreme_case.index.get_level_values(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################　　　自己加入的　　　##############################################\n",
    "\n",
    "dataset_dropna.index = pd.to_datetime(dataset_dropna.index)\n",
    "dataset_dropna = dataset_dropna.sort_index()\n",
    "\n",
    "#修復＜class ‘numpy.ndarray‘＞　https://blog.csdn.net/lxbin/article/details/114005757"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset_dropna[:'2020']\n",
    "dataset_test = dataset_dropna['2021':]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 機器學習\n",
    " - 目前只有三個，技術指標也要在增加一下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 100)               3200      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 13,401\n",
      "Trainable params: 13,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "start fitting\n",
      "Epoch 1/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.277 - ETA: 0s - loss: 0.246 - ETA: 0s - loss: 0.215 - ETA: 0s - loss: 0.176 - ETA: 0s - loss: 0.150 - ETA: 0s - loss: 0.138 - 1s 5ms/step - loss: 0.1302 - val_loss: 0.0712\n",
      "Epoch 2/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0737 - val_loss: 0.0712\n",
      "Epoch 3/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0736 - val_loss: 0.0713\n",
      "Epoch 4/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0736 - val_loss: 0.0712\n",
      "Epoch 5/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0711\n",
      "Epoch 6/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0736 - val_loss: 0.0711\n",
      "Epoch 7/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0711\n",
      "Epoch 8/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0712\n",
      "Epoch 9/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0735 - val_loss: 0.0711\n",
      "Epoch 10/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0711\n",
      "Epoch 11/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0710\n",
      "Epoch 12/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0710\n",
      "Epoch 13/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 14/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0711\n",
      "Epoch 15/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0710\n",
      "Epoch 16/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0712\n",
      "Epoch 17/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0710\n",
      "Epoch 18/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0710\n",
      "Epoch 19/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0710\n",
      "Epoch 20/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0710\n",
      "Epoch 21/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0709\n",
      "Epoch 22/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0710\n",
      "Epoch 23/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0709\n",
      "Epoch 24/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0709\n",
      "Epoch 25/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0709\n",
      "Epoch 26/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0709\n",
      "Epoch 27/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0710\n",
      "Epoch 28/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0710\n",
      "Epoch 29/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0709\n",
      "Epoch 30/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0708\n",
      "Epoch 31/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0709\n",
      "Epoch 32/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0709\n",
      "Epoch 33/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0709\n",
      "Epoch 34/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0710\n",
      "Epoch 35/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0709\n",
      "Epoch 36/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0709\n",
      "Epoch 37/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0710\n",
      "Epoch 38/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 39/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0709\n",
      "Epoch 40/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0732 - val_loss: 0.0708\n",
      "Epoch 41/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0709\n",
      "Epoch 42/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 43/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0708\n",
      "Epoch 44/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 45/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0709\n",
      "Epoch 46/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0708\n",
      "Epoch 47/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0709\n",
      "Epoch 48/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0709\n",
      "Epoch 49/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0709\n",
      "Epoch 50/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0713\n",
      "Epoch 51/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0708\n",
      "Epoch 52/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 53/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0708\n",
      "Epoch 54/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0708\n",
      "Epoch 55/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0709\n",
      "Epoch 56/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0708\n",
      "Epoch 57/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0707\n",
      "Epoch 58/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0708\n",
      "Epoch 59/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0709\n",
      "Epoch 60/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 61/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0709\n",
      "Epoch 62/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0712\n",
      "Epoch 63/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0709\n",
      "Epoch 64/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0707\n",
      "Epoch 65/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0709\n",
      "Epoch 66/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0708\n",
      "Epoch 67/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 68/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0708\n",
      "Epoch 69/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0708\n",
      "Epoch 70/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0707\n",
      "Epoch 71/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0708\n",
      "Epoch 72/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0708\n",
      "Epoch 73/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0729 - val_loss: 0.0709\n",
      "Epoch 74/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0710\n",
      "Epoch 75/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 76/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0712\n",
      "Epoch 77/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0708\n",
      "Epoch 78/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0708\n",
      "Epoch 79/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0729 - val_loss: 0.0709\n",
      "Epoch 80/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0712\n",
      "Epoch 81/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0708\n",
      "Epoch 82/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0707\n",
      "Epoch 83/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0708\n",
      "Epoch 84/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0708\n",
      "Epoch 85/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0708\n",
      "Epoch 86/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0709\n",
      "Epoch 87/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0707\n",
      "Epoch 88/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0708\n",
      "Epoch 89/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0710\n",
      "Epoch 90/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0708\n",
      "Epoch 91/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0709\n",
      "Epoch 92/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0707\n",
      "Epoch 93/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0709\n",
      "Epoch 94/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0708\n",
      "Epoch 95/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0708\n",
      "Epoch 96/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0709\n",
      "Epoch 97/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0708\n",
      "Epoch 98/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0707\n",
      "Epoch 99/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0710\n",
      "Epoch 100/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0707\n",
      "Epoch 101/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0714\n",
      "Epoch 102/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0706\n",
      "Epoch 103/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0708\n",
      "Epoch 104/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0707\n",
      "Epoch 105/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0708\n",
      "Epoch 106/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0707\n",
      "Epoch 107/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0706\n",
      "Epoch 108/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0712\n",
      "Epoch 109/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0710\n",
      "Epoch 110/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0708\n",
      "Epoch 111/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0706\n",
      "Epoch 112/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0708\n",
      "Epoch 113/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0708\n",
      "Epoch 114/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0707\n",
      "Epoch 115/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0710\n",
      "Epoch 116/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0707\n",
      "Epoch 117/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0706\n",
      "Epoch 118/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0707\n",
      "Epoch 119/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0707\n",
      "Epoch 120/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0710\n",
      "Epoch 121/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0707\n",
      "Epoch 122/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0706\n",
      "Epoch 123/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 124/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0710\n",
      "Epoch 125/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0708\n",
      "Epoch 126/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0707\n",
      "Epoch 127/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0708\n",
      "Epoch 128/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0708\n",
      "Epoch 129/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0708\n",
      "Epoch 130/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0708\n",
      "Epoch 131/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 132/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0710\n",
      "Epoch 133/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0708\n",
      "Epoch 134/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0709\n",
      "Epoch 135/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0709\n",
      "Epoch 136/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0709\n",
      "Epoch 137/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0706\n",
      "Epoch 138/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0709\n",
      "Epoch 139/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0709\n",
      "Epoch 140/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0708\n",
      "Epoch 141/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0707\n",
      "Epoch 142/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0707\n",
      "Epoch 143/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0725 - val_loss: 0.0708\n",
      "Epoch 144/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0707\n",
      "Epoch 145/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0708\n",
      "Epoch 146/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0708\n",
      "Epoch 147/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0707\n",
      "Epoch 148/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0709\n",
      "Epoch 149/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0709\n",
      "Epoch 150/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0707\n",
      "Epoch 151/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0709\n",
      "Epoch 152/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0706\n",
      "Epoch 153/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0708\n",
      "Epoch 154/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0707\n",
      "Epoch 155/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 156/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0709\n",
      "Epoch 157/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0709\n",
      "Epoch 158/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0709\n",
      "Epoch 159/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 160/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0724 - val_loss: 0.0708\n",
      "Epoch 161/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0709\n",
      "Epoch 162/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0707\n",
      "Epoch 163/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0708\n",
      "Epoch 164/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0707\n",
      "Epoch 165/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0708\n",
      "Epoch 166/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0708\n",
      "Epoch 167/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0709\n",
      "Epoch 168/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0709\n",
      "Epoch 169/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0707\n",
      "Epoch 170/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0707\n",
      "Epoch 171/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0709\n",
      "Epoch 172/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0710\n",
      "Epoch 173/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0711\n",
      "Epoch 174/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0709\n",
      "Epoch 175/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0708\n",
      "Epoch 176/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0723 - val_loss: 0.0708\n",
      "Epoch 177/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0710\n",
      "Epoch 178/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0710\n",
      "Epoch 179/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0709\n",
      "Epoch 180/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0708\n",
      "Epoch 181/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0708\n",
      "Epoch 182/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0709\n",
      "Epoch 183/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0708\n",
      "Epoch 184/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0709\n",
      "Epoch 185/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0711\n",
      "Epoch 186/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0708\n",
      "Epoch 187/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0707\n",
      "Epoch 188/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0709\n",
      "Epoch 189/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0708\n",
      "Epoch 190/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0709\n",
      "Epoch 191/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0709\n",
      "Epoch 192/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0709\n",
      "Epoch 193/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0709\n",
      "Epoch 194/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0707\n",
      "Epoch 195/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0709\n",
      "Epoch 196/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0709\n",
      "Epoch 197/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0709\n",
      "Epoch 198/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 199/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0709\n",
      "Epoch 200/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0710\n",
      "Epoch 201/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0714\n",
      "Epoch 202/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0709\n",
      "Epoch 203/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0708\n",
      "Epoch 204/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0710\n",
      "Epoch 205/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0711\n",
      "Epoch 206/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0711\n",
      "Epoch 207/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0710\n",
      "Epoch 208/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0710\n",
      "Epoch 209/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 210/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0709\n",
      "Epoch 211/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0708\n",
      "Epoch 212/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0710\n",
      "Epoch 213/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0709\n",
      "Epoch 214/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0710\n",
      "Epoch 215/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0708\n",
      "Epoch 216/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0707\n",
      "Epoch 217/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0717\n",
      "Epoch 218/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0708\n",
      "Epoch 219/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0708\n",
      "Epoch 220/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0711\n",
      "Epoch 221/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0710\n",
      "Epoch 222/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0709\n",
      "Epoch 223/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0713\n",
      "Epoch 224/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0711\n",
      "Epoch 225/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0721 - val_loss: 0.0707\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Dense(100, activation='relu',\n",
    "                      input_shape=(len(feature_names),),\n",
    "                      kernel_initializer=initializers.he_normal(seed=0)))\n",
    "model.add(layers.Dense(100, activation='relu',\n",
    "                      kernel_initializer=initializers.he_normal(seed=0)))\n",
    "model.add(layers.Dropout(0.7))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=\"adam\",)\n",
    "\n",
    "print('start fitting')\n",
    "history = model.fit(dataset_train[feature_names], dataset_train['rank'],\n",
    "                    batch_size=1000,         #1000  #每一个batch的大小\n",
    "                    epochs=225, #225          #迭代次数\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    #validation_data =        #(测试集的输入特征，测试集的标签），\n",
    "                    #validation_split =       # 从测试集中划分多少比例给训练集，\n",
    "                    #validation_freq = 20        #测试的epoch间隔数                     \n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x213f1bd0d08>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(history.history['val_loss'][1:])\n",
    "plt.plot(history.history['loss'][1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lightgbm Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---cf.fit---\n",
      "LGBMRegressor(n_estimators=5000)\n",
      "---cf.score---\n",
      "-0.06336072191070974\n",
      "---predict---\n",
      "[0.54563275 0.48796148 0.45147365 ... 0.65265199 0.30549742 0.52895923]\n"
     ]
    }
   ],
   "source": [
    "##############################################　　　自己加入的　　　##############################################\n",
    "import lightgbm as lgb\n",
    "\n",
    "cf = lgb.LGBMRegressor(n_estimators=5000)\n",
    "\n",
    "train = dataset_train[feature_names] , dataset_train['return'] > 1\n",
    "test = dataset_test[feature_names] , dataset_test['return'] > 1 \n",
    "\n",
    "print('---cf.fit---')\n",
    "print(cf.fit(*train))\n",
    "print('---cf.score---')\n",
    "print(cf.score(*test))\n",
    "print('---predict---')\n",
    "print(cf.predict(test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 參數優化_1110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.553323\n",
      "[200]\tvalid's auc: 0.55644\n",
      "Early stopping, best iteration is:\n",
      "[242]\tvalid's auc: 0.558659\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.51523\n",
      "[200]\tvalid's auc: 0.519549\n",
      "Early stopping, best iteration is:\n",
      "[248]\tvalid's auc: 0.521315\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[65]\tvalid's auc: 0.536845\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.557603\n",
      "Early stopping, best iteration is:\n",
      "[132]\tvalid's auc: 0.559263\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.516535\n",
      "Early stopping, best iteration is:\n",
      "[137]\tvalid's auc: 0.520548\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid's auc: 0.539609\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.559728\n",
      "Early stopping, best iteration is:\n",
      "[104]\tvalid's auc: 0.560329\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.512552\n",
      "[200]\tvalid's auc: 0.518755\n",
      "[300]\tvalid's auc: 0.521616\n",
      "Early stopping, best iteration is:\n",
      "[312]\tvalid's auc: 0.522166\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.537725\n",
      "Early stopping, best iteration is:\n",
      "[82]\tvalid's auc: 0.538606\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.55234\n",
      "Early stopping, best iteration is:\n",
      "[155]\tvalid's auc: 0.557158\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.504593\n",
      "[200]\tvalid's auc: 0.509546\n",
      "Early stopping, best iteration is:\n",
      "[176]\tvalid's auc: 0.509633\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.531486\n",
      "Early stopping, best iteration is:\n",
      "[97]\tvalid's auc: 0.532678\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.555432\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.507053\n",
      "[200]\tvalid's auc: 0.511819\n",
      "Early stopping, best iteration is:\n",
      "[198]\tvalid's auc: 0.512358\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalid's auc: 0.540427\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.558979\n",
      "Early stopping, best iteration is:\n",
      "[98]\tvalid's auc: 0.559277\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.51933\n",
      "Early stopping, best iteration is:\n",
      "[105]\tvalid's auc: 0.520339\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[42]\tvalid's auc: 0.539907\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.559804\n",
      "Early stopping, best iteration is:\n",
      "[102]\tvalid's auc: 0.56028\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.516892\n",
      "Early stopping, best iteration is:\n",
      "[162]\tvalid's auc: 0.521154\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid's auc: 0.537742\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.551396\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.5083\n",
      "[200]\tvalid's auc: 0.515411\n",
      "[300]\tvalid's auc: 0.517691\n",
      "[400]\tvalid's auc: 0.519402\n",
      "Early stopping, best iteration is:\n",
      "[434]\tvalid's auc: 0.520403\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[33]\tvalid's auc: 0.533115\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.547917\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.511555\n",
      "[200]\tvalid's auc: 0.517288\n",
      "[300]\tvalid's auc: 0.519906\n",
      "Early stopping, best iteration is:\n",
      "[344]\tvalid's auc: 0.522289\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid's auc: 0.530282\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.553223\n",
      "[200]\tvalid's auc: 0.556927\n",
      "Early stopping, best iteration is:\n",
      "[190]\tvalid's auc: 0.557617\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.516688\n",
      "[200]\tvalid's auc: 0.519824\n",
      "Early stopping, best iteration is:\n",
      "[183]\tvalid's auc: 0.520538\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid's auc: 0.535609\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.557786\n",
      "[200]\tvalid's auc: 0.561596\n",
      "Early stopping, best iteration is:\n",
      "[224]\tvalid's auc: 0.562207\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.517446\n",
      "[200]\tvalid's auc: 0.523658\n",
      "Early stopping, best iteration is:\n",
      "[210]\tvalid's auc: 0.52436\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[33]\tvalid's auc: 0.538877\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.555199\n",
      "Early stopping, best iteration is:\n",
      "[116]\tvalid's auc: 0.556359\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.517927\n",
      "[200]\tvalid's auc: 0.520508\n",
      "Early stopping, best iteration is:\n",
      "[252]\tvalid's auc: 0.521829\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.53436\n",
      "Early stopping, best iteration is:\n",
      "[71]\tvalid's auc: 0.53698\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.552767\n",
      "Early stopping, best iteration is:\n",
      "[112]\tvalid's auc: 0.555505\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.509127\n",
      "[200]\tvalid's auc: 0.51727\n",
      "Early stopping, best iteration is:\n",
      "[242]\tvalid's auc: 0.520284\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.532074\n",
      "Early stopping, best iteration is:\n",
      "[142]\tvalid's auc: 0.536144\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.555939\n",
      "Early stopping, best iteration is:\n",
      "[114]\tvalid's auc: 0.55763\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.518479\n",
      "Early stopping, best iteration is:\n",
      "[141]\tvalid's auc: 0.523541\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid's auc: 0.540102\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[24]\tvalid's auc: 0.549036\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.506941\n",
      "[200]\tvalid's auc: 0.515844\n",
      "Early stopping, best iteration is:\n",
      "[244]\tvalid's auc: 0.518159\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid's auc: 0.536844\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.556052\n",
      "Early stopping, best iteration is:\n",
      "[80]\tvalid's auc: 0.556795\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.514354\n",
      "[200]\tvalid's auc: 0.518923\n",
      "[300]\tvalid's auc: 0.523838\n",
      "Early stopping, best iteration is:\n",
      "[299]\tvalid's auc: 0.52411\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[63]\tvalid's auc: 0.536059\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's auc: 0.552167\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.513582\n",
      "Early stopping, best iteration is:\n",
      "[162]\tvalid's auc: 0.521059\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[39]\tvalid's auc: 0.536313\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.556919\n",
      "Early stopping, best iteration is:\n",
      "[81]\tvalid's auc: 0.558521\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.51106\n",
      "[200]\tvalid's auc: 0.517748\n",
      "Early stopping, best iteration is:\n",
      "[230]\tvalid's auc: 0.519054\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[51]\tvalid's auc: 0.539492\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid's auc: 0.549509\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.501021\n",
      "[200]\tvalid's auc: 0.508395\n",
      "Early stopping, best iteration is:\n",
      "[232]\tvalid's auc: 0.509078\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.52945\n",
      "Early stopping, best iteration is:\n",
      "[78]\tvalid's auc: 0.531333\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid's auc: 0.551898\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.505202\n",
      "[200]\tvalid's auc: 0.510706\n",
      "Early stopping, best iteration is:\n",
      "[201]\tvalid's auc: 0.510926\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.535099\n",
      "Early stopping, best iteration is:\n",
      "[123]\tvalid's auc: 0.536927\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.551529\n",
      "Early stopping, best iteration is:\n",
      "[167]\tvalid's auc: 0.555654\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.500688\n",
      "[200]\tvalid's auc: 0.50658\n",
      "Early stopping, best iteration is:\n",
      "[180]\tvalid's auc: 0.50658\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.529457\n",
      "Early stopping, best iteration is:\n",
      "[145]\tvalid's auc: 0.531343\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid's auc: 0.549715\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.502557\n",
      "[200]\tvalid's auc: 0.510672\n",
      "Early stopping, best iteration is:\n",
      "[244]\tvalid's auc: 0.512912\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.531581\n",
      "Early stopping, best iteration is:\n",
      "[132]\tvalid's auc: 0.536412\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.559213\n",
      "Early stopping, best iteration is:\n",
      "[75]\tvalid's auc: 0.560686\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.519689\n",
      "Early stopping, best iteration is:\n",
      "[120]\tvalid's auc: 0.521932\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[52]\tvalid's auc: 0.54212\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid's auc: 0.553875\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.514832\n",
      "Early stopping, best iteration is:\n",
      "[131]\tvalid's auc: 0.518963\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid's auc: 0.539613\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.557122\n",
      "Early stopping, best iteration is:\n",
      "[113]\tvalid's auc: 0.55911\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.519857\n",
      "Early stopping, best iteration is:\n",
      "[168]\tvalid's auc: 0.524306\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid's auc: 0.53937\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[25]\tvalid's auc: 0.549071\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.499985\n",
      "[200]\tvalid's auc: 0.511336\n",
      "[300]\tvalid's auc: 0.516683\n",
      "Early stopping, best iteration is:\n",
      "[314]\tvalid's auc: 0.517221\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.532738\n",
      "Early stopping, best iteration is:\n",
      "[78]\tvalid's auc: 0.533911\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.550848\n",
      "Early stopping, best iteration is:\n",
      "[80]\tvalid's auc: 0.554161\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.511558\n",
      "[200]\tvalid's auc: 0.518669\n",
      "[300]\tvalid's auc: 0.520682\n",
      "Early stopping, best iteration is:\n",
      "[319]\tvalid's auc: 0.521009\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[44]\tvalid's auc: 0.535327\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.550662\n",
      "Early stopping, best iteration is:\n",
      "[138]\tvalid's auc: 0.552936\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.504622\n",
      "Early stopping, best iteration is:\n",
      "[147]\tvalid's auc: 0.50891\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.531151\n",
      "Early stopping, best iteration is:\n",
      "[112]\tvalid's auc: 0.532054\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.557307\n",
      "Early stopping, best iteration is:\n",
      "[96]\tvalid's auc: 0.557478\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.517281\n",
      "[200]\tvalid's auc: 0.521606\n",
      "Early stopping, best iteration is:\n",
      "[243]\tvalid's auc: 0.525001\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[39]\tvalid's auc: 0.538019\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.556419\n",
      "Early stopping, best iteration is:\n",
      "[148]\tvalid's auc: 0.559063\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.515707\n",
      "[200]\tvalid's auc: 0.52088\n",
      "Early stopping, best iteration is:\n",
      "[173]\tvalid's auc: 0.521972\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.536198\n",
      "Early stopping, best iteration is:\n",
      "[79]\tvalid's auc: 0.538146\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's auc: 0.549462\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.50773\n",
      "[200]\tvalid's auc: 0.517174\n",
      "[300]\tvalid's auc: 0.522895\n",
      "Early stopping, best iteration is:\n",
      "[320]\tvalid's auc: 0.523193\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid's auc: 0.533585\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[65]\tvalid's auc: 0.555261\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.516913\n",
      "[200]\tvalid's auc: 0.525051\n",
      "Early stopping, best iteration is:\n",
      "[200]\tvalid's auc: 0.525051\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[67]\tvalid's auc: 0.54087\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.546221\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.510608\n",
      "[200]\tvalid's auc: 0.51603\n",
      "Early stopping, best iteration is:\n",
      "[219]\tvalid's auc: 0.516769\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[33]\tvalid's auc: 0.535659\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid's auc: 0.547201\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.51439\n",
      "[200]\tvalid's auc: 0.521167\n",
      "[300]\tvalid's auc: 0.524498\n",
      "Early stopping, best iteration is:\n",
      "[317]\tvalid's auc: 0.525037\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[53]\tvalid's auc: 0.536026\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.555911\n",
      "Early stopping, best iteration is:\n",
      "[85]\tvalid's auc: 0.556638\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.517087\n",
      "[200]\tvalid's auc: 0.523138\n",
      "Early stopping, best iteration is:\n",
      "[249]\tvalid's auc: 0.524879\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.533825\n",
      "Early stopping, best iteration is:\n",
      "[76]\tvalid's auc: 0.536403\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.556451\n",
      "[200]\tvalid's auc: 0.56097\n",
      "Early stopping, best iteration is:\n",
      "[172]\tvalid's auc: 0.561114\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.515172\n",
      "[200]\tvalid's auc: 0.522526\n",
      "[300]\tvalid's auc: 0.523292\n",
      "Early stopping, best iteration is:\n",
      "[273]\tvalid's auc: 0.523917\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[53]\tvalid's auc: 0.541989\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.551361\n",
      "Early stopping, best iteration is:\n",
      "[166]\tvalid's auc: 0.557585\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.504217\n",
      "Early stopping, best iteration is:\n",
      "[167]\tvalid's auc: 0.508409\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.532143\n",
      "Early stopping, best iteration is:\n",
      "[98]\tvalid's auc: 0.532605\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.558742\n",
      "[200]\tvalid's auc: 0.562996\n",
      "Early stopping, best iteration is:\n",
      "[210]\tvalid's auc: 0.563786\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.515476\n",
      "[200]\tvalid's auc: 0.520848\n",
      "Early stopping, best iteration is:\n",
      "[197]\tvalid's auc: 0.521431\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[51]\tvalid's auc: 0.538459\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.551206\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.511838\n",
      "Early stopping, best iteration is:\n",
      "[114]\tvalid's auc: 0.512717\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[42]\tvalid's auc: 0.53893\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.556795\n",
      "Early stopping, best iteration is:\n",
      "[143]\tvalid's auc: 0.559964\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.511303\n",
      "[200]\tvalid's auc: 0.520073\n",
      "Early stopping, best iteration is:\n",
      "[181]\tvalid's auc: 0.521306\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.534788\n",
      "Early stopping, best iteration is:\n",
      "[80]\tvalid's auc: 0.534866\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.552148\n",
      "Early stopping, best iteration is:\n",
      "[166]\tvalid's auc: 0.556107\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.508835\n",
      "[200]\tvalid's auc: 0.519658\n",
      "Early stopping, best iteration is:\n",
      "[241]\tvalid's auc: 0.522518\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.532499\n",
      "Early stopping, best iteration is:\n",
      "[75]\tvalid's auc: 0.533375\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's auc: 0.548693\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.50826\n",
      "[200]\tvalid's auc: 0.51867\n",
      "[300]\tvalid's auc: 0.520933\n",
      "Early stopping, best iteration is:\n",
      "[270]\tvalid's auc: 0.521267\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid's auc: 0.535088\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.557272\n",
      "[200]\tvalid's auc: 0.561662\n",
      "Early stopping, best iteration is:\n",
      "[211]\tvalid's auc: 0.562304\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.519687\n",
      "Early stopping, best iteration is:\n",
      "[166]\tvalid's auc: 0.523927\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalid's auc: 0.538284\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.550336\n",
      "Early stopping, best iteration is:\n",
      "[75]\tvalid's auc: 0.553316\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.509911\n",
      "[200]\tvalid's auc: 0.519586\n",
      "Early stopping, best iteration is:\n",
      "[242]\tvalid's auc: 0.523444\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.534421\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid's auc: 0.536688\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.556304\n",
      "[200]\tvalid's auc: 0.560889\n",
      "Early stopping, best iteration is:\n",
      "[236]\tvalid's auc: 0.562324\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.516361\n",
      "Early stopping, best iteration is:\n",
      "[149]\tvalid's auc: 0.521866\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.536506\n",
      "[200]\tvalid's auc: 0.539282\n",
      "Early stopping, best iteration is:\n",
      "[174]\tvalid's auc: 0.540065\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.550681\n",
      "Early stopping, best iteration is:\n",
      "[126]\tvalid's auc: 0.55361\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.506083\n",
      "[200]\tvalid's auc: 0.515684\n",
      "Early stopping, best iteration is:\n",
      "[263]\tvalid's auc: 0.519677\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[39]\tvalid's auc: 0.532683\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.557984\n",
      "[200]\tvalid's auc: 0.561129\n",
      "[300]\tvalid's auc: 0.56265\n",
      "Early stopping, best iteration is:\n",
      "[287]\tvalid's auc: 0.563738\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.516729\n",
      "Early stopping, best iteration is:\n",
      "[169]\tvalid's auc: 0.522235\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.537481\n",
      "Early stopping, best iteration is:\n",
      "[90]\tvalid's auc: 0.538288\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.554996\n",
      "Early stopping, best iteration is:\n",
      "[90]\tvalid's auc: 0.556302\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.511378\n",
      "[200]\tvalid's auc: 0.521256\n",
      "Early stopping, best iteration is:\n",
      "[170]\tvalid's auc: 0.522741\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.537009\n",
      "Early stopping, best iteration is:\n",
      "[95]\tvalid's auc: 0.537327\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.55478\n",
      "Early stopping, best iteration is:\n",
      "[90]\tvalid's auc: 0.556266\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.512403\n",
      "[200]\tvalid's auc: 0.520226\n",
      "Early stopping, best iteration is:\n",
      "[259]\tvalid's auc: 0.521823\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.535513\n",
      "Early stopping, best iteration is:\n",
      "[72]\tvalid's auc: 0.537149\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid's auc: 0.547111\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.502949\n",
      "Early stopping, best iteration is:\n",
      "[158]\tvalid's auc: 0.511279\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.536354\n",
      "Early stopping, best iteration is:\n",
      "[80]\tvalid's auc: 0.537601\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.554521\n",
      "Early stopping, best iteration is:\n",
      "[82]\tvalid's auc: 0.55637\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.514223\n",
      "Early stopping, best iteration is:\n",
      "[111]\tvalid's auc: 0.516482\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[58]\tvalid's auc: 0.537333\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.553541\n",
      "Early stopping, best iteration is:\n",
      "[76]\tvalid's auc: 0.556174\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.518465\n",
      "Early stopping, best iteration is:\n",
      "[110]\tvalid's auc: 0.521183\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[65]\tvalid's auc: 0.534709\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.556093\n",
      "[200]\tvalid's auc: 0.559684\n",
      "Early stopping, best iteration is:\n",
      "[260]\tvalid's auc: 0.560795\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.518814\n",
      "Early stopping, best iteration is:\n",
      "[132]\tvalid's auc: 0.521984\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.534875\n",
      "Early stopping, best iteration is:\n",
      "[76]\tvalid's auc: 0.537055\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.555685\n",
      "[200]\tvalid's auc: 0.558966\n",
      "Early stopping, best iteration is:\n",
      "[188]\tvalid's auc: 0.5599\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.51288\n",
      "[200]\tvalid's auc: 0.522793\n",
      "Early stopping, best iteration is:\n",
      "[201]\tvalid's auc: 0.52296\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.536257\n",
      "Early stopping, best iteration is:\n",
      "[79]\tvalid's auc: 0.53777\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.556187\n",
      "[200]\tvalid's auc: 0.558852\n",
      "Early stopping, best iteration is:\n",
      "[172]\tvalid's auc: 0.559356\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.516468\n",
      "Early stopping, best iteration is:\n",
      "[168]\tvalid's auc: 0.522219\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[46]\tvalid's auc: 0.538249\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.555159\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.506504\n",
      "[200]\tvalid's auc: 0.512906\n",
      "[300]\tvalid's auc: 0.515025\n",
      "[400]\tvalid's auc: 0.517431\n",
      "[500]\tvalid's auc: 0.519803\n",
      "Early stopping, best iteration is:\n",
      "[502]\tvalid's auc: 0.520064\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[44]\tvalid's auc: 0.536832\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid's auc: 0.550183\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.497239\n",
      "[200]\tvalid's auc: 0.508225\n",
      "Early stopping, best iteration is:\n",
      "[228]\tvalid's auc: 0.509032\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.527784\n",
      "Early stopping, best iteration is:\n",
      "[80]\tvalid's auc: 0.530485\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.555455\n",
      "Early stopping, best iteration is:\n",
      "[90]\tvalid's auc: 0.556849\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.515266\n",
      "[200]\tvalid's auc: 0.519509\n",
      "[300]\tvalid's auc: 0.522766\n",
      "Early stopping, best iteration is:\n",
      "[354]\tvalid's auc: 0.523753\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.537964\n",
      "Early stopping, best iteration is:\n",
      "[128]\tvalid's auc: 0.540174\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.555512\n",
      "Early stopping, best iteration is:\n",
      "[108]\tvalid's auc: 0.555986\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.512314\n",
      "Early stopping, best iteration is:\n",
      "[161]\tvalid's auc: 0.519483\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.531995\n",
      "Early stopping, best iteration is:\n",
      "[94]\tvalid's auc: 0.532919\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.555047\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.510564\n",
      "Early stopping, best iteration is:\n",
      "[113]\tvalid's auc: 0.512802\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.53832\n",
      "[200]\tvalid's auc: 0.539479\n",
      "Early stopping, best iteration is:\n",
      "[172]\tvalid's auc: 0.541453\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.555215\n",
      "[200]\tvalid's auc: 0.558435\n",
      "Early stopping, best iteration is:\n",
      "[261]\tvalid's auc: 0.56067\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.515328\n",
      "[200]\tvalid's auc: 0.52145\n",
      "[300]\tvalid's auc: 0.523712\n",
      "Early stopping, best iteration is:\n",
      "[284]\tvalid's auc: 0.523993\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[68]\tvalid's auc: 0.533431\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[25]\tvalid's auc: 0.54764\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.499508\n",
      "Early stopping, best iteration is:\n",
      "[157]\tvalid's auc: 0.509645\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.53105\n",
      "Early stopping, best iteration is:\n",
      "[76]\tvalid's auc: 0.532049\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid's auc: 0.549138\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.503834\n",
      "[200]\tvalid's auc: 0.508504\n",
      "Early stopping, best iteration is:\n",
      "[236]\tvalid's auc: 0.510787\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[55]\tvalid's auc: 0.532608\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.550977\n",
      "[200]\tvalid's auc: 0.555387\n",
      "[300]\tvalid's auc: 0.557986\n",
      "Early stopping, best iteration is:\n",
      "[292]\tvalid's auc: 0.558149\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.512427\n",
      "[200]\tvalid's auc: 0.519035\n",
      "Early stopping, best iteration is:\n",
      "[203]\tvalid's auc: 0.519286\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid's auc: 0.533675\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.552145\n",
      "Early stopping, best iteration is:\n",
      "[153]\tvalid's auc: 0.557041\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.5089\n",
      "[200]\tvalid's auc: 0.517464\n",
      "Early stopping, best iteration is:\n",
      "[185]\tvalid's auc: 0.518472\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalid's auc: 0.533566\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.55442\n",
      "Early stopping, best iteration is:\n",
      "[162]\tvalid's auc: 0.557644\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.515733\n",
      "Early stopping, best iteration is:\n",
      "[166]\tvalid's auc: 0.520732\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.534811\n",
      "Early stopping, best iteration is:\n",
      "[75]\tvalid's auc: 0.53539\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.551359\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.510157\n",
      "[200]\tvalid's auc: 0.514114\n",
      "Early stopping, best iteration is:\n",
      "[211]\tvalid's auc: 0.514742\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[52]\tvalid's auc: 0.538124\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.54717\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.5116\n",
      "[200]\tvalid's auc: 0.522373\n",
      "Early stopping, best iteration is:\n",
      "[247]\tvalid's auc: 0.525309\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.536223\n",
      "Early stopping, best iteration is:\n",
      "[111]\tvalid's auc: 0.53712\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.550887\n",
      "Early stopping, best iteration is:\n",
      "[160]\tvalid's auc: 0.555627\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.513938\n",
      "[200]\tvalid's auc: 0.519678\n",
      "Early stopping, best iteration is:\n",
      "[214]\tvalid's auc: 0.520236\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.533711\n",
      "Early stopping, best iteration is:\n",
      "[78]\tvalid's auc: 0.53548\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.558487\n",
      "Early stopping, best iteration is:\n",
      "[96]\tvalid's auc: 0.558719\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.519007\n",
      "Early stopping, best iteration is:\n",
      "[165]\tvalid's auc: 0.525632\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.535018\n",
      "Early stopping, best iteration is:\n",
      "[78]\tvalid's auc: 0.536561\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.554881\n",
      "Early stopping, best iteration is:\n",
      "[155]\tvalid's auc: 0.557442\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.516921\n",
      "[200]\tvalid's auc: 0.522565\n",
      "Early stopping, best iteration is:\n",
      "[199]\tvalid's auc: 0.522649\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[52]\tvalid's auc: 0.537174\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.554777\n",
      "Early stopping, best iteration is:\n",
      "[108]\tvalid's auc: 0.555687\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.508927\n",
      "[200]\tvalid's auc: 0.517785\n",
      "Early stopping, best iteration is:\n",
      "[230]\tvalid's auc: 0.519759\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid's auc: 0.534293\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.555299\n",
      "Early stopping, best iteration is:\n",
      "[75]\tvalid's auc: 0.555826\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.512843\n",
      "[200]\tvalid's auc: 0.519501\n",
      "Early stopping, best iteration is:\n",
      "[179]\tvalid's auc: 0.520073\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.537207\n",
      "[200]\tvalid's auc: 0.538359\n",
      "Early stopping, best iteration is:\n",
      "[173]\tvalid's auc: 0.539274\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.55979\n",
      "[200]\tvalid's auc: 0.563747\n",
      "Early stopping, best iteration is:\n",
      "[186]\tvalid's auc: 0.56465\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.519985\n",
      "[200]\tvalid's auc: 0.525411\n",
      "Early stopping, best iteration is:\n",
      "[197]\tvalid's auc: 0.525911\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.538817\n",
      "Early stopping, best iteration is:\n",
      "[77]\tvalid's auc: 0.539248\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.555738\n",
      "Early stopping, best iteration is:\n",
      "[79]\tvalid's auc: 0.556535\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.518727\n",
      "[200]\tvalid's auc: 0.522688\n",
      "[300]\tvalid's auc: 0.524114\n",
      "Early stopping, best iteration is:\n",
      "[293]\tvalid's auc: 0.524805\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[44]\tvalid's auc: 0.539038\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.55622\n",
      "Early stopping, best iteration is:\n",
      "[110]\tvalid's auc: 0.556932\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.512674\n",
      "[200]\tvalid's auc: 0.518161\n",
      "Early stopping, best iteration is:\n",
      "[248]\tvalid's auc: 0.520328\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[55]\tvalid's auc: 0.534086\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.553828\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.505518\n",
      "[200]\tvalid's auc: 0.512184\n",
      "[300]\tvalid's auc: 0.514153\n",
      "[400]\tvalid's auc: 0.516334\n",
      "[500]\tvalid's auc: 0.518325\n",
      "[600]\tvalid's auc: 0.520311\n",
      "Early stopping, best iteration is:\n",
      "[610]\tvalid's auc: 0.520454\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.539133\n",
      "Early stopping, best iteration is:\n",
      "[143]\tvalid's auc: 0.541132\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.558948\n",
      "Early stopping, best iteration is:\n",
      "[83]\tvalid's auc: 0.561517\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.518724\n",
      "Early stopping, best iteration is:\n",
      "[122]\tvalid's auc: 0.520903\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[61]\tvalid's auc: 0.539377\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.546504\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.514774\n",
      "Early stopping, best iteration is:\n",
      "[72]\tvalid's auc: 0.516453\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.536377\n",
      "Early stopping, best iteration is:\n",
      "[124]\tvalid's auc: 0.53741\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.556253\n",
      "[200]\tvalid's auc: 0.564531\n",
      "Early stopping, best iteration is:\n",
      "[206]\tvalid's auc: 0.564628\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.52033\n",
      "Early stopping, best iteration is:\n",
      "[167]\tvalid's auc: 0.525754\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[33]\tvalid's auc: 0.541246\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid's auc: 0.552445\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.513794\n",
      "[200]\tvalid's auc: 0.52007\n",
      "Early stopping, best iteration is:\n",
      "[220]\tvalid's auc: 0.521184\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.534682\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid's auc: 0.536417\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.559003\n",
      "Early stopping, best iteration is:\n",
      "[109]\tvalid's auc: 0.559959\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.516845\n",
      "[200]\tvalid's auc: 0.52273\n",
      "Early stopping, best iteration is:\n",
      "[238]\tvalid's auc: 0.523676\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.535658\n",
      "Early stopping, best iteration is:\n",
      "[77]\tvalid's auc: 0.536908\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.549932\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.509747\n",
      "[200]\tvalid's auc: 0.51423\n",
      "[300]\tvalid's auc: 0.518752\n",
      "[400]\tvalid's auc: 0.52276\n",
      "Early stopping, best iteration is:\n",
      "[387]\tvalid's auc: 0.523122\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.534589\n",
      "Early stopping, best iteration is:\n",
      "[133]\tvalid's auc: 0.53736\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.555281\n",
      "Early stopping, best iteration is:\n",
      "[114]\tvalid's auc: 0.556641\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.5096\n",
      "Early stopping, best iteration is:\n",
      "[141]\tvalid's auc: 0.514906\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[65]\tvalid's auc: 0.534849\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.55325\n",
      "[200]\tvalid's auc: 0.559222\n",
      "Early stopping, best iteration is:\n",
      "[179]\tvalid's auc: 0.559814\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.513583\n",
      "Early stopping, best iteration is:\n",
      "[140]\tvalid's auc: 0.516417\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.535085\n",
      "[200]\tvalid's auc: 0.538505\n",
      "Early stopping, best iteration is:\n",
      "[257]\tvalid's auc: 0.540603\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.554217\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.506631\n",
      "[200]\tvalid's auc: 0.515\n",
      "Early stopping, best iteration is:\n",
      "[227]\tvalid's auc: 0.516978\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.539019\n",
      "[200]\tvalid's auc: 0.539642\n",
      "Early stopping, best iteration is:\n",
      "[173]\tvalid's auc: 0.540252\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.557132\n",
      "[200]\tvalid's auc: 0.559869\n",
      "Early stopping, best iteration is:\n",
      "[242]\tvalid's auc: 0.562284\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.510276\n",
      "[200]\tvalid's auc: 0.519572\n",
      "Early stopping, best iteration is:\n",
      "[245]\tvalid's auc: 0.521744\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid's auc: 0.533811\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[10]\tvalid's auc: 0.546946\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.511435\n",
      "Early stopping, best iteration is:\n",
      "[149]\tvalid's auc: 0.514146\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid's auc: 0.539278\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.546473\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.513521\n",
      "[200]\tvalid's auc: 0.519112\n",
      "[300]\tvalid's auc: 0.521894\n",
      "Early stopping, best iteration is:\n",
      "[270]\tvalid's auc: 0.52243\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.539189\n",
      "Early stopping, best iteration is:\n",
      "[135]\tvalid's auc: 0.541471\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.548102\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.509263\n",
      "[200]\tvalid's auc: 0.517218\n",
      "Early stopping, best iteration is:\n",
      "[222]\tvalid's auc: 0.518088\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.533754\n",
      "Early stopping, best iteration is:\n",
      "[80]\tvalid's auc: 0.534885\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.532797\n",
      "[200]\tvalid's auc: 0.540861\n",
      "[300]\tvalid's auc: 0.543798\n",
      "Early stopping, best iteration is:\n",
      "[309]\tvalid's auc: 0.544098\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=LGBMClassifier(metric='None', n_estimators=5000,\n",
       "                                            n_jobs=4, random_state=314),\n",
       "                   n_iter=100,\n",
       "                   param_distributions={'colsample_bytree': <scipy.stats._distn_infrastructure.rv_frozen object at 0x00000213F8F4B608>,\n",
       "                                        'min_child_samples': <scipy.stats._distn_infrastructure.rv_frozen object at 0x00000213F8F65B48>,\n",
       "                                        'min_child_weight': [1e-05, 0.001, 0.01,\n",
       "                                                             0.1, 1, 10.0,\n",
       "                                                             100.0, 1000.0,\n",
       "                                                             10000.0],\n",
       "                                        'num_leaves': <scipy.stats._distn_infrastructure.rv_frozen object at 0x00000213F8F65848>,\n",
       "                                        'reg_alpha': [0, 0.1, 1, 2, 5, 7, 10,\n",
       "                                                      50, 100],\n",
       "                                        'reg_lambda': [0, 0.1, 1, 5, 10, 20, 50,\n",
       "                                                       100],\n",
       "                                        'subsample': <scipy.stats._distn_infrastructure.rv_frozen object at 0x00000213F8F65CC8>},\n",
       "                   random_state=314, scoring='roc_auc', verbose=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import lightgbm\n",
    "\n",
    "fit_params={\"early_stopping_rounds\":30, \n",
    "            \"eval_metric\" : 'auc', \n",
    "            \"eval_set\" : [test],\n",
    "            'eval_names': ['valid'],\n",
    "            'verbose': 100,\n",
    "            'categorical_feature': 'auto'}\n",
    "\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "param_test ={'num_leaves': sp_randint(6, 50), \n",
    "             'min_child_samples': sp_randint(100, 500), \n",
    "             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "             'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
    "             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
    "             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n",
    "\n",
    "#This parameter defines the number of HP points to be tested\n",
    "n_HP_points_to_test = 100\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "#n_estimators is set to a \"large value\". The actual number of trees build will depend on early stopping and 5000 define only the absolute maximum\n",
    "clf = lgb.LGBMClassifier(max_depth=-1, random_state=314, silent=True, metric='None', n_jobs=4, n_estimators=5000)\n",
    "gs = RandomizedSearchCV(\n",
    "    estimator=clf, param_distributions=param_test, \n",
    "    n_iter=n_HP_points_to_test,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    refit=True,\n",
    "    random_state=314,\n",
    "    verbose=True)\n",
    "\n",
    "gs.fit(*train, **fit_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(colsample_bytree=0.4250155532986693, metric='None',\n",
       "               min_child_samples=133, min_child_weight=1000.0,\n",
       "               n_estimators=5000, n_jobs=4, num_leaves=12, random_state=314,\n",
       "               reg_alpha=5, reg_lambda=50, subsample=0.9109517224525399)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#gs.best_estimator_\n",
    "#LGBMClassifier(colsample_bytree=0.6433117836032942, metric='None',\n",
    "#               min_child_samples=224, min_child_weight=1e-05, n_estimators=5000,\n",
    "#               n_jobs=4, num_leaves=20, random_state=314, reg_alpha=10,\n",
    "#               reg_lambda=10, subsample=0.8945613420997809)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.541224\n",
      "[200]\tvalid's auc: 0.545207\n",
      "Early stopping, best iteration is:\n",
      "[185]\tvalid's auc: 0.545917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.011242291509219182"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf = lgb.LGBMRegressor(colsample_bytree=0.4893610123694421, metric='None',\n",
    "               min_child_samples=325, n_estimators=5000, n_jobs=4,\n",
    "               num_leaves=44, random_state=314, reg_alpha=7, reg_lambda=20,\n",
    "               subsample=0.3338474163716765)\n",
    "\n",
    "cf.fit(dataset_train[feature_names],dataset_train['return'] > 1, **fit_params)\n",
    "cf.score(dataset_test[feature_names],dataset_test['return'] > 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Training until validation scores don't improve for 30 rounds.\n",
    "[100]\tvalid's auc: 0.551389\n",
    "Early stopping, best iteration is:\n",
    "[89]\tvalid's auc: 0.553699\n",
    "0.003857956228475623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(cf.fit(*train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import lightgbm as lgb\n",
    "#gbm = lgb.LGBMClassifier(n_estimators=100, random_state=5, learning_rate=0.01)\n",
    "#gbm.fit(dataset_train[feature_names], dataset_train['return'] > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tqdm\n",
    "#\n",
    "#n = 3\n",
    "#\n",
    "#X = []\n",
    "#y = []\n",
    "#indexes = []\n",
    "#dataset_scaled_x = dataset_scaled[feature_names]\n",
    "#\n",
    "#for i in tqdm.tqdm_notebook(range(0, len(dataset_scaled)-n)):\n",
    "#    X.append(dataset_scaled_x.iloc[i:i+n].values)\n",
    "#    y.append(dataset_scaled['return'].iloc[i+n-1])\n",
    "#    indexes.append(dataset_scaled.index[i+n-1])\n",
    "##dataset_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#X = np.array(X)\n",
    "#y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import lightgbm as lgb\n",
    "#cf = lgb.LGBMRegressor(colsample_bytree=0.7740467183023685, metric='None',\n",
    "#               min_child_samples=395, min_child_weight=0.01, n_estimators=5000,\n",
    "#               n_jobs=4, num_leaves=9, random_state=314, reg_alpha=5,\n",
    "#               reg_lambda=10, subsample=0.4643892520208455)\n",
    "#    \n",
    "#cf.fit(dataset_train[feature_names].astype(float), dataset_train['rank'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "cf2 = RandomForestRegressor(n_estimators=100)\n",
    "cf2.fit(dataset_train[feature_names].astype(float), dataset_train['rank'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 參數優化_1110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint \n",
    "from sklearn.model_selection import RandomizedSearchCV \n",
    "# build a classifier \n",
    "clf = RandomForestRegressor(n_estimators=100) \n",
    "# specify parameters and distributions to sample from \n",
    "param_dist = {\"max_depth\": [3, None], \n",
    "              \"max_features\": sp_randint(1, 11), \n",
    "              \"min_samples_split\": sp_randint(2, 11), \n",
    "              \"min_samples_leaf\": sp_randint(1, 11), \n",
    "              \"bootstrap\": [True, False], \n",
    "              \"criterion\": [\"mse\", \"mae\"]} \n",
    "# run randomized search \n",
    "n_iter_search = 20 \n",
    "rs = RandomizedSearchCV(clf, param_distributions=param_dist, \n",
    "                                   n_iter=n_iter_search) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.fit(dataset_train[features], dataset_train['return'] > 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split Train Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.DataFrame(zip(cf.feature_importances_, feature_names), \n",
    "                           columns=['Value','Feature']).sort_values('Value', ascending=False)\n",
    "feature_imp\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = dataset.index.get_level_values('date') < '2021'\n",
    "dataset_train = dataset[select]\n",
    "dataset_test = dataset[~select]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_drop = dataset.dropna(subset=feature_names+['return'])\n",
    "\n",
    "vals = model.predict(dataset_drop[feature_names].astype(float))\n",
    "dataset_drop['result1'] = pd.Series(vals.swapaxes(0,1)[0], dataset_drop.index)\n",
    "\n",
    "vals = cf.predict(dataset_drop[feature_names].astype(float))\n",
    "dataset_drop['result2'] = pd.Series(vals, dataset_drop.index)\n",
    "\n",
    "vals = cf2.predict(dataset_drop[feature_names].astype(float))\n",
    "dataset_drop['result3'] = pd.Series(vals, dataset_drop.index)\n",
    "\n",
    "dataset_drop = dataset_drop.reset_index().set_index(\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把量加進來做篩選\n",
    " * https://hahow.in/courses/5b9d3a6dca498a001e917383/shapeussions/60c96f5b018697e8a6131cbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把量加進來\n",
    "vol=data.get('成交股數')/1000\n",
    "vol_ma5=vol.rolling(5).mean()\n",
    "\n",
    "vol_filter=vol_ma5>1000\n",
    "vol_filter=vol_filter[vol_filter].fillna(0)\n",
    "#vol_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#t1 = vol_ma5.iloc[-1].dropna()\n",
    "#t1.to_csv('./tmp/132.csv')\n",
    "#t1.hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "dates = sorted(list(set(dataset_drop.index)))\n",
    "\n",
    "rs = []\n",
    "for d in dates:\n",
    "    \n",
    "    dataset_time = dataset_drop.loc[d]\n",
    "    \n",
    "    dataset_time = drop_extreme_case(dataset_time , list1 , thresh=0.01)\n",
    "    \n",
    "    rank = (dataset_time['result1'] + dataset_time['result2'] + dataset_time['result3'] ) #* vol_filter.loc[d]\n",
    "\n",
    "\n",
    "    condition = (rank >= rank.nlargest(20).iloc[-1]) \n",
    "    \n",
    "    r = dataset_time['return'][condition].mean()\n",
    "\n",
    "    rs.append(r * (1-3/1000-1.425/1000*2*0.6))\n",
    "\n",
    "rs = pd.Series(rs, index=dates)['2021':].cumprod()\n",
    "\n",
    "s0050 = close['0050']['2021':]\n",
    "\n",
    "pd.DataFrame({'nn strategy return':rs.reindex(s0050.index, method='ffill'), '0050 return':s0050/s0050[0]}).plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#\n",
    "#return_history_1026 = pd.Series(rs, index=dates)['2021':].cumprod()\n",
    "##eq = (gain[hold == 1].mean(axis=1)).fillna(1).cumprod()\n",
    "#\n",
    "#pickle.dump(rs, open('return_history_1026.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyfolio as pf\n",
    "#import pandas as pd\n",
    "#\n",
    "#close.index = close.index.tz_localize(\"Asia/Taipei\")\n",
    "##pf.create_returns_tear_sheet(close['0050'].pct_change())\n",
    "#\n",
    "## 得到 上一個單元的 回測結果\n",
    "#ret = pickle.load(open(\"return_history_1026.pkl\", \"rb\"))\n",
    "#\n",
    "## 將回測報酬率取出來\n",
    "#ret = ret.pct_change().dropna()\n",
    "#ret.index = pd.to_datetime(ret.index).tz_localize('Asia/Taipei')\n",
    "#\n",
    "## 利用pyfolio 比較報酬率\n",
    "#pf.create_returns_tear_sheet(ret, benchmark_rets=close['0050'].pct_change())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 當月持股狀況"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.index.levels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the latest dataset\n",
    "last_date = dataset.index.levels[1].max()#\"2022-10-15\"\n",
    "is_last_date = dataset.index.get_level_values('date') == last_date\n",
    "last_dataset = dataset[is_last_date].copy()\n",
    "\n",
    "\n",
    "last_dataset = drop_extreme_case(last_dataset, list1 , thresh=0.01)\n",
    "\n",
    "\n",
    "# remove NaN testcases\n",
    "last_dataset = last_dataset.dropna(subset=feature_names)\n",
    "\n",
    "# predict\n",
    "\n",
    "vals = model.predict(last_dataset[feature_names].astype(float))\n",
    "last_dataset['result1'] = pd.Series(vals.swapaxes(0,1)[0], last_dataset.index)\n",
    "\n",
    "vals = cf.predict(last_dataset[feature_names].astype(float))\n",
    "last_dataset['result2'] = pd.Series(vals, last_dataset.index)\n",
    "\n",
    "vals = cf2.predict(last_dataset[feature_names].astype(float))\n",
    "last_dataset['result3'] = pd.Series(vals, last_dataset.index)\n",
    "\n",
    "\n",
    "# calculate score\n",
    "\n",
    "rank = last_dataset['result1'] + last_dataset['result2'] + last_dataset['result3']\n",
    "\n",
    "#\n",
    "rank = rank * vol_filter.iloc[-1] #******加上量的濾網\n",
    "\n",
    "condition = (rank >= rank.nlargest(20).iloc[-1])\n",
    "#vol_filter\n",
    "\n",
    "# plot rank distribution\n",
    "rank.hist(bins=20)\n",
    "\n",
    "\n",
    "# show the best 20 stocks\n",
    "slist1 = rank[condition].reset_index()['stock_id']\n",
    "\n",
    "#https://keras-cn.readthedocs.io/en/latest/models/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date = dataset.index.levels[1].max()#\"2022-10-15\"\n",
    "is_last_date = dataset.index.get_level_values('date') == last_date\n",
    "last_dataset = dataset[is_last_date].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank.sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 平均分配資產於股票之中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close = data.get(\"收盤價\")\n",
    "\n",
    "money = 60000\n",
    "stock_prices = close[slist1].iloc[-1]\n",
    "\n",
    "\n",
    "print(\"股票平分張數:\")\n",
    "money / len(stock_prices) / stock_prices / 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@deathbeds/ipydrawio": {
   "xml": "<mxfile host=\"17-0536659-02\" modified=\"2022-10-27T03:01:05.740Z\" agent=\"5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\" etag=\"8bODyUWCdQaexky56D9k\" version=\"20.2.8\" type=\"embed\"><diagram id=\"9nsO6hMlLNMTvIbReD-d\" name=\"第1頁\"><mxGraphModel dx=\"1458\" dy=\"721\" grid=\"1\" gridSize=\"10\" guides=\"1\" tooltips=\"1\" connect=\"1\" arrows=\"1\" fold=\"1\" page=\"1\" pageScale=\"1\" pageWidth=\"827\" pageHeight=\"1169\" math=\"0\" shadow=\"0\"><root><mxCell id=\"0\"/><mxCell id=\"1\" parent=\"0\"/><UserObject label=\"Tree Root\" treeRoot=\"1\" id=\"2\"><mxCell style=\"align=center;collapsible=0;container=1;recursiveResize=0;\" parent=\"1\" vertex=\"1\"><mxGeometry x=\"40\" y=\"40\" width=\"120\" height=\"60\" as=\"geometry\"/></mxCell></UserObject></root></mxGraphModel></diagram></mxfile>"
  },
  "kernelspec": {
   "display_name": "finlab",
   "language": "python",
   "name": "finlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
