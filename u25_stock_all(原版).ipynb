{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 獲取歷史資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finlab.data import Data\n",
    "\n",
    "data = Data()\n",
    "\n",
    "rev = data.get(\"當月營收\")\n",
    "close = data.get(\"收盤價\")\n",
    "\n",
    "rev.index = rev.index.shift(5, \"d\") #營收公佈日10號後的5天，15號"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias(n):\n",
    "    return close / close.rolling(n, min_periods=1).mean()\n",
    "\n",
    "def acc(n):\n",
    "    return close.shift(n) / (close.shift(2*n) + close) * 2\n",
    "\n",
    "def rsv(n):\n",
    "    l = close.rolling(n, min_periods=1).min()\n",
    "    h = close.rolling(n, min_periods=1).max()\n",
    "    \n",
    "    return (close - l) / (h - l)\n",
    "\n",
    "def mom(n):\n",
    "    return (rev / rev.shift(1)).shift(n)\n",
    "\n",
    "features = {\n",
    "    'mom1': mom(1),\n",
    "    'mom2': mom(2),\n",
    "    'mom3': mom(3),\n",
    "    'mom4': mom(4),\n",
    "    'mom5': mom(5),\n",
    "    'mom6': mom(6),\n",
    "    'mom7': mom(7),\n",
    "    'mom8': mom(8),\n",
    "    'mom9': mom(9),\n",
    "    \n",
    "    'bias5': bias(5),\n",
    "    'bias10': bias(10),\n",
    "    'bias20': bias(20),\n",
    "    'bias60': bias(60),\n",
    "    'bias120': bias(120),\n",
    "    'bias240': bias(240),\n",
    "    \n",
    "    'acc5': acc(5),\n",
    "    'acc10': acc(10),\n",
    "    'acc20': acc(20),\n",
    "    'acc60': acc(60),\n",
    "    'acc120': acc(120),\n",
    "    'acc240': acc(240),\n",
    "    \n",
    "    'rsv5': rsv(5),\n",
    "    'rsv10': rsv(10),\n",
    "    'rsv20': rsv(20),\n",
    "    'rsv60': rsv(60),\n",
    "    'rsv120': rsv(120),\n",
    "    'rsv240': rsv(240),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 製作dataset\n",
    "\n",
    "##### 設定買賣頻率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2005-02-15', '2005-03-15', '2005-04-15', '2005-05-15',\n",
       "               '2005-06-15', '2005-07-15', '2005-08-15', '2005-09-15',\n",
       "               '2005-10-15', '2005-11-15',\n",
       "               ...\n",
       "               '2022-08-15', '2022-09-15', '2022-10-15', '2022-11-15',\n",
       "               '2022-12-15', '2023-01-15', '2023-02-15', '2023-03-15',\n",
       "               '2023-04-15', '2023-05-15'],\n",
       "              dtype='datetime64[ns]', name='date', length=220, freq=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "every_month = rev.index\n",
    "every_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 將dataframe 組裝起來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features['bias20'].reindex(every_month, method='ffill')\n",
    "\n",
    "for name, f in features.items():\n",
    "    features[name] = f.reindex(every_month, method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name, f in features.items():\n",
    "    features[name] = f.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(dataset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 新增 label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Grouper and axis must be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfinlab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ml\n\u001b[1;32m----> 3\u001b[0m \u001b[43mml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_profit_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m ml\u001b[38;5;241m.\u001b[39madd_rank_prediction(dataset)\n",
      "File \u001b[1;32mD:\\python上課\\finlab_II\\finlab\\ml.py:368\u001b[0m, in \u001b[0;36madd_profit_prediction\u001b[1;34m(dataset, n)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_profit_prediction\u001b[39m(dataset, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    366\u001b[0m     dates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mreset_index()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m])))\n\u001b[1;32m--> 368\u001b[0m     adj_open \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_adj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m開盤價\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    370\u001b[0m     tomorrow_adj_open \u001b[38;5;241m=\u001b[39m adj_open\u001b[38;5;241m.\u001b[39mshift(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    371\u001b[0m     tomorrow_adj_open \u001b[38;5;241m=\u001b[39m tomorrow_adj_open\u001b[38;5;241m.\u001b[39mreindex(dates, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbfill\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mD:\\python上課\\finlab_II\\finlab\\data.py:119\u001b[0m, in \u001b[0;36mData.get_adj\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    117\u001b[0m ratio2 \u001b[38;5;241m=\u001b[39m adj_holiday(item, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwse_cap_divide_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    118\u001b[0m ratio3 \u001b[38;5;241m=\u001b[39m adj_holiday(item, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124motc_divide_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m--> 119\u001b[0m ratio4 \u001b[38;5;241m=\u001b[39m \u001b[43madj_holiday\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtwse_divide_ratio\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m divide_ratio \u001b[38;5;241m=\u001b[39m ( ratio1\u001b[38;5;241m.\u001b[39mreindex_like(item)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    122\u001b[0m  \u001b[38;5;241m*\u001b[39mratio2\u001b[38;5;241m.\u001b[39mreindex_like(item)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    123\u001b[0m  \u001b[38;5;241m*\u001b[39mratio3\u001b[38;5;241m.\u001b[39mreindex_like(item)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    124\u001b[0m  \u001b[38;5;241m*\u001b[39mratio4\u001b[38;5;241m.\u001b[39mreindex_like(item)\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcumprod()\n\u001b[0;32m    126\u001b[0m divide_ratio[np\u001b[38;5;241m.\u001b[39misinf(divide_ratio)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mD:\\python上課\\finlab_II\\finlab\\data.py:111\u001b[0m, in \u001b[0;36mData.get_adj.<locals>.adj_holiday\u001b[1;34m(item, df)\u001b[0m\n\u001b[0;32m    109\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mreindex(df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39munion(item\u001b[38;5;241m.\u001b[39mindex))\n\u001b[0;32m    110\u001b[0m group \u001b[38;5;241m=\u001b[39m all_index\u001b[38;5;241m.\u001b[39misin(item\u001b[38;5;241m.\u001b[39mindex)\u001b[38;5;241m.\u001b[39mcumsum()\n\u001b[1;32m--> 111\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m    112\u001b[0m df\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m item\u001b[38;5;241m.\u001b[39mindex\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32m~\\.conda\\envs\\finlabG\\lib\\site-packages\\pandas\\core\\frame.py:7626\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[0;32m   7622\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[0;32m   7624\u001b[0m \u001b[38;5;66;03m# error: Argument \"squeeze\" to \"DataFrameGroupBy\" has incompatible type\u001b[39;00m\n\u001b[0;32m   7625\u001b[0m \u001b[38;5;66;03m# \"Union[bool, NoDefault]\"; expected \"bool\"\u001b[39;00m\n\u001b[1;32m-> 7626\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   7627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   7628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   7629\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   7630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   7631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   7632\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   7633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   7634\u001b[0m \u001b[43m    \u001b[49m\u001b[43msqueeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   7635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   7636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   7637\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\finlabG\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:888\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrouper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_grouper\n\u001b[1;32m--> 888\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmutated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmutated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m obj\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[1;32m~\\.conda\\envs\\finlabG\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py:877\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[0;32m    869\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    870\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of grouper (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(gpr)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) and axis (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj\u001b[38;5;241m.\u001b[39mshape[axis]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    871\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust be same length\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    872\u001b[0m         )\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;66;03m# create the Grouping\u001b[39;00m\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;66;03m# allow us to passing the actual Grouping as the gpr\u001b[39;00m\n\u001b[0;32m    876\u001b[0m     ping \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 877\u001b[0m         \u001b[43mGrouping\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgroup_axis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgpr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m            \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m            \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m            \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m            \u001b[49m\u001b[43min_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_axis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    887\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouping)\n\u001b[0;32m    888\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m gpr\n\u001b[0;32m    889\u001b[0m     )\n\u001b[0;32m    891\u001b[0m     groupings\u001b[38;5;241m.\u001b[39mappend(ping)\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(groupings) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obj):\n",
      "File \u001b[1;32m~\\.conda\\envs\\finlabG\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py:469\u001b[0m, in \u001b[0;36mGrouping.__init__\u001b[1;34m(self, index, grouper, obj, level, sort, observed, in_axis, dropna)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlevel \u001b[38;5;241m=\u001b[39m level\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_orig_grouper \u001b[38;5;241m=\u001b[39m grouper\n\u001b[1;32m--> 469\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrouping_vector \u001b[38;5;241m=\u001b[39m \u001b[43m_convert_grouper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrouper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_all_grouper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index \u001b[38;5;241m=\u001b[39m index\n",
      "File \u001b[1;32m~\\.conda\\envs\\finlabG\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py:921\u001b[0m, in \u001b[0;36m_convert_grouper\u001b[1;34m(axis, grouper)\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(grouper, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, Series, Index, np\u001b[38;5;241m.\u001b[39mndarray)):\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(grouper) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(axis):\n\u001b[1;32m--> 921\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGrouper and axis must be same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    923\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(grouper, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    924\u001b[0m         grouper \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39masarray_tuplesafe(grouper)\n",
      "\u001b[1;31mValueError\u001b[0m: Grouper and axis must be same length"
     ]
    }
   ],
   "source": [
    "from finlab import ml\n",
    "\n",
    "ml.add_profit_prediction(dataset)\n",
    "ml.add_rank_prediction(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 刪除太大太小的歷史資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(453200, 29)\n",
      "(424260, 29)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "\n",
    "def drop_extreme_case(dataset, feature_names, thresh=0.01):\n",
    "    \n",
    "    extreme_cases = pd.Series(False, index=dataset.index)\n",
    "    for f in feature_names:\n",
    "        tf = dataset[f]\n",
    "        extreme_cases = extreme_cases | (tf < tf.quantile(thresh)) | (tf > tf.quantile(1-thresh))\n",
    "    dataset = dataset[~extreme_cases]\n",
    "    return dataset\n",
    "\n",
    "dataset_drop_extreme_case = drop_extreme_case(dataset, \n",
    "    ['bias60', 'bias120', 'bias240', 'mom1', 'mom2', 'mom3', 'mom4', 'mom5', 'mom6'], thresh=0.01)\n",
    "\n",
    "print(dataset_drop_extreme_case.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dropna = dataset_drop_extreme_case.dropna(how='any')\n",
    "dataset_dropna = dataset_dropna.reset_index().set_index(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2005-02-15', '2005-03-15', '2005-04-15', '2005-05-15',\n",
       "               '2005-06-15', '2005-07-15', '2005-08-15', '2005-09-15',\n",
       "               '2005-10-15', '2005-11-15',\n",
       "               ...\n",
       "               '2022-08-15', '2022-09-15', '2022-10-15', '2022-11-15',\n",
       "               '2022-12-15', '2023-01-15', '2023-02-15', '2023-03-15',\n",
       "               '2023-04-15', '2023-05-15'],\n",
       "              dtype='datetime64[ns]', name='date', length=424260, freq=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_drop_extreme_case.index.get_level_values(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################　　　自己加入的　　　##############################################\n",
    "\n",
    "dataset_dropna.index = pd.to_datetime(dataset_dropna.index)\n",
    "dataset_dropna = dataset_dropna.sort_index()\n",
    "\n",
    "#修復＜class ‘numpy.ndarray‘＞　https://blog.csdn.net/lxbin/article/details/114005757"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset_dropna[:'2015']\n",
    "dataset_test = dataset_dropna['2016':]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神經網路模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               2800      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,001\n",
      "Trainable params: 13,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "start fitting\n",
      "Epoch 1/225\n",
      "80/80 [==============================] - 5s 15ms/step - loss: 0.0995 - val_loss: 0.0799\n",
      "Epoch 2/225\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.0799 - val_loss: 0.0798\n",
      "Epoch 3/225\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.0797 - val_loss: 0.0798\n",
      "Epoch 4/225\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.0796 - val_loss: 0.0798\n",
      "Epoch 5/225\n",
      "80/80 [==============================] - 2s 19ms/step - loss: 0.0795 - val_loss: 0.0797\n",
      "Epoch 6/225\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.0794 - val_loss: 0.0796\n",
      "Epoch 7/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0794 - val_loss: 0.0797\n",
      "Epoch 8/225\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.0793 - val_loss: 0.0797\n",
      "Epoch 9/225\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.0792 - val_loss: 0.0796\n",
      "Epoch 10/225\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.0792 - val_loss: 0.0796\n",
      "Epoch 11/225\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.0792 - val_loss: 0.0796\n",
      "Epoch 12/225\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.0791 - val_loss: 0.0796\n",
      "Epoch 13/225\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 0.0791 - val_loss: 0.0797\n",
      "Epoch 14/225\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.0791 - val_loss: 0.0796\n",
      "Epoch 15/225\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.0791 - val_loss: 0.0796\n",
      "Epoch 16/225\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.0791 - val_loss: 0.0797\n",
      "Epoch 17/225\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.0791 - val_loss: 0.0796\n",
      "Epoch 18/225\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.0790 - val_loss: 0.0797\n",
      "Epoch 19/225\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.0790 - val_loss: 0.0796\n",
      "Epoch 20/225\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.0790 - val_loss: 0.0796\n",
      "Epoch 21/225\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.0790 - val_loss: 0.0796\n",
      "Epoch 22/225\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 0.0790 - val_loss: 0.0795\n",
      "Epoch 23/225\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.0790 - val_loss: 0.0795\n",
      "Epoch 24/225\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.0790 - val_loss: 0.0796\n",
      "Epoch 25/225\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.0789 - val_loss: 0.0797\n",
      "Epoch 26/225\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.0789 - val_loss: 0.0796\n",
      "Epoch 27/225\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0789 - val_loss: 0.0795\n",
      "Epoch 28/225\n",
      "80/80 [==============================] - 0s 5ms/step - loss: 0.0788 - val_loss: 0.0795\n",
      "Epoch 29/225\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.0789 - val_loss: 0.0795\n",
      "Epoch 30/225\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.0789 - val_loss: 0.0798\n",
      "Epoch 31/225\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.0789 - val_loss: 0.0795\n",
      "Epoch 32/225\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.0788 - val_loss: 0.0795\n",
      "Epoch 33/225\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.0788 - val_loss: 0.0795\n",
      "Epoch 34/225\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.0788 - val_loss: 0.0795\n",
      "Epoch 35/225\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.0788 - val_loss: 0.0795\n",
      "Epoch 36/225\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.0788 - val_loss: 0.0795\n",
      "Epoch 37/225\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.0788 - val_loss: 0.0795\n",
      "Epoch 38/225\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.0788 - val_loss: 0.0795\n",
      "Epoch 39/225\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.0788 - val_loss: 0.0796\n",
      "Epoch 40/225\n",
      "80/80 [==============================] - 0s 6ms/step - loss: 0.0787 - val_loss: 0.0795\n",
      "Epoch 41/225\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.0787 - val_loss: 0.0795\n",
      "Epoch 42/225\n",
      "80/80 [==============================] - 1s 6ms/step - loss: 0.0787 - val_loss: 0.0794\n",
      "Epoch 43/225\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.0787 - val_loss: 0.0794\n",
      "Epoch 44/225\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.0787 - val_loss: 0.0794\n",
      "Epoch 45/225\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.0787 - val_loss: 0.0795\n",
      "Epoch 46/225\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.0787 - val_loss: 0.0795\n",
      "Epoch 47/225\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.0786 - val_loss: 0.0794\n",
      "Epoch 48/225\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.0787 - val_loss: 0.0794\n",
      "Epoch 49/225\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.0786 - val_loss: 0.0793\n",
      "Epoch 50/225\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.0786 - val_loss: 0.0794\n",
      "Epoch 51/225\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.0787 - val_loss: 0.0794\n",
      "Epoch 52/225\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.0785 - val_loss: 0.0794\n",
      "Epoch 53/225\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.0786 - val_loss: 0.0794\n",
      "Epoch 54/225\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.0785 - val_loss: 0.0793\n",
      "Epoch 55/225\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.0786 - val_loss: 0.0794\n",
      "Epoch 56/225\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.0786 - val_loss: 0.0793\n",
      "Epoch 57/225\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.0786 - val_loss: 0.0794\n",
      "Epoch 58/225\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.0786 - val_loss: 0.0794\n",
      "Epoch 59/225\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.0786 - val_loss: 0.0794\n",
      "Epoch 60/225\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.0785 - val_loss: 0.0793\n",
      "Epoch 61/225\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.0785 - val_loss: 0.0793\n",
      "Epoch 62/225\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.0784 - val_loss: 0.0792\n",
      "Epoch 63/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0784 - val_loss: 0.0794\n",
      "Epoch 64/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0784 - val_loss: 0.0792\n",
      "Epoch 65/225\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.0783 - val_loss: 0.0794\n",
      "Epoch 66/225\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.0784 - val_loss: 0.0794\n",
      "Epoch 67/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0784 - val_loss: 0.0793\n",
      "Epoch 68/225\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.0784 - val_loss: 0.0794\n",
      "Epoch 69/225\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.0784 - val_loss: 0.0793\n",
      "Epoch 70/225\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.0784 - val_loss: 0.0792\n",
      "Epoch 71/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0783 - val_loss: 0.0794\n",
      "Epoch 72/225\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.0783 - val_loss: 0.0793\n",
      "Epoch 73/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0783 - val_loss: 0.0792\n",
      "Epoch 74/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0783 - val_loss: 0.0793\n",
      "Epoch 75/225\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.0782 - val_loss: 0.0794\n",
      "Epoch 76/225\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.0783 - val_loss: 0.0792\n",
      "Epoch 77/225\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.0783 - val_loss: 0.0792\n",
      "Epoch 78/225\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.0783 - val_loss: 0.0793\n",
      "Epoch 79/225\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.0783 - val_loss: 0.0794\n",
      "Epoch 80/225\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.0782 - val_loss: 0.0792\n",
      "Epoch 81/225\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.0783 - val_loss: 0.0793\n",
      "Epoch 82/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0782 - val_loss: 0.0793\n",
      "Epoch 83/225\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.0782 - val_loss: 0.0793\n",
      "Epoch 84/225\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.0781 - val_loss: 0.0794\n",
      "Epoch 85/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0782 - val_loss: 0.0793\n",
      "Epoch 86/225\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.0781 - val_loss: 0.0793\n",
      "Epoch 87/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0782 - val_loss: 0.0792\n",
      "Epoch 88/225\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.0781 - val_loss: 0.0793\n",
      "Epoch 89/225\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.0781 - val_loss: 0.0792\n",
      "Epoch 90/225\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.0782 - val_loss: 0.0794\n",
      "Epoch 91/225\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.0780 - val_loss: 0.0793\n",
      "Epoch 92/225\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.0781 - val_loss: 0.0792\n",
      "Epoch 93/225\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.0781 - val_loss: 0.0793\n",
      "Epoch 94/225\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.0781 - val_loss: 0.0791\n",
      "Epoch 95/225\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.0781 - val_loss: 0.0792\n",
      "Epoch 96/225\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.0781 - val_loss: 0.0792\n",
      "Epoch 97/225\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.0780 - val_loss: 0.0790\n",
      "Epoch 98/225\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.0782 - val_loss: 0.0792\n",
      "Epoch 99/225\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.0780 - val_loss: 0.0792\n",
      "Epoch 100/225\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.0780 - val_loss: 0.0795\n",
      "Epoch 101/225\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.0780 - val_loss: 0.0793\n",
      "Epoch 102/225\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.0779 - val_loss: 0.0791\n",
      "Epoch 103/225\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.0781 - val_loss: 0.0791\n",
      "Epoch 104/225\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.0779 - val_loss: 0.0792\n",
      "Epoch 105/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0779 - val_loss: 0.0791\n",
      "Epoch 106/225\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.0778 - val_loss: 0.0792\n",
      "Epoch 107/225\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.0778 - val_loss: 0.0792\n",
      "Epoch 108/225\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.0779 - val_loss: 0.0792\n",
      "Epoch 109/225\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.0779 - val_loss: 0.0791\n",
      "Epoch 110/225\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.0778 - val_loss: 0.0795\n",
      "Epoch 111/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0779 - val_loss: 0.0791\n",
      "Epoch 112/225\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.0779 - val_loss: 0.0791\n",
      "Epoch 113/225\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.0777 - val_loss: 0.0790\n",
      "Epoch 114/225\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.0778 - val_loss: 0.0791\n",
      "Epoch 115/225\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.0778 - val_loss: 0.0794\n",
      "Epoch 116/225\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.0777 - val_loss: 0.0795\n",
      "Epoch 117/225\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.0777 - val_loss: 0.0790\n",
      "Epoch 118/225\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.0777 - val_loss: 0.0792\n",
      "Epoch 119/225\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.0778 - val_loss: 0.0792\n",
      "Epoch 120/225\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.0777 - val_loss: 0.0791\n",
      "Epoch 121/225\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.0776 - val_loss: 0.0792\n",
      "Epoch 122/225\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.0777 - val_loss: 0.0790\n",
      "Epoch 123/225\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.0778 - val_loss: 0.0791\n",
      "Epoch 124/225\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.0776 - val_loss: 0.0793\n",
      "Epoch 125/225\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.0777 - val_loss: 0.0793\n",
      "Epoch 126/225\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.0775 - val_loss: 0.0790\n",
      "Epoch 127/225\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.0776 - val_loss: 0.0795\n",
      "Epoch 128/225\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.0775 - val_loss: 0.0790\n",
      "Epoch 129/225\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.0776 - val_loss: 0.0790\n",
      "Epoch 130/225\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.0776 - val_loss: 0.0793\n",
      "Epoch 131/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0775 - val_loss: 0.0792\n",
      "Epoch 132/225\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.0776 - val_loss: 0.0791\n",
      "Epoch 133/225\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.0775 - val_loss: 0.0791\n",
      "Epoch 134/225\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.0775 - val_loss: 0.0793\n",
      "Epoch 135/225\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.0774 - val_loss: 0.0790\n",
      "Epoch 136/225\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.0775 - val_loss: 0.0792\n",
      "Epoch 137/225\n",
      "80/80 [==============================] - 1s 7ms/step - loss: 0.0774 - val_loss: 0.0791\n",
      "Epoch 138/225\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.0774 - val_loss: 0.0794\n",
      "Epoch 139/225\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.0774 - val_loss: 0.0790\n",
      "Epoch 140/225\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.0774 - val_loss: 0.0791\n",
      "Epoch 141/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0775 - val_loss: 0.0794\n",
      "Epoch 142/225\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.0773 - val_loss: 0.0791\n",
      "Epoch 143/225\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.0774 - val_loss: 0.0793\n",
      "Epoch 144/225\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.0773 - val_loss: 0.0790\n",
      "Epoch 145/225\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.0773 - val_loss: 0.0792\n",
      "Epoch 146/225\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.0773 - val_loss: 0.0791\n",
      "Epoch 147/225\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.0773 - val_loss: 0.0791\n",
      "Epoch 148/225\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.0773 - val_loss: 0.0790\n",
      "Epoch 149/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0772 - val_loss: 0.0791\n",
      "Epoch 150/225\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.0773 - val_loss: 0.0788\n",
      "Epoch 151/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0772 - val_loss: 0.0791\n",
      "Epoch 152/225\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.0772 - val_loss: 0.0791\n",
      "Epoch 153/225\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.0772 - val_loss: 0.0789\n",
      "Epoch 154/225\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.0771 - val_loss: 0.0791\n",
      "Epoch 155/225\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.0772 - val_loss: 0.0794\n",
      "Epoch 156/225\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.0773 - val_loss: 0.0792\n",
      "Epoch 157/225\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.0772 - val_loss: 0.0791\n",
      "Epoch 158/225\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.0772 - val_loss: 0.0790\n",
      "Epoch 159/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0771 - val_loss: 0.0792\n",
      "Epoch 160/225\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.0771 - val_loss: 0.0790\n",
      "Epoch 161/225\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.0771 - val_loss: 0.0791\n",
      "Epoch 162/225\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.0771 - val_loss: 0.0791\n",
      "Epoch 163/225\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.0771 - val_loss: 0.0792\n",
      "Epoch 164/225\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.0770 - val_loss: 0.0792\n",
      "Epoch 165/225\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.0771 - val_loss: 0.0791\n",
      "Epoch 166/225\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.0772 - val_loss: 0.0789\n",
      "Epoch 167/225\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.0771 - val_loss: 0.0792\n",
      "Epoch 168/225\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.0770 - val_loss: 0.0791\n",
      "Epoch 169/225\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.0771 - val_loss: 0.0790\n",
      "Epoch 170/225\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.0771 - val_loss: 0.0791\n",
      "Epoch 171/225\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.0770 - val_loss: 0.0790\n",
      "Epoch 172/225\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.0770 - val_loss: 0.0790\n",
      "Epoch 173/225\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.0770 - val_loss: 0.0790\n",
      "Epoch 174/225\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.0769 - val_loss: 0.0792\n",
      "Epoch 175/225\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.0769 - val_loss: 0.0788\n",
      "Epoch 176/225\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.0770 - val_loss: 0.0792\n",
      "Epoch 177/225\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.0769 - val_loss: 0.0793\n",
      "Epoch 178/225\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.0768 - val_loss: 0.0790\n",
      "Epoch 179/225\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.0769 - val_loss: 0.0789\n",
      "Epoch 180/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0769 - val_loss: 0.0792\n",
      "Epoch 181/225\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.0769 - val_loss: 0.0792\n",
      "Epoch 182/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0770 - val_loss: 0.0791\n",
      "Epoch 183/225\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.0770 - val_loss: 0.0791\n",
      "Epoch 184/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0768 - val_loss: 0.0792\n",
      "Epoch 185/225\n",
      "80/80 [==============================] - 1s 9ms/step - loss: 0.0768 - val_loss: 0.0793\n",
      "Epoch 186/225\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.0768 - val_loss: 0.0799\n",
      "Epoch 187/225\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.0767 - val_loss: 0.0793\n",
      "Epoch 188/225\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.0768 - val_loss: 0.0789\n",
      "Epoch 189/225\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.0770 - val_loss: 0.0789\n",
      "Epoch 190/225\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.0768 - val_loss: 0.0790\n",
      "Epoch 191/225\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.0768 - val_loss: 0.0794\n",
      "Epoch 192/225\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.0768 - val_loss: 0.0792\n",
      "Epoch 193/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0768 - val_loss: 0.0792\n",
      "Epoch 194/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0768 - val_loss: 0.0792\n",
      "Epoch 195/225\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.0768 - val_loss: 0.0792\n",
      "Epoch 196/225\n",
      "80/80 [==============================] - 1s 10ms/step - loss: 0.0767 - val_loss: 0.0789\n",
      "Epoch 197/225\n",
      "80/80 [==============================] - 1s 8ms/step - loss: 0.0768 - val_loss: 0.0792\n",
      "Epoch 198/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0768 - val_loss: 0.0794\n",
      "Epoch 199/225\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.0768 - val_loss: 0.0791\n",
      "Epoch 200/225\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.0767 - val_loss: 0.0792\n",
      "Epoch 201/225\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.0768 - val_loss: 0.0791\n",
      "Epoch 202/225\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.0766 - val_loss: 0.0791\n",
      "Epoch 203/225\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.0766 - val_loss: 0.0794\n",
      "Epoch 204/225\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.0767 - val_loss: 0.0794\n",
      "Epoch 205/225\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.0767 - val_loss: 0.0793\n",
      "Epoch 206/225\n",
      "80/80 [==============================] - 1s 11ms/step - loss: 0.0767 - val_loss: 0.0793\n",
      "Epoch 207/225\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.0767 - val_loss: 0.0791\n",
      "Epoch 208/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0765 - val_loss: 0.0790\n",
      "Epoch 209/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0766 - val_loss: 0.0793\n",
      "Epoch 210/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0767 - val_loss: 0.0792\n",
      "Epoch 211/225\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.0766 - val_loss: 0.0796\n",
      "Epoch 212/225\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.0767 - val_loss: 0.0793\n",
      "Epoch 213/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0766 - val_loss: 0.0791\n",
      "Epoch 214/225\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.0766 - val_loss: 0.0791\n",
      "Epoch 215/225\n",
      "80/80 [==============================] - 1s 12ms/step - loss: 0.0766 - val_loss: 0.0794\n",
      "Epoch 216/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0763 - val_loss: 0.0794\n",
      "Epoch 217/225\n",
      "80/80 [==============================] - 1s 17ms/step - loss: 0.0765 - val_loss: 0.0793\n",
      "Epoch 218/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0766 - val_loss: 0.0795\n",
      "Epoch 219/225\n",
      "80/80 [==============================] - 1s 16ms/step - loss: 0.0766 - val_loss: 0.0793\n",
      "Epoch 220/225\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.0765 - val_loss: 0.0792\n",
      "Epoch 221/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0766 - val_loss: 0.0792\n",
      "Epoch 222/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0764 - val_loss: 0.0792\n",
      "Epoch 223/225\n",
      "80/80 [==============================] - 1s 14ms/step - loss: 0.0765 - val_loss: 0.0796\n",
      "Epoch 224/225\n",
      "80/80 [==============================] - 1s 13ms/step - loss: 0.0764 - val_loss: 0.0794\n",
      "Epoch 225/225\n",
      "80/80 [==============================] - 1s 15ms/step - loss: 0.0764 - val_loss: 0.0792\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Dense(100, activation='relu',\n",
    "                      input_shape=(len(feature_names),),\n",
    "                      kernel_initializer=initializers.he_normal(seed=0)))\n",
    "model.add(layers.Dense(100, activation='relu',\n",
    "                      kernel_initializer=initializers.he_normal(seed=0)))\n",
    "model.add(layers.Dropout(0.7))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=\"adam\",)\n",
    "\n",
    "print('start fitting')\n",
    "history = model.fit(dataset_train[feature_names], dataset_train['rank'],\n",
    "                    batch_size=1000,\n",
    "                    epochs=225,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\orang\\appdata\\roaming\\python\\python310\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -upyterlab (d:\\programdata\\miniconda3\\lib\\site-packages)\n",
      "WARNING: Package(s) not found: protoc\n"
     ]
    }
   ],
   "source": [
    "!pip show protoc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(history.history['val_loss'][1:])\n",
    "plt.plot(history.history['loss'][1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lightgbm Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(n_estimators=500)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "cf = lgb.LGBMRegressor(n_estimators=500)\n",
    "cf.fit(dataset_train[feature_names].astype(float), dataset_train['rank'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor\n\u001b[0;32m      3\u001b[0m cf2 \u001b[38;5;241m=\u001b[39m RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mcf2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrank\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\finlabG\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:450\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    439\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    442\u001b[0m ]\n\u001b[0;32m    444\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 450\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_joblib_parallel_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\.conda\\envs\\finlabG\\lib\\site-packages\\joblib\\parallel.py:1051\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1051\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1055\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1057\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\finlabG\\lib\\site-packages\\joblib\\parallel.py:864\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 864\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\finlabG\\lib\\site-packages\\joblib\\parallel.py:782\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    781\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 782\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\.conda\\envs\\finlabG\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\.conda\\envs\\finlabG\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\finlabG\\lib\\site-packages\\joblib\\parallel.py:263\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    264\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\.conda\\envs\\finlabG\\lib\\site-packages\\joblib\\parallel.py:263\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    264\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\.conda\\envs\\finlabG\\lib\\site-packages\\sklearn\\utils\\fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\finlabG\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:185\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    183\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 185\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\finlabG\\lib\\site-packages\\sklearn\\tree\\_classes.py:1315\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[0;32m   1279\u001b[0m     \u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, X_idx_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1280\u001b[0m ):\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1282\u001b[0m \n\u001b[0;32m   1283\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1315\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1317\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1318\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_idx_sorted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_idx_sorted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\.conda\\envs\\finlabG\\lib\\site-packages\\sklearn\\tree\\_classes.py:420\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    411\u001b[0m         splitter,\n\u001b[0;32m    412\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    418\u001b[0m     )\n\u001b[1;32m--> 420\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "cf2 = RandomForestRegressor(n_estimators=100)\n",
    "cf2.fit(dataset_train[feature_names].astype(float), dataset_train['rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.DataFrame(zip(cf.feature_importances_, feature_names), \n",
    "                           columns=['Value','Feature']).sort_values('Value', ascending=False)\n",
    "feature_imp\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_drop = dataset.dropna(subset=feature_names+['return'])\n",
    "\n",
    "vals = model.predict(dataset_drop[feature_names].astype(float))\n",
    "dataset_drop['result1'] = pd.Series(vals.swapaxes(0,1)[0], dataset_drop.index)\n",
    "\n",
    "vals = cf.predict(dataset_drop[feature_names].astype(float))\n",
    "dataset_drop['result2'] = pd.Series(vals, dataset_drop.index)\n",
    "\n",
    "vals = cf2.predict(dataset_drop[feature_names].astype(float))\n",
    "dataset_drop['result3'] = pd.Series(vals, dataset_drop.index)\n",
    "\n",
    "dataset_drop = dataset_drop.reset_index().set_index(\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "dates = sorted(list(set(dataset_drop.index)))\n",
    "\n",
    "rs = []\n",
    "for d in dates:\n",
    "    \n",
    "    dataset_time = dataset_drop.loc[d]\n",
    "    \n",
    "    dataset_time = drop_extreme_case(dataset_time, \n",
    "        ['bias60', 'bias120', 'bias240', 'mom1', 'mom2', 'mom3', 'mom4', 'mom5', 'mom6'], thresh=0.01)\n",
    "    \n",
    "    rank = dataset_time['result1'] + dataset_time['result2'] + dataset_time['result3'] \n",
    "    \n",
    "    condition = (rank >= rank.nlargest(20).iloc[-1]) \n",
    "    r = dataset_time['return'][condition].mean()\n",
    "\n",
    "    rs.append(r * (1-3/1000-1.425/1000*2*0.6))\n",
    "\n",
    "rs = pd.Series(rs, index=dates)['2016':].cumprod()\n",
    "\n",
    "s0050 = close['0050']['2016':]\n",
    "\n",
    "pd.DataFrame({'nn strategy return':rs.reindex(s0050.index, method='ffill'), '0050 return':s0050/s0050[0]}).plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 當月持股狀況"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.index.levels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the latest dataset\n",
    "last_date = \"2019-07-15\"#dataset.index.levels[1].max()\n",
    "is_last_date = dataset.index.get_level_values('date') == last_date\n",
    "last_dataset = dataset[is_last_date].copy()\n",
    "\n",
    "\n",
    "last_dataset = drop_extreme_case(last_dataset, \n",
    "    ['bias60', 'bias120', 'bias240', 'mom1', 'mom2', 'mom3', 'mom4', 'mom5', 'mom6'], thresh=0.01)\n",
    "\n",
    "\n",
    "# remove NaN testcases\n",
    "last_dataset = last_dataset.dropna(subset=feature_names)\n",
    "\n",
    "# predict\n",
    "\n",
    "vals = model.predict(last_dataset[feature_names].astype(float))\n",
    "last_dataset['result1'] = pd.Series(vals.swapaxes(0,1)[0], last_dataset.index)\n",
    "\n",
    "vals = cf.predict(last_dataset[feature_names].astype(float))\n",
    "last_dataset['result2'] = pd.Series(vals, last_dataset.index)\n",
    "\n",
    "vals = cf2.predict(last_dataset[feature_names].astype(float))\n",
    "last_dataset['result3'] = pd.Series(vals, last_dataset.index)\n",
    "\n",
    "# calculate score\n",
    "\n",
    "rank = last_dataset['result1'] + last_dataset['result2'] + last_dataset['result3']\n",
    "condition = (rank >= rank.nlargest(20).iloc[-1]) \n",
    "\n",
    "# plot rank distribution\n",
    "rank.hist(bins=20)\n",
    "\n",
    "\n",
    "# show the best 20 stocks\n",
    "slist1 = rank[condition].reset_index()['stock_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 平均分配資產於股票之中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close = data.get(\"收盤價\")\n",
    "\n",
    "money = 1000000\n",
    "stock_prices = close[rank[condition].reset_index()['stock_id']].iloc[-1]\n",
    "\n",
    "\n",
    "print(\"股票平分張數:\")\n",
    "money / len(stock_prices) / stock_prices / 1000\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finlabG",
   "language": "python",
   "name": "finlabg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
