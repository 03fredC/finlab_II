{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 移除不必要的警告\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 獲取歷史資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from finlab.data import Data\n",
    "from finlab.ml import fundamental_features\n",
    "fdf = fundamental_features()\n",
    "\n",
    "data = Data()\n",
    "\n",
    "close = data.get(\"收盤價\")\n",
    "open_ = data.get(\"開盤價\")\n",
    "high = data.get(\"最高價\")\n",
    "low = data.get(\"最低價\")\n",
    "vol = data.get(\"成交股數\")\n",
    "\n",
    "PB = data.get(\"股價淨值比\")\n",
    "pe = data.get(\"本益比\")\n",
    "\n",
    "#close = data.get_adj(\"收盤價\").round(2)\n",
    "\n",
    "#財務指標\n",
    "rev = data.get(\"當月營收\")\n",
    "l_rev = data.get(\"去年當月營收\")\n",
    "\n",
    "#t123 = data.get('土地')\n",
    "\n",
    "#bargin_i=data.get(\"投信買賣超股數\")\n",
    "#bargin_f=data.get(\"外資自營商買賣超股數\")\n",
    "#bargin_s=data.get(\"自營商買賣超股數(自行買賣)\")\n",
    "#\n",
    "\n",
    "rev.index = rev.index.shift(5, \"d\")         #每月頻率\n",
    "#周頻率"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "```\n",
    "https://www.twblogs.net/a/5d3f3173bd9eee517422735f\n",
    "W-WED\n",
    "https://docs.python.org/zh-tw/3/library/calendar.html\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MFI  = data.talib(\"MFI\")\n",
    "##MFI.tail()\n",
    "#ub,mb,lb =data.talib(\"BBANDS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 營收相關"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################　　　自己加入的　　　##############################################\n",
    "import pandas as pd\n",
    "from finlab.__init__ import talib_all_stock\n",
    "from talib import abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias(n):\n",
    "    return close / close.rolling(n, min_periods=1).mean()\n",
    "\n",
    "def acc(n):\n",
    "    return close.shift(n) / (close.shift(2*n) + close) * 2\n",
    "\n",
    "def rsv(n):\n",
    "    l = close.rolling(n, min_periods=1).min()\n",
    "    h = close.rolling(n, min_periods=1).max()\n",
    "    \n",
    "    return (close - l) / (h - l)\n",
    "\n",
    "def mom(n):\n",
    "    return (rev / rev.shift(1)).shift(n)\n",
    "\n",
    "def yoy(n):\n",
    "    return (rev.shift(n) / rev.shift(12+n)) -1\n",
    "\n",
    "\n",
    "features = {\n",
    "    'mom1': mom(1),\n",
    "    'mom2': mom(2),\n",
    "    'mom3': mom(3),\n",
    "    'mom4': mom(4),\n",
    "    'mom5': mom(5),\n",
    "    'mom6': mom(6),\n",
    "    'mom7': mom(7),\n",
    "    'mom8': mom(8),\n",
    "    'mom9': mom(9),\n",
    "    \n",
    "    'bias5': bias(5),\n",
    "    'bias10': bias(10),\n",
    "    'bias20': bias(20),\n",
    "    'bias60': bias(60),\n",
    "    'bias120': bias(120),\n",
    "    'bias240': bias(240),\n",
    "    \n",
    "    'acc5': acc(5),\n",
    "    'acc10': acc(10),\n",
    "    'acc20': acc(20),\n",
    "    'acc60': acc(60),\n",
    "    'acc120': acc(120),\n",
    "    'acc240': acc(240),\n",
    "    \n",
    "    'rsv5': rsv(5),\n",
    "    'rsv10': rsv(10),\n",
    "    'rsv20': rsv(20),\n",
    "    'rsv60': rsv(60),\n",
    "    'rsv120': rsv(120),\n",
    "    'rsv240': rsv(240),\n",
    "###############################################\n",
    "    'yoy': yoy(1),\n",
    "    'delta_yoy':(yoy(1)/yoy(2))-1,\n",
    "    \n",
    "    'PB':PB,\n",
    "    'PE':pe,       \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 財報指標"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "兩個feature結合\n",
    "[https://hahow.in/courses/5b9d3a6dca498a001e917383/discussions/5d18b63eac23d80020ae4ce7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finlab.ml import fundamental_features\n",
    "dataset_fundamental = fundamental_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>T3395營業利益</th>\n",
       "      <th>T7210營運現金流</th>\n",
       "      <th>T3950歸屬母公司淨利</th>\n",
       "      <th>T7211折舊</th>\n",
       "      <th>T0100流動資產</th>\n",
       "      <th>T1100流動負債</th>\n",
       "      <th>T7324取得不動產廠房及設備</th>\n",
       "      <th>T3970經常稅後淨利</th>\n",
       "      <th>R101_ROA稅後息前</th>\n",
       "      <th>R11V_ROA綜合損益</th>\n",
       "      <th>...</th>\n",
       "      <th>R409_淨值成長率</th>\n",
       "      <th>R501_流動比率</th>\n",
       "      <th>R502_速動比率</th>\n",
       "      <th>R503_利息支出率</th>\n",
       "      <th>R678_營運資金</th>\n",
       "      <th>R607_總資產週轉次數</th>\n",
       "      <th>R610_存貨週轉率</th>\n",
       "      <th>R612_固定資產週轉次數</th>\n",
       "      <th>R613_淨值週轉率次</th>\n",
       "      <th>R69B_自由現金流量</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stock_id</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1101</th>\n",
       "      <th>2013-05-15</th>\n",
       "      <td>2026729.0</td>\n",
       "      <td>2274053.0</td>\n",
       "      <td>1911110.0</td>\n",
       "      <td>1571884.0</td>\n",
       "      <td>77246355.0</td>\n",
       "      <td>65913019.0</td>\n",
       "      <td>-211456.0</td>\n",
       "      <td>1999624.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>117.194382</td>\n",
       "      <td>83.137456</td>\n",
       "      <td>-8.186859</td>\n",
       "      <td>11333336.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-08-14</th>\n",
       "      <td>4588992.0</td>\n",
       "      <td>6029953.0</td>\n",
       "      <td>949640.0</td>\n",
       "      <td>1543041.0</td>\n",
       "      <td>77139390.0</td>\n",
       "      <td>68971246.0</td>\n",
       "      <td>-1033541.0</td>\n",
       "      <td>3802105.0</td>\n",
       "      <td>-1.233843</td>\n",
       "      <td>0.944284</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111.842825</td>\n",
       "      <td>79.609613</td>\n",
       "      <td>-13.298556</td>\n",
       "      <td>8168144.0</td>\n",
       "      <td>0.104927</td>\n",
       "      <td>2.279272</td>\n",
       "      <td>0.258545</td>\n",
       "      <td>0.201496</td>\n",
       "      <td>9467257.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-11-14</th>\n",
       "      <td>5910634.0</td>\n",
       "      <td>5168764.0</td>\n",
       "      <td>5025600.0</td>\n",
       "      <td>1605424.0</td>\n",
       "      <td>77212043.0</td>\n",
       "      <td>73733391.0</td>\n",
       "      <td>-470935.0</td>\n",
       "      <td>5344662.0</td>\n",
       "      <td>-1.130208</td>\n",
       "      <td>2.481737</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104.717879</td>\n",
       "      <td>83.502774</td>\n",
       "      <td>-17.083477</td>\n",
       "      <td>3478652.0</td>\n",
       "      <td>0.110355</td>\n",
       "      <td>2.335831</td>\n",
       "      <td>0.272965</td>\n",
       "      <td>0.211719</td>\n",
       "      <td>12033719.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-03-31</th>\n",
       "      <td>5249620.0</td>\n",
       "      <td>8500835.0</td>\n",
       "      <td>6450131.0</td>\n",
       "      <td>2118671.0</td>\n",
       "      <td>87788055.0</td>\n",
       "      <td>70398494.0</td>\n",
       "      <td>-1089134.0</td>\n",
       "      <td>3972292.0</td>\n",
       "      <td>-0.339410</td>\n",
       "      <td>3.102975</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>124.701609</td>\n",
       "      <td>102.946688</td>\n",
       "      <td>-32.624816</td>\n",
       "      <td>17389561.0</td>\n",
       "      <td>0.119738</td>\n",
       "      <td>2.763869</td>\n",
       "      <td>0.302962</td>\n",
       "      <td>0.222264</td>\n",
       "      <td>-6823797.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-15</th>\n",
       "      <td>3684224.0</td>\n",
       "      <td>6895832.0</td>\n",
       "      <td>4015175.0</td>\n",
       "      <td>1594505.0</td>\n",
       "      <td>92121458.0</td>\n",
       "      <td>74388787.0</td>\n",
       "      <td>-940633.0</td>\n",
       "      <td>2791235.0</td>\n",
       "      <td>-1.732411</td>\n",
       "      <td>1.630076</td>\n",
       "      <td>...</td>\n",
       "      <td>8.866880</td>\n",
       "      <td>123.837828</td>\n",
       "      <td>101.776758</td>\n",
       "      <td>-9.792979</td>\n",
       "      <td>17732671.0</td>\n",
       "      <td>0.093998</td>\n",
       "      <td>2.191306</td>\n",
       "      <td>0.245688</td>\n",
       "      <td>0.171327</td>\n",
       "      <td>4901868.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">9962</th>\n",
       "      <th>2021-05-15</th>\n",
       "      <td>8016.0</td>\n",
       "      <td>149610.0</td>\n",
       "      <td>6847.0</td>\n",
       "      <td>5027.0</td>\n",
       "      <td>699386.0</td>\n",
       "      <td>76254.0</td>\n",
       "      <td>-3081.0</td>\n",
       "      <td>6607.0</td>\n",
       "      <td>0.618888</td>\n",
       "      <td>0.636208</td>\n",
       "      <td>...</td>\n",
       "      <td>1.131801</td>\n",
       "      <td>917.179427</td>\n",
       "      <td>271.866394</td>\n",
       "      <td>1.005915</td>\n",
       "      <td>623132.0</td>\n",
       "      <td>0.380922</td>\n",
       "      <td>0.726929</td>\n",
       "      <td>1.533102</td>\n",
       "      <td>0.435345</td>\n",
       "      <td>37994.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-14</th>\n",
       "      <td>40404.0</td>\n",
       "      <td>-155514.0</td>\n",
       "      <td>41534.0</td>\n",
       "      <td>4813.0</td>\n",
       "      <td>976044.0</td>\n",
       "      <td>274123.0</td>\n",
       "      <td>-2374.0</td>\n",
       "      <td>37397.0</td>\n",
       "      <td>3.243048</td>\n",
       "      <td>3.596651</td>\n",
       "      <td>...</td>\n",
       "      <td>7.003658</td>\n",
       "      <td>356.060601</td>\n",
       "      <td>110.054975</td>\n",
       "      <td>0.178902</td>\n",
       "      <td>701921.0</td>\n",
       "      <td>0.594034</td>\n",
       "      <td>1.150951</td>\n",
       "      <td>2.593983</td>\n",
       "      <td>0.710227</td>\n",
       "      <td>-34385.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-14</th>\n",
       "      <td>48861.0</td>\n",
       "      <td>-435827.0</td>\n",
       "      <td>42439.0</td>\n",
       "      <td>4855.0</td>\n",
       "      <td>1420059.0</td>\n",
       "      <td>671731.0</td>\n",
       "      <td>-4677.0</td>\n",
       "      <td>40088.0</td>\n",
       "      <td>2.691891</td>\n",
       "      <td>2.839898</td>\n",
       "      <td>...</td>\n",
       "      <td>11.206472</td>\n",
       "      <td>211.402928</td>\n",
       "      <td>27.425413</td>\n",
       "      <td>0.432543</td>\n",
       "      <td>748328.0</td>\n",
       "      <td>0.428734</td>\n",
       "      <td>0.649981</td>\n",
       "      <td>2.435492</td>\n",
       "      <td>0.635698</td>\n",
       "      <td>2993.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-31</th>\n",
       "      <td>66532.0</td>\n",
       "      <td>74308.0</td>\n",
       "      <td>57469.0</td>\n",
       "      <td>4541.0</td>\n",
       "      <td>1413495.0</td>\n",
       "      <td>581642.0</td>\n",
       "      <td>-8826.0</td>\n",
       "      <td>56074.0</td>\n",
       "      <td>3.352989</td>\n",
       "      <td>3.383973</td>\n",
       "      <td>...</td>\n",
       "      <td>15.804715</td>\n",
       "      <td>243.018042</td>\n",
       "      <td>39.386083</td>\n",
       "      <td>1.907177</td>\n",
       "      <td>831853.0</td>\n",
       "      <td>0.503416</td>\n",
       "      <td>0.661852</td>\n",
       "      <td>3.250884</td>\n",
       "      <td>0.808212</td>\n",
       "      <td>-14303.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-15</th>\n",
       "      <td>44919.0</td>\n",
       "      <td>304147.0</td>\n",
       "      <td>39436.0</td>\n",
       "      <td>4397.0</td>\n",
       "      <td>1217003.0</td>\n",
       "      <td>352094.0</td>\n",
       "      <td>-1292.0</td>\n",
       "      <td>37016.0</td>\n",
       "      <td>2.371942</td>\n",
       "      <td>2.484802</td>\n",
       "      <td>...</td>\n",
       "      <td>19.138403</td>\n",
       "      <td>345.647185</td>\n",
       "      <td>85.204235</td>\n",
       "      <td>2.087938</td>\n",
       "      <td>864909.0</td>\n",
       "      <td>0.525429</td>\n",
       "      <td>0.758426</td>\n",
       "      <td>3.189613</td>\n",
       "      <td>0.753799</td>\n",
       "      <td>9429.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69190 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     T3395營業利益  T7210營運現金流  T3950歸屬母公司淨利    T7211折舊  \\\n",
       "stock_id date                                                         \n",
       "1101     2013-05-15  2026729.0   2274053.0     1911110.0  1571884.0   \n",
       "         2013-08-14  4588992.0   6029953.0      949640.0  1543041.0   \n",
       "         2013-11-14  5910634.0   5168764.0     5025600.0  1605424.0   \n",
       "         2014-03-31  5249620.0   8500835.0     6450131.0  2118671.0   \n",
       "         2014-05-15  3684224.0   6895832.0     4015175.0  1594505.0   \n",
       "...                        ...         ...           ...        ...   \n",
       "9962     2021-05-15     8016.0    149610.0        6847.0     5027.0   \n",
       "         2021-08-14    40404.0   -155514.0       41534.0     4813.0   \n",
       "         2021-11-14    48861.0   -435827.0       42439.0     4855.0   \n",
       "         2022-03-31    66532.0     74308.0       57469.0     4541.0   \n",
       "         2022-05-15    44919.0    304147.0       39436.0     4397.0   \n",
       "\n",
       "                      T0100流動資產   T1100流動負債  T7324取得不動產廠房及設備  T3970經常稅後淨利  \\\n",
       "stock_id date                                                               \n",
       "1101     2013-05-15  77246355.0  65913019.0        -211456.0    1999624.0   \n",
       "         2013-08-14  77139390.0  68971246.0       -1033541.0    3802105.0   \n",
       "         2013-11-14  77212043.0  73733391.0        -470935.0    5344662.0   \n",
       "         2014-03-31  87788055.0  70398494.0       -1089134.0    3972292.0   \n",
       "         2014-05-15  92121458.0  74388787.0        -940633.0    2791235.0   \n",
       "...                         ...         ...              ...          ...   \n",
       "9962     2021-05-15    699386.0     76254.0          -3081.0       6607.0   \n",
       "         2021-08-14    976044.0    274123.0          -2374.0      37397.0   \n",
       "         2021-11-14   1420059.0    671731.0          -4677.0      40088.0   \n",
       "         2022-03-31   1413495.0    581642.0          -8826.0      56074.0   \n",
       "         2022-05-15   1217003.0    352094.0          -1292.0      37016.0   \n",
       "\n",
       "                     R101_ROA稅後息前  R11V_ROA綜合損益  ...  R409_淨值成長率   R501_流動比率  \\\n",
       "stock_id date                                    ...                           \n",
       "1101     2013-05-15           NaN           NaN  ...         NaN  117.194382   \n",
       "         2013-08-14     -1.233843      0.944284  ...         NaN  111.842825   \n",
       "         2013-11-14     -1.130208      2.481737  ...         NaN  104.717879   \n",
       "         2014-03-31     -0.339410      3.102975  ...         NaN  124.701609   \n",
       "         2014-05-15     -1.732411      1.630076  ...    8.866880  123.837828   \n",
       "...                           ...           ...  ...         ...         ...   \n",
       "9962     2021-05-15      0.618888      0.636208  ...    1.131801  917.179427   \n",
       "         2021-08-14      3.243048      3.596651  ...    7.003658  356.060601   \n",
       "         2021-11-14      2.691891      2.839898  ...   11.206472  211.402928   \n",
       "         2022-03-31      3.352989      3.383973  ...   15.804715  243.018042   \n",
       "         2022-05-15      2.371942      2.484802  ...   19.138403  345.647185   \n",
       "\n",
       "                      R502_速動比率  R503_利息支出率   R678_營運資金  R607_總資產週轉次數  \\\n",
       "stock_id date                                                           \n",
       "1101     2013-05-15   83.137456   -8.186859  11333336.0           NaN   \n",
       "         2013-08-14   79.609613  -13.298556   8168144.0      0.104927   \n",
       "         2013-11-14   83.502774  -17.083477   3478652.0      0.110355   \n",
       "         2014-03-31  102.946688  -32.624816  17389561.0      0.119738   \n",
       "         2014-05-15  101.776758   -9.792979  17732671.0      0.093998   \n",
       "...                         ...         ...         ...           ...   \n",
       "9962     2021-05-15  271.866394    1.005915    623132.0      0.380922   \n",
       "         2021-08-14  110.054975    0.178902    701921.0      0.594034   \n",
       "         2021-11-14   27.425413    0.432543    748328.0      0.428734   \n",
       "         2022-03-31   39.386083    1.907177    831853.0      0.503416   \n",
       "         2022-05-15   85.204235    2.087938    864909.0      0.525429   \n",
       "\n",
       "                     R610_存貨週轉率  R612_固定資產週轉次數  R613_淨值週轉率次  R69B_自由現金流量  \n",
       "stock_id date                                                             \n",
       "1101     2013-05-15         NaN            NaN          NaN          NaN  \n",
       "         2013-08-14    2.279272       0.258545     0.201496    9467257.0  \n",
       "         2013-11-14    2.335831       0.272965     0.211719   12033719.0  \n",
       "         2014-03-31    2.763869       0.302962     0.222264   -6823797.0  \n",
       "         2014-05-15    2.191306       0.245688     0.171327    4901868.0  \n",
       "...                         ...            ...          ...          ...  \n",
       "9962     2021-05-15    0.726929       1.533102     0.435345      37994.0  \n",
       "         2021-08-14    1.150951       2.593983     0.710227     -34385.0  \n",
       "         2021-11-14    0.649981       2.435492     0.635698       2993.0  \n",
       "         2022-03-31    0.661852       3.250884     0.808212     -14303.0  \n",
       "         2022-05-15    0.758426       3.189613     0.753799       9429.0  \n",
       "\n",
       "[69190 rows x 48 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_fundamental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 技術指標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://zhuanlan.zhihu.com/p/342075180 talib函数功能一览表\n",
    "\n",
    "def bias(n):\n",
    "    return close / close.rolling(n, min_periods=1).mean()\n",
    "\n",
    "def acc(n):\n",
    "    return close.shift(n) / (close.shift(2*n) + close) * 2\n",
    "\n",
    "def rsv(n):\n",
    "    l = close.rolling(n, min_periods=1).min()\n",
    "    h = close.rolling(n, min_periods=1).max()\n",
    "    \n",
    "    return (close - l) / (h - l)\n",
    "\n",
    "def mom(n):\n",
    "    return (rev / rev.shift(1)).shift(n)\n",
    "\n",
    "\n",
    "def bi_(n):\n",
    "    return (bargin_i / vol.shift(1)).shift(n)\n",
    "\n",
    "def bf(n):\n",
    "    return (bargin_f / vol.shift(1)).shift(n)\n",
    "    \n",
    "def bs(n):\n",
    "    return (bargin_s / vol.shift(1)).shift(n)\n",
    "\n",
    "def rsi(n):\n",
    "    #return talib_all_stock(ndays=10000, func=abstract.RSI, timeperiod=n)\n",
    "    return data.talib(\"RSI\",timeperiod=n)\n",
    "\n",
    "def MFI(n):\n",
    "    return data.talib(\"MFI\",timeperiod=n)\n",
    "\n",
    "def obv(n):\n",
    "    return data.talib(\"OBV\",timeperiod=n)\n",
    "\n",
    "\n",
    "\n",
    "features = {\n",
    "    \n",
    "    #'ATR14':data.talib(\"ATR\",timeperiod=14),\n",
    "    #'NATR14':data.talib('NATR',timeperiod=14),\n",
    "    #'TRANGE':data.talib('TRANGE'),\n",
    "    #'Adosc3':data.talib('ADOSC',timeperiod=3),\n",
    "    \n",
    "    #\"MFI5\":MFI(5),\n",
    "    #\"MFI10\":MFI(10),\n",
    "    \n",
    "    #'rsi6': rsi(6),  #DataFrame\n",
    "    #'rsi10': rsi(10),  #DataFrame\n",
    "    #'rsi14': rsi(14),  #DataFrame\n",
    "    #'rsi20': rsi(20),  #DataFrame\n",
    "    #'rsi50': rsi(50),  #DataFrame\n",
    "   \n",
    "    'mom1': mom(1),\n",
    "    'mom2': mom(2),\n",
    "    'mom3': mom(3),\n",
    "    'mom4': mom(4),\n",
    "    'mom5': mom(5),\n",
    "    'mom6': mom(6),\n",
    "    'mom7': mom(7),\n",
    "    'mom8': mom(8),\n",
    "    'mom9': mom(9),\n",
    "    \n",
    "    'yoy': yoy(1),\n",
    "    'delta_yoy':yoy(1)-yoy(2),\n",
    "    \n",
    "#    'ff':ff,\n",
    "    'PB':PB,\n",
    "    'PE':pe,   \n",
    "#  \n",
    "    'bias5': bias(5),\n",
    "    'bias10': bias(10),\n",
    "    'bias20': bias(20),\n",
    "    'bias60': bias(60),\n",
    "    'bias120': bias(120),\n",
    "    'bias240': bias(240),\n",
    "    \n",
    "    'acc5': acc(5),\n",
    "    'acc10': acc(10),\n",
    "    'acc20': acc(20),\n",
    "    'acc60': acc(60),\n",
    "    'acc120': acc(120),\n",
    "    'acc240': acc(240),\n",
    "    \n",
    "    #'rsv5': rsv(5),\n",
    "    #'rsv10': rsv(10),\n",
    "    #'rsv20': rsv(20),\n",
    "    #'rsv60': rsv(60),\n",
    "    #'rsv120': rsv(120),\n",
    "    #'rsv240': rsv(240),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 組合dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認各指標清單"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PB',\n",
       " 'PE',\n",
       " 'acc10',\n",
       " 'acc120',\n",
       " 'acc20',\n",
       " 'acc240',\n",
       " 'acc5',\n",
       " 'acc60',\n",
       " 'bias10',\n",
       " 'bias120',\n",
       " 'bias20',\n",
       " 'bias240',\n",
       " 'bias5',\n",
       " 'bias60',\n",
       " 'delta_yoy',\n",
       " 'mom1',\n",
       " 'mom2',\n",
       " 'mom3',\n",
       " 'mom4',\n",
       " 'mom5',\n",
       " 'mom6',\n",
       " 'mom7',\n",
       " 'mom8',\n",
       " 'mom9',\n",
       " 'yoy']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1=sorted(features)\n",
    "list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t1 = data.talib(\"NATR\",timeperiod=14)\n",
    "#t1.to_csv('myfile.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 製作dataset\n",
    "\n",
    "##### 設定買賣頻率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2005-02-15', '2005-03-15', '2005-04-15', '2005-05-15',\n",
       "               '2005-06-15', '2005-07-15', '2005-08-15', '2005-09-15',\n",
       "               '2005-10-15', '2005-11-15',\n",
       "               ...\n",
       "               '2022-02-15', '2022-03-15', '2022-04-15', '2022-05-15',\n",
       "               '2022-06-15', '2022-07-15', '2022-08-15', '2022-09-15',\n",
       "               '2022-10-15', '2022-11-15'],\n",
       "              dtype='datetime64[ns]', name='date', length=214, freq=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rev.index = rev.index.tz_localize(\"Asia/Taipei\")\n",
    "every_month = rev.index\n",
    "every_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 將dataframe 組裝起來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features['bias20'].reindex(every_month, method='ffill')\n",
    "\n",
    "for name, f in features.items():\n",
    "    features[name] = f.reindex(every_month, method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name, f in features.items():\n",
    "    features[name] = f.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################　　　自己加入的　　　##############################################\n",
    "dataset.index = dataset.index.set_names(['stock_id','date'], level=[0,1])\n",
    "\n",
    "\n",
    "#dataset.index.levels[1].name = 'date'\n",
    "#dataset.index.levels[0].name = 'stock_id'\n",
    "\n",
    "#因為你pandas更新到新版了\n",
    "## profit.index.levels[0].name = 'year'\n",
    "## profit.index.levels[1].name = 'month'\n",
    "#這兩行的語法被棄用，請改成\n",
    "#profit.index=profit.index.set_names('year', level=0)\n",
    "#profit.index=profit.index.set_names('month', level=1)\n",
    "#or profit.index=profit.index.set_names(['year','month'], level=[0,1])\n",
    "#直接一行\n",
    "#就可以了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 新增 label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finlab import ml\n",
    "\n",
    "ml.add_profit_prediction(dataset)\n",
    "ml.add_rank_prediction(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 刪除太大太小的歷史資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(437416, 27)\n",
      "(378396, 27)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "\n",
    "def drop_extreme_case(dataset, feature_names, thresh=0.01):\n",
    "    \n",
    "    extreme_cases = pd.Series(False, index=dataset.index)\n",
    "    for f in feature_names:\n",
    "        tf = dataset[f]\n",
    "        extreme_cases = extreme_cases | (tf < tf.quantile(thresh)) | (tf > tf.quantile(1-thresh))\n",
    "    dataset = dataset[~extreme_cases]\n",
    "    return dataset\n",
    "\n",
    "dataset_drop_extreme_case = drop_extreme_case(dataset , list1 , thresh=0.01)\n",
    "\n",
    "print(dataset_drop_extreme_case.shape)\n",
    "\n",
    "##(436560, 27)\n",
    "##(377538, 27)\n",
    "\n",
    "##(505602, 75)\n",
    "##(446580, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dropna = dataset_drop_extreme_case.dropna(how='any')\n",
    "dataset_dropna = dataset_dropna.reset_index().set_index(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2005-02-15', '2005-03-15', '2005-04-15', '2005-05-15',\n",
       "               '2005-06-15', '2005-07-15', '2005-08-15', '2005-09-15',\n",
       "               '2005-10-15', '2005-11-15',\n",
       "               ...\n",
       "               '2021-11-15', '2021-12-15', '2022-02-15', '2022-03-15',\n",
       "               '2022-04-15', '2022-06-15', '2022-08-15', '2022-09-15',\n",
       "               '2022-10-15', '2022-11-15'],\n",
       "              dtype='datetime64[ns]', name='date', length=378396, freq=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_drop_extreme_case.index.get_level_values(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################　　　自己加入的　　　##############################################\n",
    "\n",
    "dataset_dropna.index = pd.to_datetime(dataset_dropna.index)\n",
    "dataset_dropna = dataset_dropna.sort_index()\n",
    "\n",
    "#修復＜class ‘numpy.ndarray‘＞　https://blog.csdn.net/lxbin/article/details/114005757"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset_dropna[:'2020']\n",
    "dataset_test = dataset_dropna['2021':]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 機器學習\n",
    " - 目前只有三個，技術指標也要在增加一下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 100)               2600      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 12,801\n",
      "Trainable params: 12,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "start fitting\n",
      "Epoch 1/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.256 - ETA: 0s - loss: 0.246 - ETA: 0s - loss: 0.225 - ETA: 0s - loss: 0.192 - ETA: 0s - loss: 0.164 - ETA: 0s - loss: 0.147 - ETA: 0s - loss: 0.135 - ETA: 0s - loss: 0.126 - 1s 6ms/step - loss: 0.1233 - val_loss: 0.0714\n",
      "Epoch 2/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0736 - val_loss: 0.0715\n",
      "Epoch 3/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0736 - val_loss: 0.0713\n",
      "Epoch 4/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0736 - val_loss: 0.0713\n",
      "Epoch 5/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0714\n",
      "Epoch 6/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0714\n",
      "Epoch 7/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 8/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 9/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 10/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 11/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 12/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 13/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 14/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 15/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 16/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 17/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 18/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 19/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 20/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 21/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 22/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 23/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 24/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 25/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0712\n",
      "Epoch 26/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 1s 5ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 27/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 28/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 29/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0712\n",
      "Epoch 30/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0712\n",
      "Epoch 31/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0712\n",
      "Epoch 32/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0714\n",
      "Epoch 33/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 34/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0713\n",
      "Epoch 35/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 36/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 37/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 38/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0713\n",
      "Epoch 39/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 40/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 41/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0711\n",
      "Epoch 42/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 43/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 44/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 45/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0712\n",
      "Epoch 46/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 47/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 48/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 49/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 50/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 51/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 52/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 53/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0711\n",
      "Epoch 54/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0711\n",
      "Epoch 55/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 56/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0734 - val_loss: 0.0711\n",
      "Epoch 57/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0711\n",
      "Epoch 58/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 59/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 60/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0711\n",
      "Epoch 61/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0711\n",
      "Epoch 62/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0712\n",
      "Epoch 63/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0711\n",
      "Epoch 64/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 65/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 66/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 67/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 1s 6ms/step - loss: 0.0733 - val_loss: 0.0712\n",
      "Epoch 68/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 1s 5ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 69/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0733 - val_loss: 0.0712\n",
      "Epoch 70/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 71/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 72/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0711\n",
      "Epoch 73/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0711\n",
      "Epoch 74/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 75/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 76/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 77/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 78/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 79/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0712\n",
      "Epoch 80/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 81/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 82/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 83/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 84/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0713\n",
      "Epoch 85/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 86/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 87/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 88/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 89/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 90/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 91/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 92/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 1s 5ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 93/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 1s 6ms/step - loss: 0.0733 - val_loss: 0.0712\n",
      "Epoch 94/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 1s 8ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 95/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 1s 7ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 96/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 1s 5ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 97/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 98/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 1s 5ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 99/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 100/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 101/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 102/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 103/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 104/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 105/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 106/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 107/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 108/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 109/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 110/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 111/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0712\n",
      "Epoch 112/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 113/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 114/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 115/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 116/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 117/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 118/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0710\n",
      "Epoch 119/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 120/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 121/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 122/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 123/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 124/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0710\n",
      "Epoch 125/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0710\n",
      "Epoch 126/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 127/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 128/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 129/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 130/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0712\n",
      "Epoch 131/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 132/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 133/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 134/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 135/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0710\n",
      "Epoch 136/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 137/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 138/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 139/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 140/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 141/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 142/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 143/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 144/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 145/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 146/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 147/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 148/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 149/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 150/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 151/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 152/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 153/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 154/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 155/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 156/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 157/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 158/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 159/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 160/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0712\n",
      "Epoch 161/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 162/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 163/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 164/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 165/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 1s 5ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 166/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 1s 6ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 167/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 168/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 169/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0712\n",
      "Epoch 170/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0712\n",
      "Epoch 171/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 172/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 173/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 174/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 175/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 176/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 177/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 178/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 179/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 180/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 181/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 182/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 183/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 184/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0712\n",
      "Epoch 185/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 186/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 187/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 188/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 189/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 190/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 191/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 192/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 193/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 194/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 195/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 196/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 197/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 198/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 199/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 200/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 201/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 202/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 203/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 204/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 205/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0712\n",
      "Epoch 206/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0712\n",
      "Epoch 207/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 208/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 209/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 210/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 211/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 212/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0714\n",
      "Epoch 213/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 214/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 215/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 216/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 217/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 218/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 219/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 220/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 221/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 222/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 223/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 224/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 225/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0710\n",
      "Epoch 226/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 227/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 228/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0710\n",
      "Epoch 229/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 230/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 231/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 232/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 233/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 234/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0710\n",
      "Epoch 235/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0710\n",
      "Epoch 236/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0729 - val_loss: 0.0710\n",
      "Epoch 237/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0712\n",
      "Epoch 238/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 239/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0713\n",
      "Epoch 240/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 241/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0729 - val_loss: 0.0710\n",
      "Epoch 242/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 5ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 243/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0709\n",
      "Epoch 244/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 245/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0710\n",
      "Epoch 246/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0710\n",
      "Epoch 247/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 248/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0710\n",
      "Epoch 249/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0710\n",
      "Epoch 250/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0710\n",
      "Epoch 251/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 252/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 253/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0712\n",
      "Epoch 254/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 255/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 256/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 257/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0710\n",
      "Epoch 258/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 259/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0728 - val_loss: 0.0712\n",
      "Epoch 260/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 261/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 262/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0710\n",
      "Epoch 263/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 264/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 265/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 266/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0710\n",
      "Epoch 267/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 268/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 269/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 270/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 271/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0712\n",
      "Epoch 272/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 273/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 274/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 275/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 276/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 277/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 278/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0712\n",
      "Epoch 279/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 280/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0712\n",
      "Epoch 281/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 282/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0712\n",
      "Epoch 283/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 284/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 285/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 286/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 287/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 288/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 289/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0710\n",
      "Epoch 290/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0712\n",
      "Epoch 291/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0712\n",
      "Epoch 292/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0712\n",
      "Epoch 293/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 294/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0713\n",
      "Epoch 295/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 296/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0712\n",
      "Epoch 297/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0712\n",
      "Epoch 298/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0710\n",
      "Epoch 299/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0712\n",
      "Epoch 300/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 301/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0712\n",
      "Epoch 302/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0727 - val_loss: 0.0710\n",
      "Epoch 303/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 304/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 305/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0710\n",
      "Epoch 306/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 307/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 308/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0712\n",
      "Epoch 309/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 310/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 311/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0712\n",
      "Epoch 312/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 313/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 314/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 315/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 316/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 317/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0712\n",
      "Epoch 318/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0710\n",
      "Epoch 319/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0713\n",
      "Epoch 320/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 321/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0710\n",
      "Epoch 322/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 323/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0712\n",
      "Epoch 324/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 325/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0710\n",
      "Epoch 326/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 327/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 328/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 329/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0710\n",
      "Epoch 330/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 331/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 332/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 333/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 334/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 335/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0713\n",
      "Epoch 336/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 337/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 338/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 339/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0713\n",
      "Epoch 340/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 341/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0713\n",
      "Epoch 342/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0710\n",
      "Epoch 343/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 344/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 345/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 346/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0713\n",
      "Epoch 347/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 348/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0710\n",
      "Epoch 349/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0712\n",
      "Epoch 350/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 351/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0713\n",
      "Epoch 352/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 353/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 354/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 355/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 356/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 357/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 358/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 359/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0713\n",
      "Epoch 360/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 361/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 362/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0713\n",
      "Epoch 363/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0713\n",
      "Epoch 364/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 365/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 366/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 5ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 367/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 368/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 369/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 370/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 371/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 372/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 373/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 7ms/step - loss: 0.0725 - val_loss: 0.0713\n",
      "Epoch 374/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 8ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 375/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 5ms/step - loss: 0.0725 - val_loss: 0.0710\n",
      "Epoch 376/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 377/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 378/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 379/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 380/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0714\n",
      "Epoch 381/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0713\n",
      "Epoch 382/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0714\n",
      "Epoch 383/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 384/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 385/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 386/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 387/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 388/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0713\n",
      "Epoch 389/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 390/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0714\n",
      "Epoch 391/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 392/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0713\n",
      "Epoch 393/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 394/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 395/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 396/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 397/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 5ms/step - loss: 0.0724 - val_loss: 0.0713\n",
      "Epoch 398/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 399/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 400/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0713\n",
      "Epoch 401/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 402/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 403/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 404/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 405/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0714\n",
      "Epoch 406/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0714\n",
      "Epoch 407/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 408/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0713\n",
      "Epoch 409/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 410/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 411/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0713\n",
      "Epoch 412/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 5ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 413/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 5ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 414/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0713\n",
      "Epoch 415/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 416/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0713\n",
      "Epoch 417/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 418/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0714\n",
      "Epoch 419/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 420/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 421/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0713\n",
      "Epoch 422/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0713\n",
      "Epoch 423/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 424/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0712\n",
      "Epoch 425/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0711\n",
      "Epoch 426/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 427/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 428/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0712\n",
      "Epoch 429/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 430/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 431/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0714\n",
      "Epoch 432/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 5ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 433/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 434/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0712\n",
      "Epoch 435/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 5ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 436/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0713\n",
      "Epoch 437/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0713\n",
      "Epoch 438/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0713\n",
      "Epoch 439/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 5ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 440/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 441/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0714\n",
      "Epoch 442/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0715\n",
      "Epoch 443/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0713\n",
      "Epoch 444/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 5ms/step - loss: 0.0724 - val_loss: 0.0714\n",
      "Epoch 445/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 5ms/step - loss: 0.0723 - val_loss: 0.0712\n",
      "Epoch 446/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 447/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0713\n",
      "Epoch 448/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 5ms/step - loss: 0.0723 - val_loss: 0.0713\n",
      "Epoch 449/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 450/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0713\n",
      "Epoch 451/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0723 - val_loss: 0.0712\n",
      "Epoch 452/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 5ms/step - loss: 0.0723 - val_loss: 0.0713\n",
      "Epoch 453/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0713\n",
      "Epoch 454/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0723 - val_loss: 0.0713\n",
      "Epoch 455/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0713\n",
      "Epoch 456/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0715\n",
      "Epoch 457/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0713\n",
      "Epoch 458/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0713\n",
      "Epoch 459/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 5ms/step - loss: 0.0723 - val_loss: 0.0714\n",
      "Epoch 460/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 7ms/step - loss: 0.0723 - val_loss: 0.0714\n",
      "Epoch 461/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0724 - val_loss: 0.0713\n",
      "Epoch 462/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 5ms/step - loss: 0.0723 - val_loss: 0.0712\n",
      "Epoch 463/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 7ms/step - loss: 0.0723 - val_loss: 0.0713\n",
      "Epoch 464/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 5ms/step - loss: 0.0723 - val_loss: 0.0713\n",
      "Epoch 465/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0712\n",
      "Epoch 466/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0712\n",
      "Epoch 467/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0712\n",
      "Epoch 468/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0723 - val_loss: 0.0713\n",
      "Epoch 469/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0715\n",
      "Epoch 470/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0714\n",
      "Epoch 471/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0724 - val_loss: 0.0713\n",
      "Epoch 472/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0713\n",
      "Epoch 473/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0723 - val_loss: 0.0714\n",
      "Epoch 474/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0712\n",
      "Epoch 475/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0713\n",
      "Epoch 476/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 477/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 5ms/step - loss: 0.0723 - val_loss: 0.0714\n",
      "Epoch 478/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0714\n",
      "Epoch 479/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0722 - val_loss: 0.0714\n",
      "Epoch 480/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0714\n",
      "Epoch 481/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0714\n",
      "Epoch 482/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0723 - val_loss: 0.0713\n",
      "Epoch 483/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 5ms/step - loss: 0.0723 - val_loss: 0.0714\n",
      "Epoch 484/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0714\n",
      "Epoch 485/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0712\n",
      "Epoch 486/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0714\n",
      "Epoch 487/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 488/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0714\n",
      "Epoch 489/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0712\n",
      "Epoch 490/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 491/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 492/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0713\n",
      "Epoch 493/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0713\n",
      "Epoch 494/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 495/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0714\n",
      "Epoch 496/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0713\n",
      "Epoch 497/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 498/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 499/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 500/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0715\n",
      "Epoch 501/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0712\n",
      "Epoch 502/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 5ms/step - loss: 0.0723 - val_loss: 0.0712\n",
      "Epoch 503/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 5ms/step - loss: 0.0722 - val_loss: 0.0715\n",
      "Epoch 504/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 6ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 505/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0722 - val_loss: 0.0714\n",
      "Epoch 506/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 507/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 508/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0714\n",
      "Epoch 509/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 510/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0714\n",
      "Epoch 511/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0722 - val_loss: 0.0714\n",
      "Epoch 512/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0714\n",
      "Epoch 513/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0714\n",
      "Epoch 514/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 515/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0722 - val_loss: 0.0714\n",
      "Epoch 516/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 517/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0714\n",
      "Epoch 518/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 519/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 520/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0716\n",
      "Epoch 521/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 522/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0712\n",
      "Epoch 523/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0714\n",
      "Epoch 524/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0713\n",
      "Epoch 525/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 526/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 527/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 528/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 529/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 530/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 531/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0721 - val_loss: 0.0712\n",
      "Epoch 532/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0712\n",
      "Epoch 533/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 534/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0713\n",
      "Epoch 535/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0715\n",
      "Epoch 536/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0713\n",
      "Epoch 537/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 538/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 539/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0713\n",
      "Epoch 540/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0712\n",
      "Epoch 541/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0715\n",
      "Epoch 542/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 543/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0714\n",
      "Epoch 544/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 545/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0715\n",
      "Epoch 546/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 547/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0712\n",
      "Epoch 548/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0716\n",
      "Epoch 549/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0713\n",
      "Epoch 550/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 551/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 552/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0715\n",
      "Epoch 553/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 554/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0712\n",
      "Epoch 555/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 556/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0721 - val_loss: 0.0715\n",
      "Epoch 557/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 6ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 558/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 559/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0713\n",
      "Epoch 560/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 561/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 562/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 563/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 564/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0715\n",
      "Epoch 565/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 566/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0716\n",
      "Epoch 567/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0715\n",
      "Epoch 568/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0713\n",
      "Epoch 569/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0713\n",
      "Epoch 570/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 5ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 571/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 5ms/step - loss: 0.0722 - val_loss: 0.0714\n",
      "Epoch 572/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 573/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 5ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 574/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0721 - val_loss: 0.0713\n",
      "Epoch 575/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0721 - val_loss: 0.0715\n",
      "Epoch 576/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 1s 5ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 577/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 578/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0715\n",
      "Epoch 579/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 580/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0720 - val_loss: 0.0713\n",
      "Epoch 581/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0712\n",
      "Epoch 582/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0716\n",
      "Epoch 583/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 584/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 585/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 586/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 587/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0715\n",
      "Epoch 588/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0716\n",
      "Epoch 589/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0716\n",
      "Epoch 590/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 591/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0713\n",
      "Epoch 592/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0713\n",
      "Epoch 593/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 594/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0713\n",
      "Epoch 595/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 596/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0715\n",
      "Epoch 597/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0713\n",
      "Epoch 598/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0715\n",
      "Epoch 599/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0715\n",
      "Epoch 600/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 601/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0713\n",
      "Epoch 602/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 603/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0713\n",
      "Epoch 604/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 605/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 606/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 607/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0715\n",
      "Epoch 608/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0715\n",
      "Epoch 609/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 610/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0715\n",
      "Epoch 611/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 612/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0720 - val_loss: 0.0716\n",
      "Epoch 613/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 614/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 615/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0715\n",
      "Epoch 616/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 617/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 618/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0715\n",
      "Epoch 619/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0716\n",
      "Epoch 620/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0721 - val_loss: 0.0715\n",
      "Epoch 621/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 622/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 623/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0715\n",
      "Epoch 624/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 625/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0713\n",
      "Epoch 626/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 627/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0715\n",
      "Epoch 628/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 629/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0715\n",
      "Epoch 630/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0715\n",
      "Epoch 631/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 632/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 633/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 634/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0715\n",
      "Epoch 635/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0716\n",
      "Epoch 636/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 637/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 638/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0715\n",
      "Epoch 639/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0713\n",
      "Epoch 640/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 641/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0715\n",
      "Epoch 642/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 643/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0713\n",
      "Epoch 644/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0715\n",
      "Epoch 645/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 646/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0716\n",
      "Epoch 647/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 648/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0716\n",
      "Epoch 649/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 650/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0716\n",
      "Epoch 651/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 652/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0715\n",
      "Epoch 653/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 654/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0716\n",
      "Epoch 655/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 656/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0715\n",
      "Epoch 657/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 658/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0716\n",
      "Epoch 659/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0715\n",
      "Epoch 660/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 661/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0715\n",
      "Epoch 662/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0713\n",
      "Epoch 663/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 664/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 665/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 666/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0713\n",
      "Epoch 667/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 668/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 669/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0715\n",
      "Epoch 670/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0715\n",
      "Epoch 671/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0715\n",
      "Epoch 672/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 673/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0716\n",
      "Epoch 674/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 675/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 676/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0715\n",
      "Epoch 677/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0715\n",
      "Epoch 678/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0713\n",
      "Epoch 679/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 680/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 681/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 682/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0715\n",
      "Epoch 683/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0713\n",
      "Epoch 684/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 685/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 686/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 687/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0713\n",
      "Epoch 688/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 689/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0716\n",
      "Epoch 690/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 691/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 692/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 693/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 694/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 695/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0717\n",
      "Epoch 696/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 697/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0715\n",
      "Epoch 698/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 699/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0717\n",
      "Epoch 700/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 701/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 702/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 703/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 704/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0715\n",
      "Epoch 705/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0715\n",
      "Epoch 706/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 707/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 708/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 1s 5ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 709/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 710/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 711/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 712/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 1s 5ms/step - loss: 0.0720 - val_loss: 0.0716\n",
      "Epoch 713/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 714/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 715/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 716/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0718\n",
      "Epoch 717/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 718/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 719/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 720/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 721/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 722/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 723/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 724/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0716\n",
      "Epoch 725/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 726/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 727/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0717\n",
      "Epoch 728/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0713\n",
      "Epoch 729/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 730/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0713\n",
      "Epoch 731/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 732/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 733/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 734/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 735/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0717\n",
      "Epoch 736/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 737/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0715\n",
      "Epoch 738/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 739/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 740/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0717\n",
      "Epoch 741/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 742/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0717\n",
      "Epoch 743/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0716\n",
      "Epoch 744/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 745/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 746/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 747/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 748/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 749/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 750/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 751/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 752/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 753/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0718\n",
      "Epoch 754/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 755/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 756/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 757/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 758/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0718\n",
      "Epoch 759/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 760/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 761/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 762/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 763/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 764/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0714\n",
      "Epoch 765/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 766/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0718\n",
      "Epoch 767/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 768/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 769/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 770/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 771/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 772/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0717\n",
      "Epoch 773/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 774/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 775/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 776/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 777/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 778/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 779/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 780/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0717\n",
      "Epoch 781/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0717\n",
      "Epoch 782/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 783/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 784/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0713\n",
      "Epoch 785/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 786/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 787/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 788/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 789/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 790/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 791/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0717\n",
      "Epoch 792/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 793/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 794/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 795/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0714\n",
      "Epoch 796/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0717\n",
      "Epoch 797/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0718\n",
      "Epoch 798/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0717\n",
      "Epoch 799/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 800/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 801/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 802/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 803/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 804/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 805/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 806/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 807/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 808/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 809/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0718\n",
      "Epoch 810/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0717\n",
      "Epoch 811/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0718\n",
      "Epoch 812/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 813/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 814/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0717\n",
      "Epoch 815/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 816/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 817/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 818/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 819/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 820/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 821/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 822/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 823/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 824/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 825/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 826/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0714\n",
      "Epoch 827/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 828/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0714\n",
      "Epoch 829/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 830/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 831/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 832/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 833/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0714\n",
      "Epoch 834/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0714\n",
      "Epoch 835/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0714\n",
      "Epoch 836/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 837/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 838/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 839/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 840/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 841/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 842/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 843/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 844/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 845/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 846/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 847/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0718\n",
      "Epoch 848/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 849/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 850/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 851/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 852/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 853/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 854/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0717\n",
      "Epoch 855/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 856/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 857/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 858/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 859/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0720\n",
      "Epoch 860/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 861/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 862/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0717\n",
      "Epoch 863/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 864/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 865/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 1s 5ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 866/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 867/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 868/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0717\n",
      "Epoch 869/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 870/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 871/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0717\n",
      "Epoch 872/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 873/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 874/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 875/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 876/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 877/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 878/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 879/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0718\n",
      "Epoch 880/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 881/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 882/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 883/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 884/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 885/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0717\n",
      "Epoch 886/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0714\n",
      "Epoch 887/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 888/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 889/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0714\n",
      "Epoch 890/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 891/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 892/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 893/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 894/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 895/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0718\n",
      "Epoch 896/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0718\n",
      "Epoch 897/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 898/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 899/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0717\n",
      "Epoch 900/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 901/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 902/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 903/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 904/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 905/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0717\n",
      "Epoch 906/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 907/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 908/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0717\n",
      "Epoch 909/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 1s 5ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 910/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 911/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 912/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0718 - val_loss: 0.0717\n",
      "Epoch 913/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 914/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 915/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0717\n",
      "Epoch 916/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0718\n",
      "Epoch 917/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 918/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 919/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 920/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 921/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0717\n",
      "Epoch 922/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0718\n",
      "Epoch 923/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 924/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0717 - val_loss: 0.0718\n",
      "Epoch 925/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 926/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 927/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 928/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0718\n",
      "Epoch 929/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0717\n",
      "Epoch 930/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 1s 5ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 931/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 1s 7ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 932/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 1s 5ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 933/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 934/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 935/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 936/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 1s 6ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 937/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 1s 5ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 938/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 939/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 1s 5ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 940/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 941/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 942/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 943/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 944/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 945/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0718\n",
      "Epoch 946/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 947/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 948/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 949/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 950/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0717\n",
      "Epoch 951/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 952/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 953/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 954/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 955/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 956/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 957/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 958/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 959/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 960/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0716\n",
      "Epoch 961/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 962/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 963/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 964/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 965/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0717 - val_loss: 0.0718\n",
      "Epoch 966/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0716 - val_loss: 0.0717\n",
      "Epoch 967/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 1s 5ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 968/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 969/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0717\n",
      "Epoch 970/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0717\n",
      "Epoch 971/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0718\n",
      "Epoch 972/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 973/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 974/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 975/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 976/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 977/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0718\n",
      "Epoch 978/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 979/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 980/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0719\n",
      "Epoch 981/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 982/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 983/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 984/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 985/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0717\n",
      "Epoch 986/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 987/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0716 - val_loss: 0.0716\n",
      "Epoch 988/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0718\n",
      "Epoch 989/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 990/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0717\n",
      "Epoch 991/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0717\n",
      "Epoch 992/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 993/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 994/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0717\n",
      "Epoch 995/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 996/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 997/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0717\n",
      "Epoch 998/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 999/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0714\n",
      "Epoch 1000/1000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0718\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Dense(100, activation='relu',\n",
    "                      input_shape=(len(feature_names),),\n",
    "                      kernel_initializer=initializers.he_normal(seed=0)))\n",
    "model.add(layers.Dense(100, activation='relu',\n",
    "                      kernel_initializer=initializers.he_normal(seed=0)))\n",
    "model.add(layers.Dropout(0.7))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=\"adam\",)\n",
    "\n",
    "print('start fitting')\n",
    "history = model.fit(dataset_train[feature_names], dataset_train['rank'],\n",
    "                    batch_size=1000,         #1000  #每一个batch的大小\n",
    "                    epochs=1000, #225          #迭代次数\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    #validation_data =        #(测试集的输入特征，测试集的标签），\n",
    "                    #validation_split =       # 从测试集中划分多少比例给训练集，\n",
    "                    #validation_freq = 20        #测试的epoch间隔数                     \n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2aa5d82b808>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(history.history['val_loss'][1:])\n",
    "plt.plot(history.history['loss'][1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lightgbm Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---cf.fit---\n",
      "LGBMRegressor(n_estimators=5000)\n",
      "---cf.score---\n",
      "-0.07241856722725859\n",
      "---predict---\n",
      "[0.38741047 0.57010251 0.72164692 ... 0.62166592 0.62779552 0.52954333]\n"
     ]
    }
   ],
   "source": [
    "##############################################　　　自己加入的　　　##############################################\n",
    "import lightgbm as lgb\n",
    "\n",
    "cf = lgb.LGBMRegressor(n_estimators=5000)\n",
    "\n",
    "train = dataset_train[feature_names] , dataset_train['return'] > 1\n",
    "test = dataset_test[feature_names] , dataset_test['return'] > 1 \n",
    "\n",
    "print('---cf.fit---')\n",
    "print(cf.fit(*train))\n",
    "print('---cf.score---')\n",
    "print(cf.score(*test))\n",
    "print('---predict---')\n",
    "print(cf.predict(test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 參數優化_1110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid's auc: 0.556627\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.523495\n",
      "Early stopping, best iteration is:\n",
      "[89]\tvalid's auc: 0.524762\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid's auc: 0.546677\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid's auc: 0.555464\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.52393\n",
      "Early stopping, best iteration is:\n",
      "[97]\tvalid's auc: 0.523971\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid's auc: 0.542674\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid's auc: 0.557736\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.526447\n",
      "Early stopping, best iteration is:\n",
      "[91]\tvalid's auc: 0.527263\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid's auc: 0.547987\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid's auc: 0.55653\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.525249\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[65]\tvalid's auc: 0.538642\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid's auc: 0.561896\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.535722\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid's auc: 0.548779\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[49]\tvalid's auc: 0.558973\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.520407\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid's auc: 0.540465\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's auc: 0.564232\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid's auc: 0.533737\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[44]\tvalid's auc: 0.549367\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid's auc: 0.559361\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.530157\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid's auc: 0.544298\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid's auc: 0.562724\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.530724\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid's auc: 0.547594\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid's auc: 0.558728\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid's auc: 0.529511\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[46]\tvalid's auc: 0.551244\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[25]\tvalid's auc: 0.560057\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.52229\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[10]\tvalid's auc: 0.548407\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid's auc: 0.562877\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid's auc: 0.529863\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid's auc: 0.546556\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[39]\tvalid's auc: 0.569728\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.542317\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid's auc: 0.556719\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid's auc: 0.558675\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.528778\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[46]\tvalid's auc: 0.546366\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.559337\n",
      "Early stopping, best iteration is:\n",
      "[85]\tvalid's auc: 0.563587\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.514841\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[33]\tvalid's auc: 0.545278\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.563957\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.543895\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.548472\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.553904\n",
      "Early stopping, best iteration is:\n",
      "[89]\tvalid's auc: 0.556572\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5179\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid's auc: 0.547049\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid's auc: 0.558764\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[33]\tvalid's auc: 0.520753\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.540572\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.555199\n",
      "Early stopping, best iteration is:\n",
      "[125]\tvalid's auc: 0.558066\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.533837\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.540405\n",
      "Early stopping, best iteration is:\n",
      "[106]\tvalid's auc: 0.541207\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid's auc: 0.567366\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.53126\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid's auc: 0.550863\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[49]\tvalid's auc: 0.555116\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.534117\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[56]\tvalid's auc: 0.540945\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's auc: 0.56362\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.542583\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid's auc: 0.552657\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid's auc: 0.559923\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.526643\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[35]\tvalid's auc: 0.547482\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid's auc: 0.558636\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid's auc: 0.520844\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's auc: 0.547176\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[21]\tvalid's auc: 0.551854\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.521857\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid's auc: 0.546883\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.560055\n",
      "Early stopping, best iteration is:\n",
      "[141]\tvalid's auc: 0.563262\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.52279\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid's auc: 0.552383\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[48]\tvalid's auc: 0.559008\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.523207\n",
      "Early stopping, best iteration is:\n",
      "[111]\tvalid's auc: 0.525577\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid's auc: 0.548114\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid's auc: 0.556974\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[67]\tvalid's auc: 0.514427\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.538533\n",
      "Early stopping, best iteration is:\n",
      "[82]\tvalid's auc: 0.541238\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.554323\n",
      "Early stopping, best iteration is:\n",
      "[82]\tvalid's auc: 0.558958\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.518101\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[35]\tvalid's auc: 0.549302\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[48]\tvalid's auc: 0.557895\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.517944\n",
      "Early stopping, best iteration is:\n",
      "[155]\tvalid's auc: 0.522284\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid's auc: 0.550457\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[39]\tvalid's auc: 0.563529\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid's auc: 0.524334\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid's auc: 0.545328\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.560193\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.534029\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[30]\tvalid's auc: 0.546787\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[39]\tvalid's auc: 0.558344\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.519823\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.547031\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid's auc: 0.559865\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.528086\n",
      "Early stopping, best iteration is:\n",
      "[105]\tvalid's auc: 0.529139\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[30]\tvalid's auc: 0.545235\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid's auc: 0.563365\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[64]\tvalid's auc: 0.528876\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid's auc: 0.548123\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[49]\tvalid's auc: 0.556417\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.531147\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's auc: 0.544042\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid's auc: 0.559542\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.532866\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[31]\tvalid's auc: 0.541606\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[47]\tvalid's auc: 0.559217\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.51913\n",
      "Early stopping, best iteration is:\n",
      "[120]\tvalid's auc: 0.52137\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's auc: 0.545677\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.558073\n",
      "Early stopping, best iteration is:\n",
      "[89]\tvalid's auc: 0.560474\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.529411\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid's auc: 0.545301\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.561702\n",
      "Early stopping, best iteration is:\n",
      "[71]\tvalid's auc: 0.563056\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.523936\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid's auc: 0.54731\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid's auc: 0.563775\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.528036\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[46]\tvalid's auc: 0.549925\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[53]\tvalid's auc: 0.55873\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[67]\tvalid's auc: 0.522553\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[24]\tvalid's auc: 0.548783\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[59]\tvalid's auc: 0.556946\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.516532\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid's auc: 0.54856\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalid's auc: 0.565476\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.528036\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[44]\tvalid's auc: 0.548223\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[39]\tvalid's auc: 0.556213\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.526542\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid's auc: 0.547412\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.556381\n",
      "Early stopping, best iteration is:\n",
      "[98]\tvalid's auc: 0.556864\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.517142\n",
      "Early stopping, best iteration is:\n",
      "[75]\tvalid's auc: 0.52065\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[56]\tvalid's auc: 0.540901\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[57]\tvalid's auc: 0.559502\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[67]\tvalid's auc: 0.520441\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalid's auc: 0.54395\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.560285\n",
      "Early stopping, best iteration is:\n",
      "[77]\tvalid's auc: 0.564441\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.528036\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid's auc: 0.5484\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[65]\tvalid's auc: 0.55605\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.545006\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid's auc: 0.543237\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's auc: 0.564204\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.548005\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid's auc: 0.554595\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalid's auc: 0.563345\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[47]\tvalid's auc: 0.527042\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid's auc: 0.549255\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid's auc: 0.56392\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid's auc: 0.529055\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid's auc: 0.551534\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.554712\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.524495\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid's auc: 0.547695\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid's auc: 0.560946\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid's auc: 0.526827\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[47]\tvalid's auc: 0.549842\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[46]\tvalid's auc: 0.559166\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[44]\tvalid's auc: 0.521271\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid's auc: 0.544764\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.561811\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.533354\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[58]\tvalid's auc: 0.544578\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid's auc: 0.562859\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.526987\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid's auc: 0.551983\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.557475\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.533007\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[35]\tvalid's auc: 0.543787\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid's auc: 0.555705\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.524182\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[10]\tvalid's auc: 0.54601\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid's auc: 0.554939\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.536822\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid's auc: 0.546135\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid's auc: 0.56047\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.523044\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid's auc: 0.548782\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid's auc: 0.561716\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.529332\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[30]\tvalid's auc: 0.545956\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's auc: 0.562797\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.562267\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid's auc: 0.55249\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[39]\tvalid's auc: 0.562793\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid's auc: 0.524574\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid's auc: 0.548691\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid's auc: 0.565362\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.518599\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid's auc: 0.545313\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[48]\tvalid's auc: 0.558415\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.523467\n",
      "Early stopping, best iteration is:\n",
      "[111]\tvalid's auc: 0.525372\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[35]\tvalid's auc: 0.545756\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid's auc: 0.558313\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.530796\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's auc: 0.54635\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.56403\n",
      "Early stopping, best iteration is:\n",
      "[84]\tvalid's auc: 0.564976\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.531596\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[52]\tvalid's auc: 0.54994\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalid's auc: 0.556695\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.523688\n",
      "Early stopping, best iteration is:\n",
      "[120]\tvalid's auc: 0.526676\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid's auc: 0.544251\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid's auc: 0.560461\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.530587\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid's auc: 0.544042\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.558286\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.541597\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[47]\tvalid's auc: 0.547076\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.558582\n",
      "Early stopping, best iteration is:\n",
      "[90]\tvalid's auc: 0.559935\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.531335\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid's auc: 0.545145\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.560685\n",
      "Early stopping, best iteration is:\n",
      "[75]\tvalid's auc: 0.562202\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.527364\n",
      "Early stopping, best iteration is:\n",
      "[85]\tvalid's auc: 0.528589\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[58]\tvalid's auc: 0.547732\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[44]\tvalid's auc: 0.559129\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid's auc: 0.51908\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[30]\tvalid's auc: 0.545502\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid's auc: 0.560434\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.523225\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.547271\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[48]\tvalid's auc: 0.553333\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.519239\n",
      "Early stopping, best iteration is:\n",
      "[80]\tvalid's auc: 0.520643\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[21]\tvalid's auc: 0.54249\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.561143\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.52967\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[31]\tvalid's auc: 0.544344\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid's auc: 0.559283\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.540188\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[35]\tvalid's auc: 0.545608\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[46]\tvalid's auc: 0.559483\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.524115\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.548241\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[48]\tvalid's auc: 0.556398\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.52501\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid's auc: 0.544113\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid's auc: 0.558352\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.522796\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's auc: 0.551636\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.555229\n",
      "Early stopping, best iteration is:\n",
      "[76]\tvalid's auc: 0.556834\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.522171\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid's auc: 0.541883\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid's auc: 0.557912\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.533668\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[32]\tvalid's auc: 0.552954\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's auc: 0.562996\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.529709\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid's auc: 0.555146\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid's auc: 0.563314\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[61]\tvalid's auc: 0.532466\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid's auc: 0.552237\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's auc: 0.559593\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.537718\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid's auc: 0.545277\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.554188\n",
      "Early stopping, best iteration is:\n",
      "[83]\tvalid's auc: 0.557553\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.519318\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's auc: 0.548936\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid's auc: 0.561847\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[53]\tvalid's auc: 0.517653\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid's auc: 0.545813\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.559601\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.539549\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.540915\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.559295\n",
      "Early stopping, best iteration is:\n",
      "[83]\tvalid's auc: 0.559905\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.530087\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid's auc: 0.548404\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[61]\tvalid's auc: 0.548585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=LGBMClassifier(metric='None', n_estimators=5000,\n",
       "                                            n_jobs=4, random_state=314),\n",
       "                   n_iter=100,\n",
       "                   param_distributions={'colsample_bytree': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000002AA62161DC8>,\n",
       "                                        'min_child_samples': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000002AA5EE81408>,\n",
       "                                        'min_child_weight': [1e-05, 0.001, 0.01,\n",
       "                                                             0.1, 1, 10.0,\n",
       "                                                             100.0, 1000.0,\n",
       "                                                             10000.0],\n",
       "                                        'num_leaves': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000002AA62161648>,\n",
       "                                        'reg_alpha': [0, 0.1, 1, 2, 5, 7, 10,\n",
       "                                                      50, 100],\n",
       "                                        'reg_lambda': [0, 0.1, 1, 5, 10, 20, 50,\n",
       "                                                       100],\n",
       "                                        'subsample': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000002AA62161D08>},\n",
       "                   random_state=314, scoring='roc_auc', verbose=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import lightgbm\n",
    "\n",
    "fit_params={\"early_stopping_rounds\":30, \n",
    "            \"eval_metric\" : 'auc', \n",
    "            \"eval_set\" : [test],\n",
    "            'eval_names': ['valid'],\n",
    "            'verbose': 100,\n",
    "            'categorical_feature': 'auto'}\n",
    "\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "param_test ={'num_leaves': sp_randint(6, 50), \n",
    "             'min_child_samples': sp_randint(100, 500), \n",
    "             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "             'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
    "             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
    "             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n",
    "\n",
    "#This parameter defines the number of HP points to be tested\n",
    "n_HP_points_to_test = 100\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "#n_estimators is set to a \"large value\". The actual number of trees build will depend on early stopping and 5000 define only the absolute maximum\n",
    "clf = lgb.LGBMClassifier(max_depth=-1, random_state=314, silent=True, metric='None', n_jobs=4, n_estimators=5000)\n",
    "gs = RandomizedSearchCV(\n",
    "    estimator=clf, param_distributions=param_test, \n",
    "    n_iter=n_HP_points_to_test,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    refit=True,\n",
    "    random_state=314,\n",
    "    verbose=True)\n",
    "\n",
    "gs.fit(*train, **fit_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(colsample_bytree=0.9442991955065878, metric='None',\n",
       "               min_child_samples=152, min_child_weight=1000.0,\n",
       "               n_estimators=5000, n_jobs=4, num_leaves=38, random_state=314,\n",
       "               reg_alpha=10, reg_lambda=0.1, subsample=0.2676349956143538)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#gs.best_estimator_\n",
    "#LGBMClassifier(colsample_bytree=0.6433117836032942, metric='None',\n",
    "#               min_child_samples=224, min_child_weight=1e-05, n_estimators=5000,\n",
    "#               n_jobs=4, num_leaves=20, random_state=314, reg_alpha=10,\n",
    "#               reg_lambda=10, subsample=0.8945613420997809)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid's auc: 0.548978\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.005342788722115577"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf = lgb.LGBMRegressor(colsample_bytree=0.4893610123694421, metric='None',\n",
    "               min_child_samples=325, n_estimators=5000, n_jobs=4,\n",
    "               num_leaves=44, random_state=314, reg_alpha=7, reg_lambda=20,\n",
    "               subsample=0.3338474163716765)\n",
    "\n",
    "cf.fit(dataset_train[feature_names],dataset_train['return'] > 1, **fit_params)\n",
    "cf.score(dataset_test[feature_names],dataset_test['return'] > 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Training until validation scores don't improve for 30 rounds.\n",
    "[100]\tvalid's auc: 0.551389\n",
    "Early stopping, best iteration is:\n",
    "[89]\tvalid's auc: 0.553699\n",
    "0.003857956228475623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(cf.fit(*train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import lightgbm as lgb\n",
    "#gbm = lgb.LGBMClassifier(n_estimators=100, random_state=5, learning_rate=0.01)\n",
    "#gbm.fit(dataset_train[feature_names], dataset_train['return'] > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tqdm\n",
    "#\n",
    "#n = 3\n",
    "#\n",
    "#X = []\n",
    "#y = []\n",
    "#indexes = []\n",
    "#dataset_scaled_x = dataset_scaled[feature_names]\n",
    "#\n",
    "#for i in tqdm.tqdm_notebook(range(0, len(dataset_scaled)-n)):\n",
    "#    X.append(dataset_scaled_x.iloc[i:i+n].values)\n",
    "#    y.append(dataset_scaled['return'].iloc[i+n-1])\n",
    "#    indexes.append(dataset_scaled.index[i+n-1])\n",
    "##dataset_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#X = np.array(X)\n",
    "#y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import lightgbm as lgb\n",
    "#cf = lgb.LGBMRegressor(colsample_bytree=0.7740467183023685, metric='None',\n",
    "#               min_child_samples=395, min_child_weight=0.01, n_estimators=5000,\n",
    "#               n_jobs=4, num_leaves=9, random_state=314, reg_alpha=5,\n",
    "#               reg_lambda=10, subsample=0.4643892520208455)\n",
    "#    \n",
    "#cf.fit(dataset_train[feature_names].astype(float), dataset_train['rank'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "cf2 = RandomForestRegressor(n_estimators=100)\n",
    "cf2.fit(dataset_train[feature_names].astype(float), dataset_train['rank'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 參數優化_1110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint \n",
    "from sklearn.model_selection import RandomizedSearchCV \n",
    "# build a classifier \n",
    "clf = RandomForestRegressor(n_estimators=100) \n",
    "# specify parameters and distributions to sample from \n",
    "param_dist = {\"max_depth\": [3, None], \n",
    "              \"max_features\": sp_randint(1, 11), \n",
    "              \"min_samples_split\": sp_randint(2, 11), \n",
    "              \"min_samples_leaf\": sp_randint(1, 11), \n",
    "              \"bootstrap\": [True, False], \n",
    "              \"criterion\": [\"mse\", \"mae\"]} \n",
    "# run randomized search \n",
    "n_iter_search = 20 \n",
    "rs = RandomizedSearchCV(clf, param_distributions=param_dist, \n",
    "                                   n_iter=n_iter_search) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "##rs.fit(dataset_train[features], dataset_train['return'] > 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split Train Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Value', ylabel='Feature'>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_imp = pd.DataFrame(zip(cf.feature_importances_, feature_names), \n",
    "                           columns=['Value','Feature']).sort_values('Value', ascending=False)\n",
    "feature_imp\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = dataset.index.get_level_values('date') < '2021'\n",
    "dataset_train = dataset[select]\n",
    "dataset_test = dataset[~select]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_drop = dataset.dropna(subset=feature_names+['return'])\n",
    "\n",
    "vals = model.predict(dataset_drop[feature_names].astype(float))\n",
    "dataset_drop['result1'] = pd.Series(vals.swapaxes(0,1)[0], dataset_drop.index)\n",
    "\n",
    "vals = cf.predict(dataset_drop[feature_names].astype(float))\n",
    "dataset_drop['result2'] = pd.Series(vals, dataset_drop.index)\n",
    "\n",
    "vals = cf2.predict(dataset_drop[feature_names].astype(float))\n",
    "dataset_drop['result3'] = pd.Series(vals, dataset_drop.index)\n",
    "\n",
    "dataset_drop = dataset_drop.reset_index().set_index(\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把量加進來做篩選\n",
    " * https://hahow.in/courses/5b9d3a6dca498a001e917383/shapeussions/60c96f5b018697e8a6131cbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>stock_id</th>\n",
       "      <th>0015</th>\n",
       "      <th>0050</th>\n",
       "      <th>0051</th>\n",
       "      <th>0052</th>\n",
       "      <th>0053</th>\n",
       "      <th>0054</th>\n",
       "      <th>0055</th>\n",
       "      <th>0056</th>\n",
       "      <th>0057</th>\n",
       "      <th>0058</th>\n",
       "      <th>...</th>\n",
       "      <th>9944</th>\n",
       "      <th>9945</th>\n",
       "      <th>9946</th>\n",
       "      <th>9949</th>\n",
       "      <th>9950</th>\n",
       "      <th>9951</th>\n",
       "      <th>9955</th>\n",
       "      <th>9958</th>\n",
       "      <th>9960</th>\n",
       "      <th>9962</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-04-23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-04-24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-04-25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-04-26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-04-27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-02</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-05</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-06</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-07</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-08</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3860 rows × 2011 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "stock_id    0015  0050  0051  0052  0053  0054  0055  0056  0057  0058  ...  \\\n",
       "date                                                                    ...   \n",
       "2007-04-23   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2007-04-24   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2007-04-25   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2007-04-26   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2007-04-27   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "...          ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "2022-12-02   0.0   1.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0  ...   \n",
       "2022-12-05   0.0   1.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0  ...   \n",
       "2022-12-06   0.0   1.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0  ...   \n",
       "2022-12-07   0.0   1.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0  ...   \n",
       "2022-12-08   0.0   1.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0  ...   \n",
       "\n",
       "stock_id    9944  9945  9946  9949  9950  9951  9955  9958  9960  9962  \n",
       "date                                                                    \n",
       "2007-04-23   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2007-04-24   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2007-04-25   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2007-04-26   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2007-04-27   1.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0  \n",
       "...          ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "2022-12-02   0.0   1.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0  \n",
       "2022-12-05   0.0   1.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0  \n",
       "2022-12-06   0.0   1.0   0.0   0.0   0.0   0.0   1.0   1.0   0.0   0.0  \n",
       "2022-12-07   0.0   1.0   0.0   0.0   0.0   0.0   1.0   1.0   0.0   0.0  \n",
       "2022-12-08   0.0   1.0   0.0   0.0   0.0   0.0   1.0   1.0   0.0   0.0  \n",
       "\n",
       "[3860 rows x 2011 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#把量加進來\n",
    "vol=data.get('成交股數')/1000\n",
    "vol_ma5=vol.rolling(5).mean()\n",
    "\n",
    "vol_filter=vol_ma5>1000\n",
    "vol_filter=vol_filter[vol_filter].fillna(0).astype(float)\n",
    "vol_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#t1 = vol_ma5.iloc[-1].dropna()\n",
    "#t1.to_csv('./tmp/132.csv')\n",
    "#t1.hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='date'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "dates = sorted(list(set(dataset_drop.index)))\n",
    "\n",
    "rs = []\n",
    "for d in dates:\n",
    "    \n",
    "    dataset_time = dataset_drop.loc[d]\n",
    "    \n",
    "    dataset_time = drop_extreme_case(dataset_time , list1 , thresh=0.01)\n",
    "    \n",
    "    rank = (dataset_time['result1'] + dataset_time['result2'] + dataset_time['result3'] ) #* vol_filter.loc[d]\n",
    "\n",
    "\n",
    "    condition = (rank >= rank.nlargest(20).iloc[-1]) \n",
    "    \n",
    "    r = dataset_time['return'][condition].mean()\n",
    "\n",
    "    rs.append(r * (1-3/1000-1.425/1000*2*0.6))\n",
    "\n",
    "rs = pd.Series(rs, index=dates)['2021':].cumprod()\n",
    "\n",
    "s0050 = close['0050']['2021':]\n",
    "\n",
    "pd.DataFrame({'nn strategy return':rs.reindex(s0050.index, method='ffill'), '0050 return':s0050/s0050[0]}).plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#\n",
    "#return_history_1026 = pd.Series(rs, index=dates)['2021':].cumprod()\n",
    "##eq = (gain[hold == 1].mean(axis=1)).fillna(1).cumprod()\n",
    "#\n",
    "#pickle.dump(rs, open('return_history_1026.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyfolio as pf\n",
    "#import pandas as pd\n",
    "#\n",
    "#close.index = close.index.tz_localize(\"Asia/Taipei\")\n",
    "##pf.create_returns_tear_sheet(close['0050'].pct_change())\n",
    "#\n",
    "## 得到 上一個單元的 回測結果\n",
    "#ret = pickle.load(open(\"return_history_1026.pkl\", \"rb\"))\n",
    "#\n",
    "## 將回測報酬率取出來\n",
    "#ret = ret.pct_change().dropna()\n",
    "#ret.index = pd.to_datetime(ret.index).tz_localize('Asia/Taipei')\n",
    "#\n",
    "## 利用pyfolio 比較報酬率\n",
    "#pf.create_returns_tear_sheet(ret, benchmark_rets=close['0050'].pct_change())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 當月持股狀況"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.index.levels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the latest dataset\n",
    "last_date = dataset.index.levels[1].max()#\"2022-10-15\"\n",
    "is_last_date = dataset.index.get_level_values('date') == last_date\n",
    "last_dataset = dataset[is_last_date].copy()\n",
    "\n",
    "\n",
    "last_dataset = drop_extreme_case(last_dataset, list1 , thresh=0.01)\n",
    "\n",
    "\n",
    "# remove NaN testcases\n",
    "last_dataset = last_dataset.dropna(subset=feature_names)\n",
    "\n",
    "# predict\n",
    "\n",
    "vals = model.predict(last_dataset[feature_names].astype(float))\n",
    "last_dataset['result1'] = pd.Series(vals.swapaxes(0,1)[0], last_dataset.index)\n",
    "\n",
    "vals = cf.predict(last_dataset[feature_names].astype(float))\n",
    "last_dataset['result2'] = pd.Series(vals, last_dataset.index)\n",
    "\n",
    "vals = cf2.predict(last_dataset[feature_names].astype(float))\n",
    "last_dataset['result3'] = pd.Series(vals, last_dataset.index)\n",
    "\n",
    "\n",
    "# calculate score\n",
    "\n",
    "rank = last_dataset['result1'] + last_dataset['result2'] + last_dataset['result3']\n",
    "\n",
    "#\n",
    "rank = rank * vol_filter.iloc[-1] #******加上量的濾網\n",
    "\n",
    "condition = (rank >= rank.nlargest(20).iloc[-1])\n",
    "#vol_filter\n",
    "\n",
    "# plot rank distribution\n",
    "rank.hist(bins=20)\n",
    "\n",
    "\n",
    "# show the best 20 stocks\n",
    "slist1 = rank[condition].reset_index()['stock_id']\n",
    "\n",
    "#https://keras-cn.readthedocs.io/en/latest/models/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stock_id\n",
       "0015    0.0\n",
       "0050    1.0\n",
       "0051    0.0\n",
       "0052    0.0\n",
       "0053    0.0\n",
       "       ... \n",
       "9951    0.0\n",
       "9955    1.0\n",
       "9958    1.0\n",
       "9960    0.0\n",
       "9962    0.0\n",
       "Name: 2022-12-08 00:00:00, Length: 2011, dtype: float64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol_filter.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date = dataset.index.levels[1].max()#\"2022-10-15\"\n",
    "is_last_date = dataset.index.get_level_values('date') == last_date\n",
    "last_dataset = dataset[is_last_date].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank.sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 平均分配資產於股票之中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "股票平分張數:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "stock_id\n",
       "2108    0.081633\n",
       "2313    0.060729\n",
       "2368    0.031513\n",
       "2392    0.076923\n",
       "2610    0.155440\n",
       "2618    0.106952\n",
       "2834    0.235294\n",
       "3006    0.043103\n",
       "3038    0.144231\n",
       "3231    0.107914\n",
       "3264    0.059172\n",
       "3317    0.050167\n",
       "3372    0.253165\n",
       "3711    0.030928\n",
       "4977    0.027273\n",
       "6138    0.022472\n",
       "6176    0.028302\n",
       "6190    0.091185\n",
       "8091    0.032859\n",
       "8155    0.025641\n",
       "Name: 2022-12-08 00:00:00, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close = data.get(\"收盤價\")\n",
    "\n",
    "money = 60000\n",
    "stock_prices = close[slist1].iloc[-1]\n",
    "\n",
    "\n",
    "print(\"股票平分張數:\")\n",
    "money / len(stock_prices) / stock_prices / 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@deathbeds/ipydrawio": {
   "xml": "<mxfile host=\"17-0536659-02\" modified=\"2022-10-27T03:01:05.740Z\" agent=\"5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\" etag=\"8bODyUWCdQaexky56D9k\" version=\"20.2.8\" type=\"embed\"><diagram id=\"9nsO6hMlLNMTvIbReD-d\" name=\"第1頁\"><mxGraphModel dx=\"1458\" dy=\"721\" grid=\"1\" gridSize=\"10\" guides=\"1\" tooltips=\"1\" connect=\"1\" arrows=\"1\" fold=\"1\" page=\"1\" pageScale=\"1\" pageWidth=\"827\" pageHeight=\"1169\" math=\"0\" shadow=\"0\"><root><mxCell id=\"0\"/><mxCell id=\"1\" parent=\"0\"/><UserObject label=\"Tree Root\" treeRoot=\"1\" id=\"2\"><mxCell style=\"align=center;collapsible=0;container=1;recursiveResize=0;\" parent=\"1\" vertex=\"1\"><mxGeometry x=\"40\" y=\"40\" width=\"120\" height=\"60\" as=\"geometry\"/></mxCell></UserObject></root></mxGraphModel></diagram></mxfile>"
  },
  "kernelspec": {
   "display_name": "finlab",
   "language": "python",
   "name": "finlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
