{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[版本連結](http://localhost:8888/doc/tree/!%25E9%2581%25B8%25E8%2582%25A1%25E5%259F%25B7%25E8%25A1%258C_%25E7%2589%2588%25E6%259C%25AC%25E6%25BC%2594%25E9%2580%25B2.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 移除不必要的警告\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 獲取歷史資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from finlab.data import Data\n",
    "#from finlab.ml import fundamental_features\n",
    "#fdf = fundamental_features()\n",
    "\n",
    "data = Data()\n",
    "\n",
    "close = data.get(\"收盤價\")\n",
    "open_ = data.get(\"開盤價\")\n",
    "high = data.get(\"最高價\")\n",
    "low = data.get(\"最低價\")\n",
    "vol = data.get(\"成交股數\")\n",
    "\n",
    "PB = data.get(\"股價淨值比\")\n",
    "pe = data.get(\"本益比\")\n",
    "\n",
    "#bi = data.get(\"營業利益\")\n",
    "\n",
    "\n",
    "#close = data.get_adj(\"收盤價\").round(2)\n",
    "\n",
    "#財務指標\n",
    "rev = data.get(\"當月營收\")\n",
    "l_rev = data.get(\"去年當月營收\")\n",
    "\n",
    "#t123 = data.get('土地')\n",
    "\n",
    "bargin_i=data.get(\"投信買賣超股數\")/data.get(\"成交股數\")\n",
    "bargin_f=data.get(\"外資自營商買賣超股數\")/data.get(\"成交股數\")\n",
    "bargin_s=data.get(\"自營商買賣超股數(自行買賣)\")/data.get(\"成交股數\")\n",
    "#\n",
    "\n",
    "vol=data.get('成交股數')/1000\n",
    "vol_ma5=vol.rolling(5).mean()\n",
    "\n",
    "rev.index = rev.index.shift(5, \"d\")         #每月頻率\n",
    "#周頻率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MFI  = data.talib(\"MFI\")\n",
    "##MFI.tail()\n",
    "#ub,mb,lb =data.talib(\"BBANDS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 營收相關"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################　　　自己加入的　　　##############################################\n",
    "import pandas as pd\n",
    "from finlab.__init__ import talib_all_stock\n",
    "from talib import abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias(n):\n",
    "    return close / close.rolling(n, min_periods=1).mean()\n",
    "\n",
    "def acc(n):\n",
    "    return close.shift(n) / (close.shift(2*n) + close) * 2\n",
    "\n",
    "def rsv(n):\n",
    "    l = close.rolling(n, min_periods=1).min() \n",
    "    h = close.rolling(n, min_periods=1).max()\n",
    "    \n",
    "    return (close - l) / (h - l)\n",
    "\n",
    "def mom(n):\n",
    "    return (rev / rev.shift(1)).shift(n)\n",
    "\n",
    "def yoy(n):\n",
    "    return (rev.shift(n) / rev.shift(12+n)) -1\n",
    "\n",
    "def bi(n):\n",
    "    return bargin_i/bargin_i.rolling(n).mean()\n",
    "\n",
    "def bf(n):\n",
    "    return bargin_f/bargin_f.rolling(n).mean()\n",
    "    \n",
    "def bs(n):\n",
    "    return bargin_s/bargin_s.rolling(n).mean()\n",
    "\n",
    "#-------------------------------------------\n",
    "\n",
    "features = {\n",
    "    'mom1': mom(1),\n",
    "    'mom2': mom(2),\n",
    "    'mom3': mom(3),\n",
    "    'mom4': mom(4),\n",
    "    'mom5': mom(5),\n",
    "    'mom6': mom(6),\n",
    "    'mom7': mom(7),\n",
    "    'mom8': mom(8),\n",
    "    'mom9': mom(9),\n",
    "\n",
    "    \n",
    "    'bias5': bias(5),\n",
    "    'bias10': bias(10),\n",
    "    'bias20': bias(20),\n",
    "    'bias60': bias(60),\n",
    "    'bias120': bias(120),\n",
    "    'bias240': bias(240),\n",
    "    \n",
    "    'acc5': acc(5),\n",
    "    'acc10': acc(10),\n",
    "    'acc20': acc(20),\n",
    "    'acc60': acc(60),\n",
    "    'acc120': acc(120),\n",
    "    'acc240': acc(240),\n",
    "    \n",
    "    'rsv5': rsv(5),\n",
    "    'rsv10': rsv(10),\n",
    "    'rsv20': rsv(20),\n",
    "    'rsv60': rsv(60),\n",
    "    'rsv120': rsv(120),\n",
    "    'rsv240': rsv(240),\n",
    "###############################################\n",
    "    'yoy': yoy(1),\n",
    "    'delta_yoy':yoy(1)-yoy(2),\n",
    "    \n",
    "    'PB':PB,\n",
    "    'PE':pe,\n",
    "    \n",
    "   #'bi5' : bi(5),\n",
    "   #'bi10' : bi(10),\n",
    "   #'bi20' : bi(20),\n",
    "   #'bi60' : bi(60),\n",
    "   # \n",
    "   #'bf5' : bf(5),\n",
    "   # #'bf10' : bf(10),\n",
    "   # #'bf20' : bf(20),\n",
    "   # #'bf60' : bf(60),\n",
    "   # \n",
    "   # 'bs5' : bs(5),\n",
    "   # 'bs10' : bs(10),\n",
    "   # 'bs20' : bs(20),\n",
    "   # 'bs60' : bs(60),\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bi(60).dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 財報指標"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "兩個feature結合[[連結網址]](https://hahow.in/courses/5b9d3a6dca498a001e917383/discussions/5d18b63eac23d80020ae4ce7)\n",
    "\n",
    "---\n",
    "```python\n",
    "from finlab import ml\n",
    "from finlab.data import Data\n",
    "\n",
    "data = Data()\n",
    "rsi = data.talib(\"RSI\")\n",
    "\n",
    "dataset = ml.fundamental_features()\n",
    "ml.add_feature(dataset, 'RSI', rsi)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finlab.ml import fundamental_features\n",
    "dataset_fundamental = fundamental_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_fundamental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 技術指標"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# https://zhuanlan.zhihu.com/p/342075180 talib函数功能一览表\n",
    "\n",
    "def bias(n):\n",
    "    return close / close.rolling(n, min_periods=1).mean()\n",
    "\n",
    "def acc(n):\n",
    "    return close.shift(n) / (close.shift(2*n) + close) * 2\n",
    "\n",
    "def rsv(n):\n",
    "    l = close.rolling(n, min_periods=1).min()\n",
    "    h = close.rolling(n, min_periods=1).max()\n",
    "    \n",
    "    return (close - l) / (h - l)\n",
    "\n",
    "def mom(n):\n",
    "    return (rev / rev.shift(1)).shift(n)\n",
    "\n",
    "\n",
    "def bi_(n):\n",
    "    return (bargin_i / vol.shift(1)).shift(n)\n",
    "\n",
    "def bf(n):\n",
    "    return (bargin_f / vol.shift(1)).shift(n)\n",
    "    \n",
    "def bs(n):\n",
    "    return (bargin_s / vol.shift(1)).shift(n)\n",
    "\n",
    "def rsi(n):\n",
    "    #return talib_all_stock(ndays=10000, func=abstract.RSI, timeperiod=n)\n",
    "    return data.talib(\"RSI\",timeperiod=n)\n",
    "\n",
    "def MFI(n):\n",
    "    return data.talib(\"MFI\",timeperiod=n)\n",
    "\n",
    "def obv(n):\n",
    "    return data.talib(\"OBV\",timeperiod=n)\n",
    "\n",
    "\n",
    "\n",
    "features = {\n",
    "    \n",
    "    #'ATR14':data.talib(\"ATR\",timeperiod=14),\n",
    "    #'NATR14':data.talib('NATR',timeperiod=14),\n",
    "    #'TRANGE':data.talib('TRANGE'),\n",
    "    #'Adosc3':data.talib('ADOSC',timeperiod=3),\n",
    "    \n",
    "    #\"MFI5\":MFI(5),\n",
    "    #\"MFI10\":MFI(10),\n",
    "    \n",
    "    #'rsi6': rsi(6),  #DataFrame\n",
    "    #'rsi10': rsi(10),  #DataFrame\n",
    "    #'rsi14': rsi(14),  #DataFrame\n",
    "    #'rsi20': rsi(20),  #DataFrame\n",
    "    #'rsi50': rsi(50),  #DataFrame\n",
    "   \n",
    "    'mom1': mom(1),\n",
    "    'mom2': mom(2),\n",
    "    'mom3': mom(3),\n",
    "    'mom4': mom(4),\n",
    "    'mom5': mom(5),\n",
    "    'mom6': mom(6),\n",
    "    'mom7': mom(7),\n",
    "    'mom8': mom(8),\n",
    "    'mom9': mom(9),\n",
    "    \n",
    "    'yoy': yoy(1),\n",
    "    'delta_yoy':yoy(1)-yoy(2),\n",
    "    \n",
    "#    'ff':ff,\n",
    "    'PB':PB,\n",
    "    'PE':pe,   \n",
    "#  \n",
    "    'bias5': bias(5),\n",
    "    'bias10': bias(10),\n",
    "    'bias20': bias(20),\n",
    "    'bias60': bias(60),\n",
    "    'bias120': bias(120),\n",
    "    'bias240': bias(240),\n",
    "    \n",
    "    'acc5': acc(5),\n",
    "    'acc10': acc(10),\n",
    "    'acc20': acc(20),\n",
    "    'acc60': acc(60),\n",
    "    'acc120': acc(120),\n",
    "    'acc240': acc(240),\n",
    "    \n",
    "    #'rsv5': rsv(5),\n",
    "    #'rsv10': rsv(10),\n",
    "    #'rsv20': rsv(20),\n",
    "    #'rsv60': rsv(60),\n",
    "    #'rsv120': rsv(120),\n",
    "    #'rsv240': rsv(240),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "```\n",
    "https://www.twblogs.net/a/5d3f3173bd9eee517422735f\n",
    "W-WED\n",
    "https://docs.python.org/zh-tw/3/library/calendar.html\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加入其他features\n",
    "* http://finlabcourse.imotor.com/viewthread.php?tid=660&extra=page%3D1\n",
    "\n",
    "```python\n",
    "from finlab import ml\n",
    "from finlab.data import Data\n",
    "\n",
    "data = Data()\n",
    "rsi = data.talib(\"RSI\")\n",
    "\n",
    "dataset = ml.fundamental_features()\n",
    "ml.add_feature(dataset, 'RSI', rsi)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 組合dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認各指標清單"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t1 = data.talib(\"NATR\",timeperiod=14)\n",
    "#t1.to_csv('myfile.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 製作dataset\n",
    "\n",
    "##### 設定買賣頻率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2005-02-15', '2005-03-15', '2005-04-15', '2005-05-15',\n",
       "               '2005-06-15', '2005-07-15', '2005-08-15', '2005-09-15',\n",
       "               '2005-10-15', '2005-11-15',\n",
       "               ...\n",
       "               '2022-03-15', '2022-04-15', '2022-05-15', '2022-06-15',\n",
       "               '2022-07-15', '2022-08-15', '2022-09-15', '2022-10-15',\n",
       "               '2022-11-15', '2022-12-15'],\n",
       "              dtype='datetime64[ns]', name='date', length=215, freq=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rev.index = rev.index.tz_localize(\"Asia/Taipei\")\n",
    "every_month = rev.index\n",
    "every_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 將dataframe 組裝起來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features['bias20'].reindex(every_month, method='ffill')\n",
    "\n",
    "for name, f in features.items():\n",
    "    features[name] = f.reindex(every_month, method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, f in features.items():\n",
    "    features[name] = f.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(dataset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 組裝自己要的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finlab import ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "股本 = data.get('股本合計').reindex(close.index, method='ffill')\n",
    "市值 = 股本 * close / 10 * 1000\n",
    "\n",
    "#t1 = 股本.reindex(close.index, method='ffill')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.add_feature(dataset, '市值', 市值)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mom1</th>\n",
       "      <th>mom2</th>\n",
       "      <th>mom3</th>\n",
       "      <th>mom4</th>\n",
       "      <th>mom5</th>\n",
       "      <th>mom6</th>\n",
       "      <th>mom7</th>\n",
       "      <th>mom8</th>\n",
       "      <th>mom9</th>\n",
       "      <th>bias5</th>\n",
       "      <th>...</th>\n",
       "      <th>rsv20</th>\n",
       "      <th>rsv60</th>\n",
       "      <th>rsv120</th>\n",
       "      <th>rsv240</th>\n",
       "      <th>yoy</th>\n",
       "      <th>delta_yoy</th>\n",
       "      <th>PB</th>\n",
       "      <th>PE</th>\n",
       "      <th>市值</th>\n",
       "      <th>vol_ma5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stock_id</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0015</th>\n",
       "      <th>2005-02-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-03-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-04-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-05-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-06-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">9962</th>\n",
       "      <th>2022-08-15</th>\n",
       "      <td>1.694666</td>\n",
       "      <td>0.917472</td>\n",
       "      <td>0.678559</td>\n",
       "      <td>1.435387</td>\n",
       "      <td>1.209077</td>\n",
       "      <td>0.859901</td>\n",
       "      <td>0.729493</td>\n",
       "      <td>1.238664</td>\n",
       "      <td>1.033162</td>\n",
       "      <td>1.042199</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.820755</td>\n",
       "      <td>0.386667</td>\n",
       "      <td>0.386667</td>\n",
       "      <td>0.240544</td>\n",
       "      <td>0.175498</td>\n",
       "      <td>1.35</td>\n",
       "      <td>7.48</td>\n",
       "      <td>1.470591e+09</td>\n",
       "      <td>818.5636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-15</th>\n",
       "      <td>0.641536</td>\n",
       "      <td>1.694666</td>\n",
       "      <td>0.917472</td>\n",
       "      <td>0.678559</td>\n",
       "      <td>1.435387</td>\n",
       "      <td>1.209077</td>\n",
       "      <td>0.859901</td>\n",
       "      <td>0.729493</td>\n",
       "      <td>1.238664</td>\n",
       "      <td>1.022514</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.391111</td>\n",
       "      <td>0.391111</td>\n",
       "      <td>0.286455</td>\n",
       "      <td>0.045911</td>\n",
       "      <td>1.35</td>\n",
       "      <td>7.50</td>\n",
       "      <td>1.475102e+09</td>\n",
       "      <td>825.8902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-15</th>\n",
       "      <td>1.280940</td>\n",
       "      <td>0.641536</td>\n",
       "      <td>1.694666</td>\n",
       "      <td>0.917472</td>\n",
       "      <td>0.678559</td>\n",
       "      <td>1.435387</td>\n",
       "      <td>1.209077</td>\n",
       "      <td>0.859901</td>\n",
       "      <td>0.729493</td>\n",
       "      <td>0.956989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.198582</td>\n",
       "      <td>0.124444</td>\n",
       "      <td>0.725788</td>\n",
       "      <td>0.439333</td>\n",
       "      <td>1.10</td>\n",
       "      <td>6.12</td>\n",
       "      <td>1.204441e+09</td>\n",
       "      <td>638.6044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-15</th>\n",
       "      <td>0.830797</td>\n",
       "      <td>1.280940</td>\n",
       "      <td>0.641536</td>\n",
       "      <td>1.694666</td>\n",
       "      <td>0.917472</td>\n",
       "      <td>0.678559</td>\n",
       "      <td>1.435387</td>\n",
       "      <td>1.209077</td>\n",
       "      <td>0.859901</td>\n",
       "      <td>1.024242</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933962</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.003516</td>\n",
       "      <td>-0.722271</td>\n",
       "      <td>1.33</td>\n",
       "      <td>7.01</td>\n",
       "      <td>1.524723e+09</td>\n",
       "      <td>462.1834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-15</th>\n",
       "      <td>1.163303</td>\n",
       "      <td>0.830797</td>\n",
       "      <td>1.280940</td>\n",
       "      <td>0.641536</td>\n",
       "      <td>1.694666</td>\n",
       "      <td>0.917472</td>\n",
       "      <td>0.678559</td>\n",
       "      <td>1.435387</td>\n",
       "      <td>1.209077</td>\n",
       "      <td>1.022727</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.949495</td>\n",
       "      <td>0.417778</td>\n",
       "      <td>0.129923</td>\n",
       "      <td>0.126406</td>\n",
       "      <td>1.31</td>\n",
       "      <td>6.91</td>\n",
       "      <td>1.502168e+09</td>\n",
       "      <td>450.5504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>439890 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         mom1      mom2      mom3      mom4      mom5  \\\n",
       "stock_id date                                                           \n",
       "0015     2005-02-15       NaN       NaN       NaN       NaN       NaN   \n",
       "         2005-03-15       NaN       NaN       NaN       NaN       NaN   \n",
       "         2005-04-15       NaN       NaN       NaN       NaN       NaN   \n",
       "         2005-05-15       NaN       NaN       NaN       NaN       NaN   \n",
       "         2005-06-15       NaN       NaN       NaN       NaN       NaN   \n",
       "...                       ...       ...       ...       ...       ...   \n",
       "9962     2022-08-15  1.694666  0.917472  0.678559  1.435387  1.209077   \n",
       "         2022-09-15  0.641536  1.694666  0.917472  0.678559  1.435387   \n",
       "         2022-10-15  1.280940  0.641536  1.694666  0.917472  0.678559   \n",
       "         2022-11-15  0.830797  1.280940  0.641536  1.694666  0.917472   \n",
       "         2022-12-15  1.163303  0.830797  1.280940  0.641536  1.694666   \n",
       "\n",
       "                         mom6      mom7      mom8      mom9     bias5  ...  \\\n",
       "stock_id date                                                          ...   \n",
       "0015     2005-02-15       NaN       NaN       NaN       NaN       NaN  ...   \n",
       "         2005-03-15       NaN       NaN       NaN       NaN       NaN  ...   \n",
       "         2005-04-15       NaN       NaN       NaN       NaN       NaN  ...   \n",
       "         2005-05-15       NaN       NaN       NaN       NaN       NaN  ...   \n",
       "         2005-06-15       NaN       NaN       NaN       NaN       NaN  ...   \n",
       "...                       ...       ...       ...       ...       ...  ...   \n",
       "9962     2022-08-15  0.859901  0.729493  1.238664  1.033162  1.042199  ...   \n",
       "         2022-09-15  1.209077  0.859901  0.729493  1.238664  1.022514  ...   \n",
       "         2022-10-15  1.435387  1.209077  0.859901  0.729493  0.956989  ...   \n",
       "         2022-11-15  0.678559  1.435387  1.209077  0.859901  1.024242  ...   \n",
       "         2022-12-15  0.917472  0.678559  1.435387  1.209077  1.022727  ...   \n",
       "\n",
       "                        rsv20     rsv60    rsv120    rsv240       yoy  \\\n",
       "stock_id date                                                           \n",
       "0015     2005-02-15       NaN       NaN       NaN       NaN       NaN   \n",
       "         2005-03-15       NaN       NaN       NaN       NaN       NaN   \n",
       "         2005-04-15       NaN       NaN       NaN       NaN       NaN   \n",
       "         2005-05-15       NaN       NaN       NaN       NaN       NaN   \n",
       "         2005-06-15       NaN       NaN       NaN       NaN       NaN   \n",
       "...                       ...       ...       ...       ...       ...   \n",
       "9962     2022-08-15  1.000000  0.820755  0.386667  0.386667  0.240544   \n",
       "         2022-09-15  1.000000  1.000000  0.391111  0.391111  0.286455   \n",
       "         2022-10-15  0.142857  0.130435  0.198582  0.124444  0.725788   \n",
       "         2022-11-15  1.000000  1.000000  0.933962  0.440000  0.003516   \n",
       "         2022-12-15  1.000000  0.937500  0.949495  0.417778  0.129923   \n",
       "\n",
       "                     delta_yoy    PB    PE            市值   vol_ma5  \n",
       "stock_id date                                                       \n",
       "0015     2005-02-15        NaN   NaN   NaN           NaN       NaN  \n",
       "         2005-03-15        NaN   NaN   NaN           NaN       NaN  \n",
       "         2005-04-15        NaN   NaN   NaN           NaN       NaN  \n",
       "         2005-05-15        NaN   NaN   NaN           NaN       NaN  \n",
       "         2005-06-15        NaN   NaN   NaN           NaN       NaN  \n",
       "...                        ...   ...   ...           ...       ...  \n",
       "9962     2022-08-15   0.175498  1.35  7.48  1.470591e+09  818.5636  \n",
       "         2022-09-15   0.045911  1.35  7.50  1.475102e+09  825.8902  \n",
       "         2022-10-15   0.439333  1.10  6.12  1.204441e+09  638.6044  \n",
       "         2022-11-15  -0.722271  1.33  7.01  1.524723e+09  462.1834  \n",
       "         2022-12-15   0.126406  1.31  6.91  1.502168e+09  450.5504  \n",
       "\n",
       "[439890 rows x 33 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml.add_feature(dataset, 'vol_ma5', vol_ma5)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################　　　自己加入的　　　##############################################\n",
    "dataset.index = dataset.index.set_names(['stock_id','date'], level=[0,1])\n",
    "\n",
    "\n",
    "#dataset.index.levels[1].name = 'date'\n",
    "#dataset.index.levels[0].name = 'stock_id'\n",
    "\n",
    "#因為你pandas更新到新版了\n",
    "## profit.index.levels[0].name = 'year'\n",
    "## profit.index.levels[1].name = 'month'\n",
    "#這兩行的語法被棄用，請改成\n",
    "#profit.index=profit.index.set_names('year', level=0)\n",
    "#profit.index=profit.index.set_names('month', level=1)\n",
    "#or profit.index=profit.index.set_names(['year','month'], level=[0,1])\n",
    "#直接一行\n",
    "#就可以了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#list(dataset_fundamental.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataset_fundamental.reindex(dataset.index).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data[組合](https://hahow.in/courses/5b9d3a6dca498a001e917383/discussions/5d18b63eac23d80020ae4ce7)\n",
    "```python\n",
    "new_df = pd.concat([dataset_fundamental['R402_營業毛利成長率'],dataset],axis=1).dropna(how='any')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df = pd.concat([dataset_fundamental,dataset],axis=1).dropna(how='all').fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df = pd.concat([dataset_fundamental,dataset],axis=1).dropna(how='any')\n",
    "#dataset1 = new_df.fillna(method='ffill')#[(new_df.index.get_level_values('stock_id')=='2330')]\n",
    "##dataset = dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#feature_names=list(dataset1.columns)\n",
    "#feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 新增 label(績效/排名)\n",
    " - 定義一下要比績效還是要比排名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finlab import ml\n",
    "\n",
    "ml.add_profit_prediction(dataset)\n",
    "ml.add_rank_prediction(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profit(return) rank\n",
    "predi_target = 'rank'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 刪除太大太小的歷史資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(439890, 35)\n",
      "(380532, 35)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "\n",
    "def drop_extreme_case(dataset, feature_names, thresh=0.01):\n",
    "    \n",
    "    extreme_cases = pd.Series(False, index=dataset.index)\n",
    "    for f in feature_names:\n",
    "        tf = dataset[f]\n",
    "        extreme_cases = extreme_cases | (tf < tf.quantile(thresh)) | (tf > tf.quantile(1-thresh))\n",
    "    dataset = dataset[~extreme_cases]\n",
    "    return dataset\n",
    "\n",
    "dataset_drop_extreme_case = drop_extreme_case(dataset , feature_names , thresh=0.01)\n",
    "\n",
    "print(dataset_drop_extreme_case.shape)\n",
    "\n",
    "##(436988, 34)\n",
    "##(377968, 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dropna = dataset_drop_extreme_case.dropna(how='any')\n",
    "dataset_dropna = dataset_dropna.reset_index().set_index(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_drop_extreme_case.index.get_level_values(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################\n",
    "##############################################　　　自己加入的　　　##############################################\n",
    "##################################################################################################################\n",
    "\n",
    "dataset_dropna.index = pd.to_datetime(dataset_dropna.index)\n",
    "dataset_dropna = dataset_dropna.sort_index()\n",
    "\n",
    "#修復＜class ‘numpy.ndarray‘＞　https://blog.csdn.net/lxbin/article/details/114005757"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset_dropna.loc[:'2015']\n",
    "dataset_test = dataset_dropna.loc['2016':]\n",
    "\n",
    "#date_arr = dataset.index.get_level_values('date') < '2020'\n",
    "#dataset_train = dataset[date_arr]\n",
    "#dataset_test = dataset[~date_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset_train[feature_names] , dataset_train['return'] > 1\n",
    "test = dataset_test[feature_names] , dataset_test['return'] > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 機器學習\n",
    " - 目前只有三個，技術指標也要再增加一下feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_names = feature_names1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2013-05-15', '2013-05-15', '2013-05-15', '2013-05-15',\n",
       "               '2013-05-15', '2013-05-15', '2013-05-15', '2013-05-15',\n",
       "               '2013-05-15', '2013-05-15',\n",
       "               ...\n",
       "               '2015-12-15', '2015-12-15', '2015-12-15', '2015-12-15',\n",
       "               '2015-12-15', '2015-12-15', '2015-12-15', '2015-12-15',\n",
       "               '2015-12-15', '2015-12-15'],\n",
       "              dtype='datetime64[ns]', name='date', length=26451, freq=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 100)               3200      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 13,401\n",
      "Trainable params: 13,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "start fitting\n",
      "Epoch 1/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.267 - ETA: 0s - loss: 0.253 - 0s 12ms/step - loss: 0.2450 - val_loss: 0.0947\n",
      "Epoch 2/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.214 - ETA: 0s - loss: 0.183 - 0s 5ms/step - loss: 0.1624 - val_loss: 0.0764\n",
      "Epoch 3/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.104 - ETA: 0s - loss: 0.085 - 0s 5ms/step - loss: 0.0818 - val_loss: 0.0740\n",
      "Epoch 4/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0739 - val_loss: 0.0741\n",
      "Epoch 5/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.074 - 0s 5ms/step - loss: 0.0738 - val_loss: 0.0741\n",
      "Epoch 6/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 6ms/step - loss: 0.0737 - val_loss: 0.0743\n",
      "Epoch 7/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0737 - val_loss: 0.0741\n",
      "Epoch 8/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0737 - val_loss: 0.0740\n",
      "Epoch 9/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0736 - val_loss: 0.0740\n",
      "Epoch 10/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0736 - val_loss: 0.0741\n",
      "Epoch 11/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - 0s 6ms/step - loss: 0.0736 - val_loss: 0.0741\n",
      "Epoch 12/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0736 - val_loss: 0.0740\n",
      "Epoch 13/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 6ms/step - loss: 0.0735 - val_loss: 0.0741\n",
      "Epoch 14/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0736 - val_loss: 0.0740\n",
      "Epoch 15/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0736 - val_loss: 0.0741\n",
      "Epoch 16/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0735 - val_loss: 0.0740\n",
      "Epoch 17/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0735 - val_loss: 0.0741\n",
      "Epoch 18/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0734 - val_loss: 0.0741\n",
      "Epoch 19/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0736 - val_loss: 0.0741\n",
      "Epoch 20/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - 0s 6ms/step - loss: 0.0735 - val_loss: 0.0741\n",
      "Epoch 21/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0735 - val_loss: 0.0739\n",
      "Epoch 22/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0735 - val_loss: 0.0740\n",
      "Epoch 23/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0735 - val_loss: 0.0740\n",
      "Epoch 24/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0735 - val_loss: 0.0740\n",
      "Epoch 25/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 6ms/step - loss: 0.0735 - val_loss: 0.0740\n",
      "Epoch 26/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0735 - val_loss: 0.0740\n",
      "Epoch 27/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0735 - val_loss: 0.0741\n",
      "Epoch 28/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0735 - val_loss: 0.0741\n",
      "Epoch 29/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0735 - val_loss: 0.0741\n",
      "Epoch 30/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0734 - val_loss: 0.0739\n",
      "Epoch 31/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - 0s 6ms/step - loss: 0.0734 - val_loss: 0.0741\n",
      "Epoch 32/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0734 - val_loss: 0.0741\n",
      "Epoch 33/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0735 - val_loss: 0.0739\n",
      "Epoch 34/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0734 - val_loss: 0.0739\n",
      "Epoch 35/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 7ms/step - loss: 0.0734 - val_loss: 0.0741\n",
      "Epoch 36/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 8ms/step - loss: 0.0735 - val_loss: 0.0740\n",
      "Epoch 37/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 7ms/step - loss: 0.0734 - val_loss: 0.0740\n",
      "Epoch 38/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 7ms/step - loss: 0.0735 - val_loss: 0.0740\n",
      "Epoch 39/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 6ms/step - loss: 0.0734 - val_loss: 0.0740\n",
      "Epoch 40/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - 0s 9ms/step - loss: 0.0733 - val_loss: 0.0741\n",
      "Epoch 41/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 7ms/step - loss: 0.0734 - val_loss: 0.0740\n",
      "Epoch 42/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 7ms/step - loss: 0.0735 - val_loss: 0.0740\n",
      "Epoch 43/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 7ms/step - loss: 0.0734 - val_loss: 0.0740\n",
      "Epoch 44/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - 0s 5ms/step - loss: 0.0733 - val_loss: 0.0740\n",
      "Epoch 45/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 6ms/step - loss: 0.0734 - val_loss: 0.0740\n",
      "Epoch 46/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 7ms/step - loss: 0.0734 - val_loss: 0.0741\n",
      "Epoch 47/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 7ms/step - loss: 0.0734 - val_loss: 0.0741\n",
      "Epoch 48/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 6ms/step - loss: 0.0733 - val_loss: 0.0740\n",
      "Epoch 49/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 7ms/step - loss: 0.0734 - val_loss: 0.0740\n",
      "Epoch 50/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 7ms/step - loss: 0.0735 - val_loss: 0.0740\n",
      "Epoch 51/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 8ms/step - loss: 0.0733 - val_loss: 0.0741\n",
      "Epoch 52/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 7ms/step - loss: 0.0734 - val_loss: 0.0740\n",
      "Epoch 53/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 8ms/step - loss: 0.0733 - val_loss: 0.0742\n",
      "Epoch 54/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 6ms/step - loss: 0.0732 - val_loss: 0.0741\n",
      "Epoch 55/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0733 - val_loss: 0.0740\n",
      "Epoch 56/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 8ms/step - loss: 0.0733 - val_loss: 0.0741\n",
      "Epoch 57/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 8ms/step - loss: 0.0733 - val_loss: 0.0741\n",
      "Epoch 58/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 6ms/step - loss: 0.0733 - val_loss: 0.0741\n",
      "Epoch 59/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0733 - val_loss: 0.0740\n",
      "Epoch 60/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0733 - val_loss: 0.0740\n",
      "Epoch 61/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0733 - val_loss: 0.0741\n",
      "Epoch 62/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0732 - val_loss: 0.0742\n",
      "Epoch 63/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 7ms/step - loss: 0.0732 - val_loss: 0.0741\n",
      "Epoch 64/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0734 - val_loss: 0.0740\n",
      "Epoch 65/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0733 - val_loss: 0.0742\n",
      "Epoch 66/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0734 - val_loss: 0.0740\n",
      "Epoch 67/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0733 - val_loss: 0.0741\n",
      "Epoch 68/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0733 - val_loss: 0.0741\n",
      "Epoch 69/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0732 - val_loss: 0.0741\n",
      "Epoch 70/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 6ms/step - loss: 0.0731 - val_loss: 0.0741\n",
      "Epoch 71/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0732 - val_loss: 0.0744\n",
      "Epoch 72/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0731 - val_loss: 0.0742\n",
      "Epoch 73/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0731 - val_loss: 0.0742\n",
      "Epoch 74/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0731 - val_loss: 0.0742\n",
      "Epoch 75/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0732 - val_loss: 0.0741\n",
      "Epoch 76/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - 0s 6ms/step - loss: 0.0732 - val_loss: 0.0741\n",
      "Epoch 77/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 6ms/step - loss: 0.0732 - val_loss: 0.0742\n",
      "Epoch 78/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0732 - val_loss: 0.0741\n",
      "Epoch 79/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0732 - val_loss: 0.0742\n",
      "Epoch 80/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0731 - val_loss: 0.0741\n",
      "Epoch 81/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0732 - val_loss: 0.0741\n",
      "Epoch 82/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0731 - val_loss: 0.0742\n",
      "Epoch 83/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0730 - val_loss: 0.0744\n",
      "Epoch 84/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - 0s 6ms/step - loss: 0.0732 - val_loss: 0.0743\n",
      "Epoch 85/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0732 - val_loss: 0.0742\n",
      "Epoch 86/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0731 - val_loss: 0.0741\n",
      "Epoch 87/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0732 - val_loss: 0.0742\n",
      "Epoch 88/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0731 - val_loss: 0.0742\n",
      "Epoch 89/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0731 - val_loss: 0.0741\n",
      "Epoch 90/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0731 - val_loss: 0.0742\n",
      "Epoch 91/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 6ms/step - loss: 0.0731 - val_loss: 0.0742\n",
      "Epoch 92/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - 0s 6ms/step - loss: 0.0730 - val_loss: 0.0742\n",
      "Epoch 93/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0730 - val_loss: 0.0742\n",
      "Epoch 94/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0731 - val_loss: 0.0742\n",
      "Epoch 95/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0730 - val_loss: 0.0741\n",
      "Epoch 96/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0731 - val_loss: 0.0742\n",
      "Epoch 97/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0732 - val_loss: 0.0743\n",
      "Epoch 98/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 6ms/step - loss: 0.0731 - val_loss: 0.0742\n",
      "Epoch 99/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - 0s 6ms/step - loss: 0.0730 - val_loss: 0.0741\n",
      "Epoch 100/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 7ms/step - loss: 0.0731 - val_loss: 0.0742\n",
      "Epoch 101/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 7ms/step - loss: 0.0731 - val_loss: 0.0742\n",
      "Epoch 102/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 7ms/step - loss: 0.0730 - val_loss: 0.0742\n",
      "Epoch 103/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 8ms/step - loss: 0.0730 - val_loss: 0.0742\n",
      "Epoch 104/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 8ms/step - loss: 0.0732 - val_loss: 0.0740\n",
      "Epoch 105/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 7ms/step - loss: 0.0730 - val_loss: 0.0743\n",
      "Epoch 106/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0730 - val_loss: 0.0743\n",
      "Epoch 107/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 7ms/step - loss: 0.0730 - val_loss: 0.0743\n",
      "Epoch 108/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 9ms/step - loss: 0.0730 - val_loss: 0.0742\n",
      "Epoch 109/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 9ms/step - loss: 0.0729 - val_loss: 0.0742\n",
      "Epoch 110/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 8ms/step - loss: 0.0731 - val_loss: 0.0743\n",
      "Epoch 111/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0729 - val_loss: 0.0740\n",
      "Epoch 112/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - 0s 6ms/step - loss: 0.0730 - val_loss: 0.0742\n",
      "Epoch 113/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0729 - val_loss: 0.0742\n",
      "Epoch 114/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 8ms/step - loss: 0.0730 - val_loss: 0.0742\n",
      "Epoch 115/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 8ms/step - loss: 0.0729 - val_loss: 0.0741\n",
      "Epoch 116/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0729 - val_loss: 0.0742\n",
      "Epoch 117/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0729 - val_loss: 0.0741\n",
      "Epoch 118/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 7ms/step - loss: 0.0730 - val_loss: 0.0743\n",
      "Epoch 119/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0728 - val_loss: 0.0743\n",
      "Epoch 120/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 8ms/step - loss: 0.0729 - val_loss: 0.0742\n",
      "Epoch 121/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0729 - val_loss: 0.0742\n",
      "Epoch 122/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - 0s 6ms/step - loss: 0.0727 - val_loss: 0.0746\n",
      "Epoch 123/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0729 - val_loss: 0.0742\n",
      "Epoch 124/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 8ms/step - loss: 0.0729 - val_loss: 0.0743\n",
      "Epoch 125/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 8ms/step - loss: 0.0729 - val_loss: 0.0742\n",
      "Epoch 126/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 7ms/step - loss: 0.0729 - val_loss: 0.0743\n",
      "Epoch 127/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0729 - val_loss: 0.0744\n",
      "Epoch 128/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0729 - val_loss: 0.0742\n",
      "Epoch 129/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 6ms/step - loss: 0.0729 - val_loss: 0.0745\n",
      "Epoch 130/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0729 - val_loss: 0.0745\n",
      "Epoch 131/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0729 - val_loss: 0.0742\n",
      "Epoch 132/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0726 - val_loss: 0.0743\n",
      "Epoch 133/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0728 - val_loss: 0.0744\n",
      "Epoch 134/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 6ms/step - loss: 0.0728 - val_loss: 0.0741\n",
      "Epoch 135/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - 0s 6ms/step - loss: 0.0727 - val_loss: 0.0745\n",
      "Epoch 136/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 7ms/step - loss: 0.0731 - val_loss: 0.0743\n",
      "Epoch 137/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0729 - val_loss: 0.0744\n",
      "Epoch 138/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0728 - val_loss: 0.0741\n",
      "Epoch 139/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 6ms/step - loss: 0.0728 - val_loss: 0.0740\n",
      "Epoch 140/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0727 - val_loss: 0.0741\n",
      "Epoch 141/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0727 - val_loss: 0.0743\n",
      "Epoch 142/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 8ms/step - loss: 0.0727 - val_loss: 0.0741\n",
      "Epoch 143/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 10ms/step - loss: 0.0728 - val_loss: 0.0742\n",
      "Epoch 144/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - 0s 6ms/step - loss: 0.0729 - val_loss: 0.0742\n",
      "Epoch 145/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - 0s 6ms/step - loss: 0.0727 - val_loss: 0.0742\n",
      "Epoch 146/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0728 - val_loss: 0.0742\n",
      "Epoch 147/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 6ms/step - loss: 0.0727 - val_loss: 0.0745\n",
      "Epoch 148/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0727 - val_loss: 0.0746\n",
      "Epoch 149/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0727 - val_loss: 0.0742\n",
      "Epoch 150/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0729 - val_loss: 0.0747\n",
      "Epoch 151/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 9ms/step - loss: 0.0726 - val_loss: 0.0744\n",
      "Epoch 152/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0728 - val_loss: 0.0743\n",
      "Epoch 153/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0726 - val_loss: 0.0743\n",
      "Epoch 154/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0726 - val_loss: 0.0746\n",
      "Epoch 155/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0727 - val_loss: 0.0743\n",
      "Epoch 156/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 6ms/step - loss: 0.0727 - val_loss: 0.0743\n",
      "Epoch 157/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0727 - val_loss: 0.0742\n",
      "Epoch 158/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0725 - val_loss: 0.0743\n",
      "Epoch 159/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0726 - val_loss: 0.0742\n",
      "Epoch 160/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0725 - val_loss: 0.0745\n",
      "Epoch 161/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0725 - val_loss: 0.0743\n",
      "Epoch 162/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0726 - val_loss: 0.0746\n",
      "Epoch 163/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0727 - val_loss: 0.0742\n",
      "Epoch 164/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0726 - val_loss: 0.0742\n",
      "Epoch 165/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0726 - val_loss: 0.0743\n",
      "Epoch 166/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 10ms/step - loss: 0.0725 - val_loss: 0.0743\n",
      "Epoch 167/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0726 - val_loss: 0.0744\n",
      "Epoch 168/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0726 - val_loss: 0.0744\n",
      "Epoch 169/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0725 - val_loss: 0.0744\n",
      "Epoch 170/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0725 - val_loss: 0.0742\n",
      "Epoch 171/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0725 - val_loss: 0.0743\n",
      "Epoch 172/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0724 - val_loss: 0.0750\n",
      "Epoch 173/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0723 - val_loss: 0.0743\n",
      "Epoch 174/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0725 - val_loss: 0.0745\n",
      "Epoch 175/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0724 - val_loss: 0.0745\n",
      "Epoch 176/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0724 - val_loss: 0.0745\n",
      "Epoch 177/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0724 - val_loss: 0.0746\n",
      "Epoch 178/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0724 - val_loss: 0.0747\n",
      "Epoch 179/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0724 - val_loss: 0.0744\n",
      "Epoch 180/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0723 - val_loss: 0.0744\n",
      "Epoch 181/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0725 - val_loss: 0.0742\n",
      "Epoch 182/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 8ms/step - loss: 0.0725 - val_loss: 0.0744\n",
      "Epoch 183/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0725 - val_loss: 0.0743\n",
      "Epoch 184/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0723 - val_loss: 0.0743\n",
      "Epoch 185/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0727 - val_loss: 0.0745\n",
      "Epoch 186/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0724 - val_loss: 0.0742\n",
      "Epoch 187/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0724 - val_loss: 0.0745\n",
      "Epoch 188/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0722 - val_loss: 0.0745\n",
      "Epoch 189/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0724 - val_loss: 0.0743\n",
      "Epoch 190/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0723 - val_loss: 0.0746\n",
      "Epoch 191/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0724 - val_loss: 0.0746\n",
      "Epoch 192/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0723 - val_loss: 0.0746\n",
      "Epoch 193/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0724 - val_loss: 0.0745\n",
      "Epoch 194/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0722 - val_loss: 0.0744\n",
      "Epoch 195/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0722 - val_loss: 0.0743\n",
      "Epoch 196/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0723 - val_loss: 0.0744\n",
      "Epoch 197/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0724 - val_loss: 0.0747\n",
      "Epoch 198/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0722 - val_loss: 0.0746\n",
      "Epoch 199/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0721 - val_loss: 0.0745\n",
      "Epoch 200/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0722 - val_loss: 0.0751\n",
      "Epoch 201/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0723 - val_loss: 0.0745\n",
      "Epoch 202/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 6ms/step - loss: 0.0724 - val_loss: 0.0745\n",
      "Epoch 203/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0722 - val_loss: 0.0747\n",
      "Epoch 204/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0721 - val_loss: 0.0745\n",
      "Epoch 205/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0722 - val_loss: 0.0747\n",
      "Epoch 206/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0721 - val_loss: 0.0745\n",
      "Epoch 207/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0722 - val_loss: 0.0749\n",
      "Epoch 208/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0721 - val_loss: 0.0746\n",
      "Epoch 209/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0721 - val_loss: 0.0747\n",
      "Epoch 210/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0721 - val_loss: 0.0748\n",
      "Epoch 211/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0721 - val_loss: 0.0748\n",
      "Epoch 212/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0722 - val_loss: 0.0745\n",
      "Epoch 213/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0722 - val_loss: 0.0746\n",
      "Epoch 214/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0722 - val_loss: 0.0743\n",
      "Epoch 215/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0722 - val_loss: 0.0746\n",
      "Epoch 216/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0721 - val_loss: 0.0744\n",
      "Epoch 217/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0721 - val_loss: 0.0752\n",
      "Epoch 218/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0722 - val_loss: 0.0745\n",
      "Epoch 219/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0722 - val_loss: 0.0746\n",
      "Epoch 220/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0721 - val_loss: 0.0743\n",
      "Epoch 221/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0720 - val_loss: 0.0746\n",
      "Epoch 222/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0722 - val_loss: 0.0746\n",
      "Epoch 223/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 9ms/step - loss: 0.0721 - val_loss: 0.0747\n",
      "Epoch 224/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0725 - val_loss: 0.0745\n",
      "Epoch 225/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0721 - val_loss: 0.0746\n",
      "Epoch 226/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0720 - val_loss: 0.0746\n",
      "Epoch 227/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 8ms/step - loss: 0.0721 - val_loss: 0.0745\n",
      "Epoch 228/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0720 - val_loss: 0.0745\n",
      "Epoch 229/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0721 - val_loss: 0.0749\n",
      "Epoch 230/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0720 - val_loss: 0.0745\n",
      "Epoch 231/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0719 - val_loss: 0.0746\n",
      "Epoch 232/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0719 - val_loss: 0.0745\n",
      "Epoch 233/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0720 - val_loss: 0.0744\n",
      "Epoch 234/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0719 - val_loss: 0.0745\n",
      "Epoch 235/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0720 - val_loss: 0.0748\n",
      "Epoch 236/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0720 - val_loss: 0.0743\n",
      "Epoch 237/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0720 - val_loss: 0.0745\n",
      "Epoch 238/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 8ms/step - loss: 0.0720 - val_loss: 0.0749\n",
      "Epoch 239/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 8ms/step - loss: 0.0719 - val_loss: 0.0745\n",
      "Epoch 240/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0718 - val_loss: 0.0747\n",
      "Epoch 241/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0721 - val_loss: 0.0748\n",
      "Epoch 242/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0720 - val_loss: 0.0744\n",
      "Epoch 243/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0718 - val_loss: 0.0747\n",
      "Epoch 244/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0719 - val_loss: 0.0748\n",
      "Epoch 245/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 7ms/step - loss: 0.0718 - val_loss: 0.0752\n",
      "Epoch 246/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0718 - val_loss: 0.0746\n",
      "Epoch 247/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0719 - val_loss: 0.0746\n",
      "Epoch 248/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0720 - val_loss: 0.0751\n",
      "Epoch 249/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0718 - val_loss: 0.0747\n",
      "Epoch 250/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0719 - val_loss: 0.0748\n",
      "Epoch 251/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0718 - val_loss: 0.0747\n",
      "Epoch 252/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0719 - val_loss: 0.0747\n",
      "Epoch 253/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0719 - val_loss: 0.0748\n",
      "Epoch 254/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0719 - val_loss: 0.0749\n",
      "Epoch 255/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 10ms/step - loss: 0.0717 - val_loss: 0.0752\n",
      "Epoch 256/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0719 - val_loss: 0.0749\n",
      "Epoch 257/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0718 - val_loss: 0.0749\n",
      "Epoch 258/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0718 - val_loss: 0.0746\n",
      "Epoch 259/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0717 - val_loss: 0.0749\n",
      "Epoch 260/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0718 - val_loss: 0.0747\n",
      "Epoch 261/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0717 - val_loss: 0.0750\n",
      "Epoch 262/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0717 - val_loss: 0.0750\n",
      "Epoch 263/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0719 - val_loss: 0.0749\n",
      "Epoch 264/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0717 - val_loss: 0.0750\n",
      "Epoch 265/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0718 - val_loss: 0.0750\n",
      "Epoch 266/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0718 - val_loss: 0.0749\n",
      "Epoch 267/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0717 - val_loss: 0.0749\n",
      "Epoch 268/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0717 - val_loss: 0.0749\n",
      "Epoch 269/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0717 - val_loss: 0.0751\n",
      "Epoch 270/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0718 - val_loss: 0.0743\n",
      "Epoch 271/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 11ms/step - loss: 0.0716 - val_loss: 0.0750\n",
      "Epoch 272/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0716 - val_loss: 0.0749\n",
      "Epoch 273/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0717 - val_loss: 0.0744\n",
      "Epoch 274/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0716 - val_loss: 0.0747\n",
      "Epoch 275/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0717 - val_loss: 0.0745\n",
      "Epoch 276/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0715 - val_loss: 0.0747\n",
      "Epoch 277/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0716 - val_loss: 0.0747\n",
      "Epoch 278/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0716 - val_loss: 0.0745\n",
      "Epoch 279/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0716 - val_loss: 0.0750\n",
      "Epoch 280/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0715 - val_loss: 0.0747\n",
      "Epoch 281/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0716 - val_loss: 0.0751\n",
      "Epoch 282/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0716 - val_loss: 0.0745\n",
      "Epoch 283/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0716 - val_loss: 0.0748\n",
      "Epoch 284/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 8ms/step - loss: 0.0714 - val_loss: 0.0746\n",
      "Epoch 285/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0716 - val_loss: 0.0747\n",
      "Epoch 286/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0715 - val_loss: 0.0746\n",
      "Epoch 287/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0715 - val_loss: 0.0744\n",
      "Epoch 288/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0715 - val_loss: 0.0748\n",
      "Epoch 289/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0716 - val_loss: 0.0748\n",
      "Epoch 290/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 8ms/step - loss: 0.0716 - val_loss: 0.0746\n",
      "Epoch 291/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0717 - val_loss: 0.0752\n",
      "Epoch 292/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0714 - val_loss: 0.0743\n",
      "Epoch 293/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0714 - val_loss: 0.0749\n",
      "Epoch 294/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0715 - val_loss: 0.0750\n",
      "Epoch 295/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0715 - val_loss: 0.0753\n",
      "Epoch 296/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 10ms/step - loss: 0.0715 - val_loss: 0.0748\n",
      "Epoch 297/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0715 - val_loss: 0.0750\n",
      "Epoch 298/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0716 - val_loss: 0.0750\n",
      "Epoch 299/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0716 - val_loss: 0.0745\n",
      "Epoch 300/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0714 - val_loss: 0.0751\n",
      "Epoch 301/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0712 - val_loss: 0.0746\n",
      "Epoch 302/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0715 - val_loss: 0.0751\n",
      "Epoch 303/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - 0s 6ms/step - loss: 0.0714 - val_loss: 0.0748\n",
      "Epoch 304/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0714 - val_loss: 0.0747\n",
      "Epoch 305/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0714 - val_loss: 0.0747\n",
      "Epoch 306/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0712 - val_loss: 0.0746\n",
      "Epoch 307/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0713 - val_loss: 0.0750\n",
      "Epoch 308/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0714 - val_loss: 0.0747\n",
      "Epoch 309/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0712 - val_loss: 0.0753\n",
      "Epoch 310/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0714 - val_loss: 0.0751\n",
      "Epoch 311/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0713 - val_loss: 0.0749\n",
      "Epoch 312/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 10ms/step - loss: 0.0713 - val_loss: 0.0750\n",
      "Epoch 313/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 8ms/step - loss: 0.0714 - val_loss: 0.0755\n",
      "Epoch 314/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0714 - val_loss: 0.0747\n",
      "Epoch 315/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0713 - val_loss: 0.0756\n",
      "Epoch 316/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0715 - val_loss: 0.0755\n",
      "Epoch 317/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0713 - val_loss: 0.0749\n",
      "Epoch 318/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0714 - val_loss: 0.0748\n",
      "Epoch 319/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0715 - val_loss: 0.0747\n",
      "Epoch 320/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0710 - val_loss: 0.0751\n",
      "Epoch 321/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 8ms/step - loss: 0.0713 - val_loss: 0.0753\n",
      "Epoch 322/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 8ms/step - loss: 0.0714 - val_loss: 0.0744\n",
      "Epoch 323/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - 0s 8ms/step - loss: 0.0713 - val_loss: 0.0753\n",
      "Epoch 324/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0712 - val_loss: 0.0752\n",
      "Epoch 325/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0713 - val_loss: 0.0751\n",
      "Epoch 326/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0713 - val_loss: 0.0751\n",
      "Epoch 327/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0712 - val_loss: 0.0751\n",
      "Epoch 328/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0710 - val_loss: 0.0748\n",
      "Epoch 329/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0712 - val_loss: 0.0746\n",
      "Epoch 330/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0714 - val_loss: 0.0750\n",
      "Epoch 331/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0713 - val_loss: 0.0752\n",
      "Epoch 332/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0710 - val_loss: 0.0749\n",
      "Epoch 333/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0710 - val_loss: 0.0760\n",
      "Epoch 334/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0712 - val_loss: 0.0751\n",
      "Epoch 335/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0714 - val_loss: 0.0751\n",
      "Epoch 336/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 9ms/step - loss: 0.0711 - val_loss: 0.0753\n",
      "Epoch 337/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0713 - val_loss: 0.0753\n",
      "Epoch 338/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0710 - val_loss: 0.0750\n",
      "Epoch 339/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0711 - val_loss: 0.0757\n",
      "Epoch 340/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0710 - val_loss: 0.0750\n",
      "Epoch 341/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0711 - val_loss: 0.0750\n",
      "Epoch 342/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0710 - val_loss: 0.0752\n",
      "Epoch 343/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0711 - val_loss: 0.0750\n",
      "Epoch 344/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0711 - val_loss: 0.0752\n",
      "Epoch 345/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0711 - val_loss: 0.0752\n",
      "Epoch 346/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0711 - val_loss: 0.0748\n",
      "Epoch 347/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0712 - val_loss: 0.0758\n",
      "Epoch 348/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0711 - val_loss: 0.0753\n",
      "Epoch 349/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0710 - val_loss: 0.0751\n",
      "Epoch 350/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0709 - val_loss: 0.0753\n",
      "Epoch 351/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0713 - val_loss: 0.0757\n",
      "Epoch 352/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0710 - val_loss: 0.0748\n",
      "Epoch 353/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 7ms/step - loss: 0.0710 - val_loss: 0.0752\n",
      "Epoch 354/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0712 - val_loss: 0.0749\n",
      "Epoch 355/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0711 - val_loss: 0.0755\n",
      "Epoch 356/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0710 - val_loss: 0.0743\n",
      "Epoch 357/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0713 - val_loss: 0.0749\n",
      "Epoch 358/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0710 - val_loss: 0.0750\n",
      "Epoch 359/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0711 - val_loss: 0.0756\n",
      "Epoch 360/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0711 - val_loss: 0.0750\n",
      "Epoch 361/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0711 - val_loss: 0.0751\n",
      "Epoch 362/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0709 - val_loss: 0.0750\n",
      "Epoch 363/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0709 - val_loss: 0.0751\n",
      "Epoch 364/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0708 - val_loss: 0.0750\n",
      "Epoch 365/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0709 - val_loss: 0.0749\n",
      "Epoch 366/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0710 - val_loss: 0.0753\n",
      "Epoch 367/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - 0s 7ms/step - loss: 0.0707 - val_loss: 0.0751\n",
      "Epoch 368/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 8ms/step - loss: 0.0709 - val_loss: 0.0749\n",
      "Epoch 369/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0709 - val_loss: 0.0758\n",
      "Epoch 370/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 7ms/step - loss: 0.0708 - val_loss: 0.0754\n",
      "Epoch 371/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0709 - val_loss: 0.0762\n",
      "Epoch 372/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 7ms/step - loss: 0.0708 - val_loss: 0.0751\n",
      "Epoch 373/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0709 - val_loss: 0.0752\n",
      "Epoch 374/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0709 - val_loss: 0.0755\n",
      "Epoch 375/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0707 - val_loss: 0.0758\n",
      "Epoch 376/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0707 - val_loss: 0.0755\n",
      "Epoch 377/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0710 - val_loss: 0.0750\n",
      "Epoch 378/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0710 - val_loss: 0.0752\n",
      "Epoch 379/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0710 - val_loss: 0.0747\n",
      "Epoch 380/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0707 - val_loss: 0.0757\n",
      "Epoch 381/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0706 - val_loss: 0.0750\n",
      "Epoch 382/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0710 - val_loss: 0.0754\n",
      "Epoch 383/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 10ms/step - loss: 0.0709 - val_loss: 0.0756\n",
      "Epoch 384/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0708 - val_loss: 0.0747\n",
      "Epoch 385/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0708 - val_loss: 0.0753\n",
      "Epoch 386/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0707 - val_loss: 0.0753\n",
      "Epoch 387/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0710 - val_loss: 0.0752\n",
      "Epoch 388/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0709 - val_loss: 0.0756\n",
      "Epoch 389/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0710 - val_loss: 0.0752\n",
      "Epoch 390/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0710 - val_loss: 0.0754\n",
      "Epoch 391/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0709 - val_loss: 0.0753\n",
      "Epoch 392/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0708 - val_loss: 0.0754\n",
      "Epoch 393/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0708 - val_loss: 0.0754\n",
      "Epoch 394/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 9ms/step - loss: 0.0706 - val_loss: 0.0751\n",
      "Epoch 395/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0709 - val_loss: 0.0761\n",
      "Epoch 396/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 7ms/step - loss: 0.0709 - val_loss: 0.0754\n",
      "Epoch 397/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - 0s 7ms/step - loss: 0.0707 - val_loss: 0.0752\n",
      "Epoch 398/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 7ms/step - loss: 0.0708 - val_loss: 0.0751\n",
      "Epoch 399/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0707 - val_loss: 0.0752\n",
      "Epoch 400/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0707 - val_loss: 0.0752\n",
      "Epoch 401/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0709 - val_loss: 0.0754\n",
      "Epoch 402/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0709 - val_loss: 0.0754\n",
      "Epoch 403/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0705 - val_loss: 0.0759\n",
      "Epoch 404/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 7ms/step - loss: 0.0708 - val_loss: 0.0755\n",
      "Epoch 405/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0708 - val_loss: 0.0753\n",
      "Epoch 406/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0706 - val_loss: 0.0753\n",
      "Epoch 407/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0706 - val_loss: 0.0753\n",
      "Epoch 408/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - 0s 7ms/step - loss: 0.0707 - val_loss: 0.0750\n",
      "Epoch 409/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0708 - val_loss: 0.0756\n",
      "Epoch 410/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0706 - val_loss: 0.0751\n",
      "Epoch 411/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0709 - val_loss: 0.0752\n",
      "Epoch 412/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 9ms/step - loss: 0.0708 - val_loss: 0.0749\n",
      "Epoch 413/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0708 - val_loss: 0.0757\n",
      "Epoch 414/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0707 - val_loss: 0.0754\n",
      "Epoch 415/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0708 - val_loss: 0.0751\n",
      "Epoch 416/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0705 - val_loss: 0.0753\n",
      "Epoch 417/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0706 - val_loss: 0.0748\n",
      "Epoch 418/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0705 - val_loss: 0.0757\n",
      "Epoch 419/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0708 - val_loss: 0.0747\n",
      "Epoch 420/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0707 - val_loss: 0.0756\n",
      "Epoch 421/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0706 - val_loss: 0.0755\n",
      "Epoch 422/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0706 - val_loss: 0.0753\n",
      "Epoch 423/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0706 - val_loss: 0.0757\n",
      "Epoch 424/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0706 - val_loss: 0.0759\n",
      "Epoch 425/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 7ms/step - loss: 0.0706 - val_loss: 0.0750\n",
      "Epoch 426/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 7ms/step - loss: 0.0708 - val_loss: 0.0765\n",
      "Epoch 427/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 7ms/step - loss: 0.0705 - val_loss: 0.0759\n",
      "Epoch 428/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - 0s 7ms/step - loss: 0.0705 - val_loss: 0.0754\n",
      "Epoch 429/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0705 - val_loss: 0.0748\n",
      "Epoch 430/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0706 - val_loss: 0.0753\n",
      "Epoch 431/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 7ms/step - loss: 0.0705 - val_loss: 0.0760\n",
      "Epoch 432/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 8ms/step - loss: 0.0704 - val_loss: 0.0747\n",
      "Epoch 433/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - 0s 7ms/step - loss: 0.0705 - val_loss: 0.0752\n",
      "Epoch 434/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 7ms/step - loss: 0.0703 - val_loss: 0.0756\n",
      "Epoch 435/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0703 - val_loss: 0.0755\n",
      "Epoch 436/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0704 - val_loss: 0.0759\n",
      "Epoch 437/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0704 - val_loss: 0.0757\n",
      "Epoch 438/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0704 - val_loss: 0.0758\n",
      "Epoch 439/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 7ms/step - loss: 0.0704 - val_loss: 0.0754\n",
      "Epoch 440/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0705 - val_loss: 0.0761\n",
      "Epoch 441/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0707 - val_loss: 0.0750\n",
      "Epoch 442/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0704 - val_loss: 0.0754\n",
      "Epoch 443/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0703 - val_loss: 0.0756\n",
      "Epoch 444/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0703 - val_loss: 0.0754\n",
      "Epoch 445/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0702 - val_loss: 0.0767\n",
      "Epoch 446/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0706 - val_loss: 0.0756\n",
      "Epoch 447/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0707 - val_loss: 0.0750\n",
      "Epoch 448/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0705 - val_loss: 0.0753\n",
      "Epoch 449/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0705 - val_loss: 0.0753\n",
      "Epoch 450/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0703 - val_loss: 0.0761\n",
      "Epoch 451/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0706 - val_loss: 0.0761\n",
      "Epoch 452/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0702 - val_loss: 0.0761\n",
      "Epoch 453/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0704 - val_loss: 0.0757\n",
      "Epoch 454/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0705 - val_loss: 0.0751\n",
      "Epoch 455/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0705 - val_loss: 0.0754\n",
      "Epoch 456/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0703 - val_loss: 0.0756\n",
      "Epoch 457/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0704 - val_loss: 0.0762\n",
      "Epoch 458/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0703 - val_loss: 0.0759\n",
      "Epoch 459/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0704 - val_loss: 0.0752\n",
      "Epoch 460/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0702 - val_loss: 0.0762\n",
      "Epoch 461/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0703 - val_loss: 0.0757\n",
      "Epoch 462/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0701 - val_loss: 0.0753\n",
      "Epoch 463/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0706 - val_loss: 0.0751\n",
      "Epoch 464/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0702 - val_loss: 0.0760\n",
      "Epoch 465/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0702 - val_loss: 0.0758\n",
      "Epoch 466/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0703 - val_loss: 0.0761\n",
      "Epoch 467/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0703 - val_loss: 0.0763\n",
      "Epoch 468/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0703 - val_loss: 0.0761\n",
      "Epoch 469/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0703 - val_loss: 0.0756\n",
      "Epoch 470/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0704 - val_loss: 0.0750\n",
      "Epoch 471/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0704 - val_loss: 0.0757\n",
      "Epoch 472/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0701 - val_loss: 0.0766\n",
      "Epoch 473/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 7ms/step - loss: 0.0702 - val_loss: 0.0759\n",
      "Epoch 474/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - 0s 8ms/step - loss: 0.0700 - val_loss: 0.0754\n",
      "Epoch 475/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 7ms/step - loss: 0.0702 - val_loss: 0.0762\n",
      "Epoch 476/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0700 - val_loss: 0.0758\n",
      "Epoch 477/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0701 - val_loss: 0.0764\n",
      "Epoch 478/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0701 - val_loss: 0.0757\n",
      "Epoch 479/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0700 - val_loss: 0.0755\n",
      "Epoch 480/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 7ms/step - loss: 0.0701 - val_loss: 0.0756\n",
      "Epoch 481/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0700 - val_loss: 0.0763\n",
      "Epoch 482/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0701 - val_loss: 0.0756\n",
      "Epoch 483/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0701 - val_loss: 0.0761\n",
      "Epoch 484/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0700 - val_loss: 0.0756\n",
      "Epoch 485/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0700 - val_loss: 0.0755\n",
      "Epoch 486/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0699 - val_loss: 0.0764\n",
      "Epoch 487/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0701 - val_loss: 0.0758\n",
      "Epoch 488/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0703 - val_loss: 0.0761\n",
      "Epoch 489/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0701 - val_loss: 0.0754\n",
      "Epoch 490/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0702 - val_loss: 0.0765\n",
      "Epoch 491/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0703 - val_loss: 0.0757\n",
      "Epoch 492/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0701 - val_loss: 0.0759\n",
      "Epoch 493/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0701 - val_loss: 0.0754\n",
      "Epoch 494/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0700 - val_loss: 0.0765\n",
      "Epoch 495/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0700 - val_loss: 0.0752\n",
      "Epoch 496/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0703 - val_loss: 0.0763\n",
      "Epoch 497/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0701 - val_loss: 0.0756\n",
      "Epoch 498/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0701 - val_loss: 0.0761\n",
      "Epoch 499/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0703 - val_loss: 0.0760\n",
      "Epoch 500/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0700 - val_loss: 0.0753\n",
      "Epoch 501/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0699 - val_loss: 0.0756\n",
      "Epoch 502/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 7ms/step - loss: 0.0700 - val_loss: 0.0764\n",
      "Epoch 503/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0702 - val_loss: 0.0768\n",
      "Epoch 504/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0701 - val_loss: 0.0751\n",
      "Epoch 505/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0699 - val_loss: 0.0761\n",
      "Epoch 506/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0697 - val_loss: 0.0752\n",
      "Epoch 507/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0700 - val_loss: 0.0759\n",
      "Epoch 508/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 7ms/step - loss: 0.0698 - val_loss: 0.0763\n",
      "Epoch 509/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0701 - val_loss: 0.0764\n",
      "Epoch 510/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0699 - val_loss: 0.0758\n",
      "Epoch 511/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0700 - val_loss: 0.0760\n",
      "Epoch 512/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0700 - val_loss: 0.0759\n",
      "Epoch 513/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0699 - val_loss: 0.0763\n",
      "Epoch 514/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0698 - val_loss: 0.0760\n",
      "Epoch 515/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - 0s 6ms/step - loss: 0.0702 - val_loss: 0.0753\n",
      "Epoch 516/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0700 - val_loss: 0.0758\n",
      "Epoch 517/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0698 - val_loss: 0.0757\n",
      "Epoch 518/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0698 - val_loss: 0.0758\n",
      "Epoch 519/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0696 - val_loss: 0.0757\n",
      "Epoch 520/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0701 - val_loss: 0.0765\n",
      "Epoch 521/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0698 - val_loss: 0.0758\n",
      "Epoch 522/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - 0s 7ms/step - loss: 0.0699 - val_loss: 0.0755\n",
      "Epoch 523/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0698 - val_loss: 0.0765\n",
      "Epoch 524/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0696 - val_loss: 0.0755\n",
      "Epoch 525/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0697 - val_loss: 0.0759\n",
      "Epoch 526/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0698 - val_loss: 0.0766\n",
      "Epoch 527/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0699 - val_loss: 0.0751\n",
      "Epoch 528/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0699 - val_loss: 0.0761\n",
      "Epoch 529/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0699 - val_loss: 0.0759\n",
      "Epoch 530/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0698 - val_loss: 0.0760\n",
      "Epoch 531/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0697 - val_loss: 0.0760\n",
      "Epoch 532/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0698 - val_loss: 0.0754\n",
      "Epoch 533/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0697 - val_loss: 0.0764\n",
      "Epoch 534/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0699 - val_loss: 0.0754\n",
      "Epoch 535/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0698 - val_loss: 0.0761\n",
      "Epoch 536/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 7ms/step - loss: 0.0698 - val_loss: 0.0758\n",
      "Epoch 537/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0698 - val_loss: 0.0762\n",
      "Epoch 538/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0698 - val_loss: 0.0754\n",
      "Epoch 539/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0697 - val_loss: 0.0763\n",
      "Epoch 540/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0697 - val_loss: 0.0755\n",
      "Epoch 541/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0699 - val_loss: 0.0761\n",
      "Epoch 542/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0697 - val_loss: 0.0760\n",
      "Epoch 543/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0698 - val_loss: 0.0767\n",
      "Epoch 544/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0696 - val_loss: 0.0754\n",
      "Epoch 545/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0699 - val_loss: 0.0766\n",
      "Epoch 546/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0698 - val_loss: 0.0759\n",
      "Epoch 547/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0700 - val_loss: 0.0758\n",
      "Epoch 548/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0701 - val_loss: 0.0756\n",
      "Epoch 549/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0698 - val_loss: 0.0762\n",
      "Epoch 550/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0700 - val_loss: 0.0760\n",
      "Epoch 551/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0698 - val_loss: 0.0761\n",
      "Epoch 552/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0698 - val_loss: 0.0755\n",
      "Epoch 553/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0698 - val_loss: 0.0762\n",
      "Epoch 554/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0697 - val_loss: 0.0762\n",
      "Epoch 555/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0695 - val_loss: 0.0767\n",
      "Epoch 556/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 9ms/step - loss: 0.0696 - val_loss: 0.0758\n",
      "Epoch 557/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0695 - val_loss: 0.0764\n",
      "Epoch 558/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0696 - val_loss: 0.0762\n",
      "Epoch 559/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0698 - val_loss: 0.0767\n",
      "Epoch 560/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0698 - val_loss: 0.0760\n",
      "Epoch 561/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0698 - val_loss: 0.0760\n",
      "Epoch 562/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0696 - val_loss: 0.0762\n",
      "Epoch 563/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 7ms/step - loss: 0.0695 - val_loss: 0.0763\n",
      "Epoch 564/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 7ms/step - loss: 0.0695 - val_loss: 0.0764\n",
      "Epoch 565/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 7ms/step - loss: 0.0695 - val_loss: 0.0760\n",
      "Epoch 566/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 7ms/step - loss: 0.0697 - val_loss: 0.0770\n",
      "Epoch 567/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0697 - val_loss: 0.0760\n",
      "Epoch 568/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0696 - val_loss: 0.0762\n",
      "Epoch 569/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 8ms/step - loss: 0.0698 - val_loss: 0.0754\n",
      "Epoch 570/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 7ms/step - loss: 0.0696 - val_loss: 0.0758\n",
      "Epoch 571/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0696 - val_loss: 0.0761\n",
      "Epoch 572/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0695 - val_loss: 0.0763\n",
      "Epoch 573/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0696 - val_loss: 0.0758\n",
      "Epoch 574/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0696 - val_loss: 0.0757\n",
      "Epoch 575/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 8ms/step - loss: 0.0697 - val_loss: 0.0758\n",
      "Epoch 576/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0694 - val_loss: 0.0762\n",
      "Epoch 577/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0696 - val_loss: 0.0756\n",
      "Epoch 578/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0697 - val_loss: 0.0763\n",
      "Epoch 579/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 9ms/step - loss: 0.0697 - val_loss: 0.0766\n",
      "Epoch 580/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0696 - val_loss: 0.0767\n",
      "Epoch 581/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 7ms/step - loss: 0.0694 - val_loss: 0.0766\n",
      "Epoch 582/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0698 - val_loss: 0.0761\n",
      "Epoch 583/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0696 - val_loss: 0.0755\n",
      "Epoch 584/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0695 - val_loss: 0.0762\n",
      "Epoch 585/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0695 - val_loss: 0.0759\n",
      "Epoch 586/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0696 - val_loss: 0.0763\n",
      "Epoch 587/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0694 - val_loss: 0.0764\n",
      "Epoch 588/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - 0s 7ms/step - loss: 0.0695 - val_loss: 0.0763\n",
      "Epoch 589/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0692 - val_loss: 0.0760\n",
      "Epoch 590/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0695 - val_loss: 0.0761\n",
      "Epoch 591/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0695 - val_loss: 0.0762\n",
      "Epoch 592/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0693 - val_loss: 0.0760\n",
      "Epoch 593/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0695 - val_loss: 0.0765\n",
      "Epoch 594/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0695 - val_loss: 0.0767\n",
      "Epoch 595/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0697 - val_loss: 0.0765\n",
      "Epoch 596/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0696 - val_loss: 0.0763\n",
      "Epoch 597/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0695 - val_loss: 0.0768\n",
      "Epoch 598/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0697 - val_loss: 0.0761\n",
      "Epoch 599/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0697 - val_loss: 0.0761\n",
      "Epoch 600/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0697 - val_loss: 0.0764\n",
      "Epoch 601/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 10ms/step - loss: 0.0695 - val_loss: 0.0758\n",
      "Epoch 602/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0694 - val_loss: 0.0763\n",
      "Epoch 603/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0692 - val_loss: 0.0764\n",
      "Epoch 604/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0695 - val_loss: 0.0763\n",
      "Epoch 605/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0695 - val_loss: 0.0757\n",
      "Epoch 606/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0695 - val_loss: 0.0759\n",
      "Epoch 607/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 7ms/step - loss: 0.0695 - val_loss: 0.0771\n",
      "Epoch 608/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0693 - val_loss: 0.0757\n",
      "Epoch 609/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0693 - val_loss: 0.0762\n",
      "Epoch 610/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0694 - val_loss: 0.0763\n",
      "Epoch 611/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.070 - 0s 6ms/step - loss: 0.0697 - val_loss: 0.0760\n",
      "Epoch 612/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 7ms/step - loss: 0.0692 - val_loss: 0.0765\n",
      "Epoch 613/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0695 - val_loss: 0.0762\n",
      "Epoch 614/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 7ms/step - loss: 0.0694 - val_loss: 0.0766\n",
      "Epoch 615/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0695 - val_loss: 0.0773\n",
      "Epoch 616/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0694 - val_loss: 0.0759\n",
      "Epoch 617/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0694 - val_loss: 0.0760\n",
      "Epoch 618/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0693 - val_loss: 0.0770\n",
      "Epoch 619/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0692 - val_loss: 0.0758\n",
      "Epoch 620/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 9ms/step - loss: 0.0697 - val_loss: 0.0757\n",
      "Epoch 621/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0696 - val_loss: 0.0770\n",
      "Epoch 622/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0694 - val_loss: 0.0762\n",
      "Epoch 623/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0693 - val_loss: 0.0758\n",
      "Epoch 624/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0694 - val_loss: 0.0762\n",
      "Epoch 625/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0693 - val_loss: 0.0770\n",
      "Epoch 626/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0694 - val_loss: 0.0760\n",
      "Epoch 627/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 7ms/step - loss: 0.0693 - val_loss: 0.0767\n",
      "Epoch 628/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0693 - val_loss: 0.0761\n",
      "Epoch 629/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0692 - val_loss: 0.0767\n",
      "Epoch 630/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0693 - val_loss: 0.0766\n",
      "Epoch 631/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0692 - val_loss: 0.0760\n",
      "Epoch 632/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0692 - val_loss: 0.0763\n",
      "Epoch 633/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - 0s 5ms/step - loss: 0.0694 - val_loss: 0.0763\n",
      "Epoch 634/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 7ms/step - loss: 0.0694 - val_loss: 0.0760\n",
      "Epoch 635/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0694 - val_loss: 0.0760\n",
      "Epoch 636/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0692 - val_loss: 0.0757\n",
      "Epoch 637/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0694 - val_loss: 0.0757\n",
      "Epoch 638/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 9ms/step - loss: 0.0695 - val_loss: 0.0766\n",
      "Epoch 639/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0693 - val_loss: 0.0761\n",
      "Epoch 640/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0695 - val_loss: 0.0762\n",
      "Epoch 641/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0693 - val_loss: 0.0764\n",
      "Epoch 642/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0694 - val_loss: 0.0767\n",
      "Epoch 643/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0693 - val_loss: 0.0755\n",
      "Epoch 644/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0695 - val_loss: 0.0757\n",
      "Epoch 645/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0693 - val_loss: 0.0764\n",
      "Epoch 646/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0692 - val_loss: 0.0769\n",
      "Epoch 647/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0693 - val_loss: 0.0771\n",
      "Epoch 648/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0692 - val_loss: 0.0762\n",
      "Epoch 649/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0692 - val_loss: 0.0758\n",
      "Epoch 650/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0692 - val_loss: 0.0761\n",
      "Epoch 651/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.067 - 0s 6ms/step - loss: 0.0691 - val_loss: 0.0764\n",
      "Epoch 652/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0693 - val_loss: 0.0761\n",
      "Epoch 653/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0691 - val_loss: 0.0770\n",
      "Epoch 654/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0692 - val_loss: 0.0773\n",
      "Epoch 655/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 9ms/step - loss: 0.0692 - val_loss: 0.0760\n",
      "Epoch 656/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0691 - val_loss: 0.0766\n",
      "Epoch 657/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0693 - val_loss: 0.0761\n",
      "Epoch 658/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0692 - val_loss: 0.0766\n",
      "Epoch 659/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0692 - val_loss: 0.0759\n",
      "Epoch 660/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0697 - val_loss: 0.0765\n",
      "Epoch 661/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0692 - val_loss: 0.0762\n",
      "Epoch 662/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 7ms/step - loss: 0.0695 - val_loss: 0.0759\n",
      "Epoch 663/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0691 - val_loss: 0.0764\n",
      "Epoch 664/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0690 - val_loss: 0.0763\n",
      "Epoch 665/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0694 - val_loss: 0.0765\n",
      "Epoch 666/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0692 - val_loss: 0.0762\n",
      "Epoch 667/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 7ms/step - loss: 0.0692 - val_loss: 0.0771\n",
      "Epoch 668/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0693 - val_loss: 0.0763\n",
      "Epoch 669/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0693 - val_loss: 0.0758\n",
      "Epoch 670/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0692 - val_loss: 0.0765\n",
      "Epoch 671/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 7ms/step - loss: 0.0693 - val_loss: 0.0758\n",
      "Epoch 672/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 8ms/step - loss: 0.0694 - val_loss: 0.0775\n",
      "Epoch 673/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 7ms/step - loss: 0.0692 - val_loss: 0.0767\n",
      "Epoch 674/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 7ms/step - loss: 0.0691 - val_loss: 0.0761\n",
      "Epoch 675/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0695 - val_loss: 0.0762\n",
      "Epoch 676/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0693 - val_loss: 0.0764\n",
      "Epoch 677/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0689 - val_loss: 0.0762\n",
      "Epoch 678/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0690 - val_loss: 0.0756\n",
      "Epoch 679/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0692 - val_loss: 0.0767\n",
      "Epoch 680/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0691 - val_loss: 0.0760\n",
      "Epoch 681/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0690 - val_loss: 0.0770\n",
      "Epoch 682/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0691 - val_loss: 0.0765\n",
      "Epoch 683/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 7ms/step - loss: 0.0691 - val_loss: 0.0759\n",
      "Epoch 684/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0693 - val_loss: 0.0780\n",
      "Epoch 685/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 7ms/step - loss: 0.0692 - val_loss: 0.0769\n",
      "Epoch 686/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 9ms/step - loss: 0.0692 - val_loss: 0.0764\n",
      "Epoch 687/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0691 - val_loss: 0.0772\n",
      "Epoch 688/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0691 - val_loss: 0.0761\n",
      "Epoch 689/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0693 - val_loss: 0.0780\n",
      "Epoch 690/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0690 - val_loss: 0.0767\n",
      "Epoch 691/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0691 - val_loss: 0.0761\n",
      "Epoch 692/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0692 - val_loss: 0.0764\n",
      "Epoch 693/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0693 - val_loss: 0.0771\n",
      "Epoch 694/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0690 - val_loss: 0.0759\n",
      "Epoch 695/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0691 - val_loss: 0.0755\n",
      "Epoch 696/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 7ms/step - loss: 0.0691 - val_loss: 0.0765\n",
      "Epoch 697/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0692 - val_loss: 0.0764\n",
      "Epoch 698/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 7ms/step - loss: 0.0693 - val_loss: 0.0761\n",
      "Epoch 699/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0691 - val_loss: 0.0770\n",
      "Epoch 700/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 11ms/step - loss: 0.0691 - val_loss: 0.0764\n",
      "Epoch 701/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0689 - val_loss: 0.0764\n",
      "Epoch 702/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 8ms/step - loss: 0.0691 - val_loss: 0.0761\n",
      "Epoch 703/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 7ms/step - loss: 0.0692 - val_loss: 0.0761\n",
      "Epoch 704/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 7ms/step - loss: 0.0691 - val_loss: 0.0767\n",
      "Epoch 705/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 7ms/step - loss: 0.0689 - val_loss: 0.0762\n",
      "Epoch 706/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 7ms/step - loss: 0.0689 - val_loss: 0.0768\n",
      "Epoch 707/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0687 - val_loss: 0.0772\n",
      "Epoch 708/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0690 - val_loss: 0.0774\n",
      "Epoch 709/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0689 - val_loss: 0.0761\n",
      "Epoch 710/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0689 - val_loss: 0.0763\n",
      "Epoch 711/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0690 - val_loss: 0.0760\n",
      "Epoch 712/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 8ms/step - loss: 0.0689 - val_loss: 0.0774\n",
      "Epoch 713/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0689 - val_loss: 0.0767\n",
      "Epoch 714/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0690 - val_loss: 0.0772\n",
      "Epoch 715/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0690 - val_loss: 0.0760\n",
      "Epoch 716/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0689 - val_loss: 0.0764\n",
      "Epoch 717/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0689 - val_loss: 0.0776\n",
      "Epoch 718/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0692 - val_loss: 0.0766\n",
      "Epoch 719/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0689 - val_loss: 0.0767\n",
      "Epoch 720/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0689 - val_loss: 0.0767\n",
      "Epoch 721/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0691 - val_loss: 0.0761\n",
      "Epoch 722/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0690 - val_loss: 0.0761\n",
      "Epoch 723/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0689 - val_loss: 0.0768\n",
      "Epoch 724/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0689 - val_loss: 0.0768\n",
      "Epoch 725/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 9ms/step - loss: 0.0691 - val_loss: 0.0759\n",
      "Epoch 726/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0689 - val_loss: 0.0769\n",
      "Epoch 727/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0689 - val_loss: 0.0764\n",
      "Epoch 728/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 7ms/step - loss: 0.0689 - val_loss: 0.0768\n",
      "Epoch 729/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0689 - val_loss: 0.0765\n",
      "Epoch 730/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 8ms/step - loss: 0.0688 - val_loss: 0.0770\n",
      "Epoch 731/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 7ms/step - loss: 0.0692 - val_loss: 0.0768\n",
      "Epoch 732/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0690 - val_loss: 0.0766\n",
      "Epoch 733/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0690 - val_loss: 0.0768\n",
      "Epoch 734/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 7ms/step - loss: 0.0689 - val_loss: 0.0770\n",
      "Epoch 735/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0689 - val_loss: 0.0765\n",
      "Epoch 736/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 9ms/step - loss: 0.0689 - val_loss: 0.0765\n",
      "Epoch 737/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0690 - val_loss: 0.0774\n",
      "Epoch 738/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0689 - val_loss: 0.0761\n",
      "Epoch 739/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0688 - val_loss: 0.0766\n",
      "Epoch 740/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0689 - val_loss: 0.0774\n",
      "Epoch 741/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0688 - val_loss: 0.0762\n",
      "Epoch 742/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0688 - val_loss: 0.0767\n",
      "Epoch 743/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0690 - val_loss: 0.0774\n",
      "Epoch 744/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0688 - val_loss: 0.0762\n",
      "Epoch 745/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0691 - val_loss: 0.0770\n",
      "Epoch 746/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0691 - val_loss: 0.0759\n",
      "Epoch 747/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0690 - val_loss: 0.0764\n",
      "Epoch 748/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 8ms/step - loss: 0.0689 - val_loss: 0.0766\n",
      "Epoch 749/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0689 - val_loss: 0.0779\n",
      "Epoch 750/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0688 - val_loss: 0.0770\n",
      "Epoch 751/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0690 - val_loss: 0.0765\n",
      "Epoch 752/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0689 - val_loss: 0.0768\n",
      "Epoch 753/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0688 - val_loss: 0.0772\n",
      "Epoch 754/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0688 - val_loss: 0.0773\n",
      "Epoch 755/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0688 - val_loss: 0.0759\n",
      "Epoch 756/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0689 - val_loss: 0.0764\n",
      "Epoch 757/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0686 - val_loss: 0.0762\n",
      "Epoch 758/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0687 - val_loss: 0.0768\n",
      "Epoch 759/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0688 - val_loss: 0.0767\n",
      "Epoch 760/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 10ms/step - loss: 0.0687 - val_loss: 0.0766\n",
      "Epoch 761/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0688 - val_loss: 0.0764\n",
      "Epoch 762/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0689 - val_loss: 0.0769\n",
      "Epoch 763/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0687 - val_loss: 0.0768\n",
      "Epoch 764/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0690 - val_loss: 0.0760\n",
      "Epoch 765/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0686 - val_loss: 0.0765\n",
      "Epoch 766/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0688 - val_loss: 0.0765\n",
      "Epoch 767/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0689 - val_loss: 0.0768\n",
      "Epoch 768/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0689 - val_loss: 0.0765\n",
      "Epoch 769/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0686 - val_loss: 0.0768\n",
      "Epoch 770/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0688 - val_loss: 0.0769\n",
      "Epoch 771/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0686 - val_loss: 0.0771\n",
      "Epoch 772/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 9ms/step - loss: 0.0686 - val_loss: 0.0763\n",
      "Epoch 773/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0691 - val_loss: 0.0773\n",
      "Epoch 774/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0689 - val_loss: 0.0767\n",
      "Epoch 775/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0687 - val_loss: 0.0775\n",
      "Epoch 776/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0689 - val_loss: 0.0761\n",
      "Epoch 777/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0688 - val_loss: 0.0769\n",
      "Epoch 778/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0688 - val_loss: 0.0771\n",
      "Epoch 779/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0689 - val_loss: 0.0773\n",
      "Epoch 780/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0688 - val_loss: 0.0772\n",
      "Epoch 781/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0688 - val_loss: 0.0764\n",
      "Epoch 782/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0689 - val_loss: 0.0772\n",
      "Epoch 783/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0688 - val_loss: 0.0770\n",
      "Epoch 784/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 9ms/step - loss: 0.0685 - val_loss: 0.0773\n",
      "Epoch 785/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0686 - val_loss: 0.0768\n",
      "Epoch 786/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0687 - val_loss: 0.0763\n",
      "Epoch 787/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0689 - val_loss: 0.0778\n",
      "Epoch 788/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0689 - val_loss: 0.0763\n",
      "Epoch 789/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0688 - val_loss: 0.0765\n",
      "Epoch 790/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0688 - val_loss: 0.0773\n",
      "Epoch 791/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0686 - val_loss: 0.0760\n",
      "Epoch 792/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0687 - val_loss: 0.0768\n",
      "Epoch 793/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0689 - val_loss: 0.0767\n",
      "Epoch 794/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0688 - val_loss: 0.0768\n",
      "Epoch 795/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 8ms/step - loss: 0.0687 - val_loss: 0.0773\n",
      "Epoch 796/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0689 - val_loss: 0.0771\n",
      "Epoch 797/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 8ms/step - loss: 0.0688 - val_loss: 0.0777\n",
      "Epoch 798/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0687 - val_loss: 0.0765\n",
      "Epoch 799/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0688 - val_loss: 0.0764\n",
      "Epoch 800/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0688 - val_loss: 0.0764\n",
      "Epoch 801/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0688 - val_loss: 0.0771\n",
      "Epoch 802/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0691 - val_loss: 0.0784\n",
      "Epoch 803/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0687 - val_loss: 0.0765\n",
      "Epoch 804/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0686 - val_loss: 0.0769\n",
      "Epoch 805/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 8ms/step - loss: 0.0685 - val_loss: 0.0772\n",
      "Epoch 806/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0686 - val_loss: 0.0774\n",
      "Epoch 807/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0686 - val_loss: 0.0763\n",
      "Epoch 808/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0689 - val_loss: 0.0765\n",
      "Epoch 809/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0687 - val_loss: 0.0783\n",
      "Epoch 810/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0686 - val_loss: 0.0768\n",
      "Epoch 811/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0685 - val_loss: 0.0769\n",
      "Epoch 812/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0684 - val_loss: 0.0769\n",
      "Epoch 813/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0687 - val_loss: 0.0768\n",
      "Epoch 814/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0686 - val_loss: 0.0770\n",
      "Epoch 815/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0686 - val_loss: 0.0771\n",
      "Epoch 816/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 10ms/step - loss: 0.0686 - val_loss: 0.0769\n",
      "Epoch 817/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0684 - val_loss: 0.0762\n",
      "Epoch 818/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0686 - val_loss: 0.0770\n",
      "Epoch 819/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0686 - val_loss: 0.0766\n",
      "Epoch 820/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0686 - val_loss: 0.0777\n",
      "Epoch 821/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0685 - val_loss: 0.0770\n",
      "Epoch 822/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0687 - val_loss: 0.0773\n",
      "Epoch 823/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0687 - val_loss: 0.0767\n",
      "Epoch 824/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0683 - val_loss: 0.0777\n",
      "Epoch 825/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0685 - val_loss: 0.0772\n",
      "Epoch 826/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0684 - val_loss: 0.0778\n",
      "Epoch 827/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0688 - val_loss: 0.0769\n",
      "Epoch 828/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.067 - 0s 7ms/step - loss: 0.0687 - val_loss: 0.0777\n",
      "Epoch 829/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0688 - val_loss: 0.0769\n",
      "Epoch 830/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0686 - val_loss: 0.0774\n",
      "Epoch 831/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0686 - val_loss: 0.0786\n",
      "Epoch 832/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0687 - val_loss: 0.0791\n",
      "Epoch 833/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0687 - val_loss: 0.0769\n",
      "Epoch 834/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0687 - val_loss: 0.0781\n",
      "Epoch 835/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0687 - val_loss: 0.0773\n",
      "Epoch 836/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0686 - val_loss: 0.0774\n",
      "Epoch 837/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0686 - val_loss: 0.0762\n",
      "Epoch 838/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0689 - val_loss: 0.0767\n",
      "Epoch 839/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0685 - val_loss: 0.0778\n",
      "Epoch 840/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0686 - val_loss: 0.0768\n",
      "Epoch 841/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0685 - val_loss: 0.0766\n",
      "Epoch 842/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0686 - val_loss: 0.0774\n",
      "Epoch 843/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0683 - val_loss: 0.0775\n",
      "Epoch 844/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0687 - val_loss: 0.0769\n",
      "Epoch 845/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.067 - 0s 5ms/step - loss: 0.0682 - val_loss: 0.0767\n",
      "Epoch 846/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - 0s 5ms/step - loss: 0.0690 - val_loss: 0.0786\n",
      "Epoch 847/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0689 - val_loss: 0.0770\n",
      "Epoch 848/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.067 - 0s 6ms/step - loss: 0.0686 - val_loss: 0.0765\n",
      "Epoch 849/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0687 - val_loss: 0.0762\n",
      "Epoch 850/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0687 - val_loss: 0.0768\n",
      "Epoch 851/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 8ms/step - loss: 0.0687 - val_loss: 0.0769\n",
      "Epoch 852/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0687 - val_loss: 0.0771\n",
      "Epoch 853/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0686 - val_loss: 0.0767\n",
      "Epoch 854/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0686 - val_loss: 0.0772\n",
      "Epoch 855/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0688 - val_loss: 0.0765\n",
      "Epoch 856/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0685 - val_loss: 0.0765\n",
      "Epoch 857/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0684 - val_loss: 0.0769\n",
      "Epoch 858/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0688 - val_loss: 0.0773\n",
      "Epoch 859/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0684 - val_loss: 0.0762\n",
      "Epoch 860/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0687 - val_loss: 0.0767\n",
      "Epoch 861/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 11ms/step - loss: 0.0686 - val_loss: 0.0767\n",
      "Epoch 862/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 11ms/step - loss: 0.0685 - val_loss: 0.0771\n",
      "Epoch 863/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 10ms/step - loss: 0.0686 - val_loss: 0.0760\n",
      "Epoch 864/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 9ms/step - loss: 0.0685 - val_loss: 0.0768\n",
      "Epoch 865/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 8ms/step - loss: 0.0686 - val_loss: 0.0765\n",
      "Epoch 866/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 8ms/step - loss: 0.0683 - val_loss: 0.0768\n",
      "Epoch 867/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 8ms/step - loss: 0.0684 - val_loss: 0.0767\n",
      "Epoch 868/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0682 - val_loss: 0.0767\n",
      "Epoch 869/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0686 - val_loss: 0.0767\n",
      "Epoch 870/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0683 - val_loss: 0.0777\n",
      "Epoch 871/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0685 - val_loss: 0.0767\n",
      "Epoch 872/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0685 - val_loss: 0.0767\n",
      "Epoch 873/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0683 - val_loss: 0.0768\n",
      "Epoch 874/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0685 - val_loss: 0.0769\n",
      "Epoch 875/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0685 - val_loss: 0.0773\n",
      "Epoch 876/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0684 - val_loss: 0.0772\n",
      "Epoch 877/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 9ms/step - loss: 0.0686 - val_loss: 0.0780\n",
      "Epoch 878/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0685 - val_loss: 0.0770\n",
      "Epoch 879/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0685 - val_loss: 0.0778\n",
      "Epoch 880/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 8ms/step - loss: 0.0683 - val_loss: 0.0775\n",
      "Epoch 881/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0684 - val_loss: 0.0770\n",
      "Epoch 882/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 10ms/step - loss: 0.0686 - val_loss: 0.0764\n",
      "Epoch 883/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 11ms/step - loss: 0.0687 - val_loss: 0.0773\n",
      "Epoch 884/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0685 - val_loss: 0.0765\n",
      "Epoch 885/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0686 - val_loss: 0.0779\n",
      "Epoch 886/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0685 - val_loss: 0.0777\n",
      "Epoch 887/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 9ms/step - loss: 0.0682 - val_loss: 0.0782\n",
      "Epoch 888/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 9ms/step - loss: 0.0684 - val_loss: 0.0769\n",
      "Epoch 889/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0684 - val_loss: 0.0771\n",
      "Epoch 890/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0684 - val_loss: 0.0777\n",
      "Epoch 891/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0684 - val_loss: 0.0778\n",
      "Epoch 892/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 8ms/step - loss: 0.0685 - val_loss: 0.0773\n",
      "Epoch 893/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0687 - val_loss: 0.0769\n",
      "Epoch 894/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 8ms/step - loss: 0.0687 - val_loss: 0.0789\n",
      "Epoch 895/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0684 - val_loss: 0.0774\n",
      "Epoch 896/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0686 - val_loss: 0.0761\n",
      "Epoch 897/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0683 - val_loss: 0.0774\n",
      "Epoch 898/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0686 - val_loss: 0.0765\n",
      "Epoch 899/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 8ms/step - loss: 0.0687 - val_loss: 0.0773\n",
      "Epoch 900/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0684 - val_loss: 0.0771\n",
      "Epoch 901/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0682 - val_loss: 0.0773\n",
      "Epoch 902/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0684 - val_loss: 0.0779\n",
      "Epoch 903/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 8ms/step - loss: 0.0687 - val_loss: 0.0779\n",
      "Epoch 904/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0684 - val_loss: 0.0777\n",
      "Epoch 905/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.067 - 0s 6ms/step - loss: 0.0684 - val_loss: 0.0776\n",
      "Epoch 906/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0685 - val_loss: 0.0783\n",
      "Epoch 907/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0686 - val_loss: 0.0784\n",
      "Epoch 908/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0686 - val_loss: 0.0777\n",
      "Epoch 909/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0683 - val_loss: 0.0771\n",
      "Epoch 910/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0681 - val_loss: 0.0779\n",
      "Epoch 911/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.068 - 0s 10ms/step - loss: 0.0683 - val_loss: 0.0776\n",
      "Epoch 912/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 8ms/step - loss: 0.0683 - val_loss: 0.0774\n",
      "Epoch 913/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.067 - 0s 6ms/step - loss: 0.0682 - val_loss: 0.0783\n",
      "Epoch 914/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0685 - val_loss: 0.0779\n",
      "Epoch 915/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0684 - val_loss: 0.0767\n",
      "Epoch 916/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0683 - val_loss: 0.0766\n",
      "Epoch 917/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0686 - val_loss: 0.0780\n",
      "Epoch 918/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0684 - val_loss: 0.0770\n",
      "Epoch 919/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.063 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0684 - val_loss: 0.0770\n",
      "Epoch 920/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0684 - val_loss: 0.0771\n",
      "Epoch 921/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0682 - val_loss: 0.0768\n",
      "Epoch 922/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.067 - 0s 7ms/step - loss: 0.0680 - val_loss: 0.0776\n",
      "Epoch 923/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0683 - val_loss: 0.0786\n",
      "Epoch 924/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0684 - val_loss: 0.0765\n",
      "Epoch 925/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0683 - val_loss: 0.0776\n",
      "Epoch 926/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.067 - 0s 6ms/step - loss: 0.0683 - val_loss: 0.0781\n",
      "Epoch 927/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0684 - val_loss: 0.0788\n",
      "Epoch 928/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.067 - 0s 6ms/step - loss: 0.0680 - val_loss: 0.0775\n",
      "Epoch 929/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0684 - val_loss: 0.0777\n",
      "Epoch 930/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0684 - val_loss: 0.0773\n",
      "Epoch 931/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0682 - val_loss: 0.0765\n",
      "Epoch 932/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0684 - val_loss: 0.0771\n",
      "Epoch 933/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.067 - 0s 6ms/step - loss: 0.0683 - val_loss: 0.0788\n",
      "Epoch 934/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0684 - val_loss: 0.0767\n",
      "Epoch 935/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0685 - val_loss: 0.0763\n",
      "Epoch 936/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0685 - val_loss: 0.0777\n",
      "Epoch 937/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0684 - val_loss: 0.0765\n",
      "Epoch 938/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0683 - val_loss: 0.0778\n",
      "Epoch 939/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 8ms/step - loss: 0.0682 - val_loss: 0.0765\n",
      "Epoch 940/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0683 - val_loss: 0.0786\n",
      "Epoch 941/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.067 - 0s 6ms/step - loss: 0.0683 - val_loss: 0.0769\n",
      "Epoch 942/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0687 - val_loss: 0.0767\n",
      "Epoch 943/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0682 - val_loss: 0.0772\n",
      "Epoch 944/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0683 - val_loss: 0.0763\n",
      "Epoch 945/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0685 - val_loss: 0.0779\n",
      "Epoch 946/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0682 - val_loss: 0.0768\n",
      "Epoch 947/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0684 - val_loss: 0.0776\n",
      "Epoch 948/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0682 - val_loss: 0.0777\n",
      "Epoch 949/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0684 - val_loss: 0.0779\n",
      "Epoch 950/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0685 - val_loss: 0.0775\n",
      "Epoch 951/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0686 - val_loss: 0.0775\n",
      "Epoch 952/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0680 - val_loss: 0.0771\n",
      "Epoch 953/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0680 - val_loss: 0.0770\n",
      "Epoch 954/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0685 - val_loss: 0.0780\n",
      "Epoch 955/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0683 - val_loss: 0.0764\n",
      "Epoch 956/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0683 - val_loss: 0.0769\n",
      "Epoch 957/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 8ms/step - loss: 0.0681 - val_loss: 0.0771\n",
      "Epoch 958/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0681 - val_loss: 0.0778\n",
      "Epoch 959/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0685 - val_loss: 0.0771\n",
      "Epoch 960/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0683 - val_loss: 0.0768\n",
      "Epoch 961/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0682 - val_loss: 0.0771\n",
      "Epoch 962/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0682 - val_loss: 0.0764\n",
      "Epoch 963/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.067 - 0s 5ms/step - loss: 0.0683 - val_loss: 0.0781\n",
      "Epoch 964/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0684 - val_loss: 0.0770\n",
      "Epoch 965/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0680 - val_loss: 0.0776\n",
      "Epoch 966/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.067 - 0s 6ms/step - loss: 0.0682 - val_loss: 0.0780\n",
      "Epoch 967/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0682 - val_loss: 0.0773\n",
      "Epoch 968/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.067 - 0s 6ms/step - loss: 0.0682 - val_loss: 0.0774\n",
      "Epoch 969/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0684 - val_loss: 0.0769\n",
      "Epoch 970/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0685 - val_loss: 0.0770\n",
      "Epoch 971/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - 0s 6ms/step - loss: 0.0682 - val_loss: 0.0777\n",
      "Epoch 972/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 8ms/step - loss: 0.0680 - val_loss: 0.0778\n",
      "Epoch 973/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0682 - val_loss: 0.0772\n",
      "Epoch 974/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0682 - val_loss: 0.0786\n",
      "Epoch 975/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0682 - val_loss: 0.0770\n",
      "Epoch 976/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0681 - val_loss: 0.0772\n",
      "Epoch 977/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0681 - val_loss: 0.0767\n",
      "Epoch 978/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0681 - val_loss: 0.0772\n",
      "Epoch 979/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.062 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0681 - val_loss: 0.0770\n",
      "Epoch 980/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0681 - val_loss: 0.0772\n",
      "Epoch 981/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.067 - 0s 6ms/step - loss: 0.0679 - val_loss: 0.0778\n",
      "Epoch 982/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0683 - val_loss: 0.0778\n",
      "Epoch 983/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0682 - val_loss: 0.0772\n",
      "Epoch 984/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0680 - val_loss: 0.0773\n",
      "Epoch 985/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0682 - val_loss: 0.0776\n",
      "Epoch 986/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0683 - val_loss: 0.0766\n",
      "Epoch 987/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0682 - val_loss: 0.0765\n",
      "Epoch 988/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0685 - val_loss: 0.0783\n",
      "Epoch 989/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0683 - val_loss: 0.0772\n",
      "Epoch 990/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 5ms/step - loss: 0.0683 - val_loss: 0.0772\n",
      "Epoch 991/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.067 - 0s 5ms/step - loss: 0.0678 - val_loss: 0.0772\n",
      "Epoch 992/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0682 - val_loss: 0.0771\n",
      "Epoch 993/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.067 - 0s 6ms/step - loss: 0.0682 - val_loss: 0.0764\n",
      "Epoch 994/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.067 - 0s 6ms/step - loss: 0.0681 - val_loss: 0.0782\n",
      "Epoch 995/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0681 - val_loss: 0.0777\n",
      "Epoch 996/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.068 - 0s 7ms/step - loss: 0.0680 - val_loss: 0.0769\n",
      "Epoch 997/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0683 - val_loss: 0.0770\n",
      "Epoch 998/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.067 - 0s 5ms/step - loss: 0.0680 - val_loss: 0.0773\n",
      "Epoch 999/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.067 - 0s 6ms/step - loss: 0.0679 - val_loss: 0.0779\n",
      "Epoch 1000/1000\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.068 - 0s 6ms/step - loss: 0.0682 - val_loss: 0.0777\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Dense(100, activation='relu',\n",
    "                      input_shape=(len(feature_names),),\n",
    "                      kernel_initializer=initializers.he_normal(seed=0)))\n",
    "model.add(layers.Dense(100, activation='relu',\n",
    "                      kernel_initializer=initializers.he_normal(seed=0)))\n",
    "model.add(layers.Dropout(0.7))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=\"adam\",)\n",
    "\n",
    "print('start fitting')\n",
    "history = model.fit(dataset_train[feature_names], dataset_train[predi_target],\n",
    "                    batch_size=1000,         #1000  #每一个batch的大小\n",
    "                    epochs=1000, #225          #迭代次数\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    #validation_data =        #(测试集的输入特征，测试集的标签），\n",
    "                    #validation_split =       # 从测试集中划分多少比例给训练集，\n",
    "                    #validation_freq = 20        #测试的epoch间隔数                     \n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1899604fa08>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(history.history['val_loss'][1:])\n",
    "plt.plot(history.history['loss'][1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lightgbm Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---cf.fit---\n",
      "LGBMRegressor(n_estimators=5000)\n",
      "---cf.score---\n",
      "-0.12828719998293647\n",
      "---predict---\n",
      "[0.24071573 0.39856006 0.51996054 ... 0.64851327 0.55666052 0.1145708 ]\n"
     ]
    }
   ],
   "source": [
    "##############################################　　　自己加入的　　　##############################################\n",
    "import lightgbm as lgb\n",
    "\n",
    "cf = lgb.LGBMRegressor(n_estimators=5000)\n",
    "\n",
    "print('---cf.fit---')\n",
    "print(cf.fit(*train))\n",
    "print('---cf.score---')\n",
    "print(cf.score(*test))\n",
    "print('---predict---')\n",
    "print(cf.predict(test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 參數優化_1110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import lightgbm\n",
    "\n",
    "fit_params={\"early_stopping_rounds\":30, \n",
    "            \"eval_metric\" : 'auc', \n",
    "            \"eval_set\" : [test],\n",
    "            'eval_names': ['valid'],\n",
    "            'verbose': 100,\n",
    "            'categorical_feature': 'auto'}\n",
    "\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "param_test ={'num_leaves': sp_randint(6, 50), \n",
    "             'min_child_samples': sp_randint(100, 500), \n",
    "             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "             'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
    "             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
    "             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n",
    "\n",
    "#This parameter defines the number of HP points to be tested\n",
    "n_HP_points_to_test = 100\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "#n_estimators is set to a \"large value\". The actual number of trees build will depend on early stopping and 5000 define only the absolute maximum\n",
    "clf = lgb.LGBMClassifier(max_depth=-1, random_state=314, silent=True, metric='None', n_jobs=4, n_estimators=5000)\n",
    "gs = RandomizedSearchCV(\n",
    "    estimator=clf, param_distributions=param_test, \n",
    "    n_iter=n_HP_points_to_test,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    refit=True,\n",
    "    random_state=314,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.529865\n",
      "Early stopping, best iteration is:\n",
      "[130]\tvalid's auc: 0.531258\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid's auc: 0.49131\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.570722\n",
      "Early stopping, best iteration is:\n",
      "[147]\tvalid's auc: 0.5723\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.532823\n",
      "Early stopping, best iteration is:\n",
      "[139]\tvalid's auc: 0.533666\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.492937\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.571767\n",
      "Early stopping, best iteration is:\n",
      "[92]\tvalid's auc: 0.572007\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.515626\n",
      "[200]\tvalid's auc: 0.521547\n",
      "[300]\tvalid's auc: 0.52239\n",
      "Early stopping, best iteration is:\n",
      "[271]\tvalid's auc: 0.523112\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[44]\tvalid's auc: 0.486856\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.558696\n",
      "[200]\tvalid's auc: 0.560936\n",
      "Early stopping, best iteration is:\n",
      "[190]\tvalid's auc: 0.561424\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[59]\tvalid's auc: 0.518206\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.489226\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[68]\tvalid's auc: 0.566412\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.530164\n",
      "[200]\tvalid's auc: 0.532668\n",
      "Early stopping, best iteration is:\n",
      "[229]\tvalid's auc: 0.534698\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.488501\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.572452\n",
      "[200]\tvalid's auc: 0.57306\n",
      "Early stopping, best iteration is:\n",
      "[191]\tvalid's auc: 0.573591\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.532343\n",
      "Early stopping, best iteration is:\n",
      "[110]\tvalid's auc: 0.533126\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.489045\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[33]\tvalid's auc: 0.570517\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.53257\n",
      "Early stopping, best iteration is:\n",
      "[95]\tvalid's auc: 0.533196\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid's auc: 0.492059\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.568428\n",
      "Early stopping, best iteration is:\n",
      "[80]\tvalid's auc: 0.570281\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.532446\n",
      "Early stopping, best iteration is:\n",
      "[134]\tvalid's auc: 0.5348\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[50]\tvalid's auc: 0.489899\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.572735\n",
      "Early stopping, best iteration is:\n",
      "[119]\tvalid's auc: 0.573263\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.530678\n",
      "Early stopping, best iteration is:\n",
      "[143]\tvalid's auc: 0.533243\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.489748\n",
      "Early stopping, best iteration is:\n",
      "[130]\tvalid's auc: 0.491883\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.571709\n",
      "Early stopping, best iteration is:\n",
      "[95]\tvalid's auc: 0.572276\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.53201\n",
      "Early stopping, best iteration is:\n",
      "[110]\tvalid's auc: 0.533397\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[27]\tvalid's auc: 0.491474\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.571555\n",
      "Early stopping, best iteration is:\n",
      "[90]\tvalid's auc: 0.571997\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.533422\n",
      "Early stopping, best iteration is:\n",
      "[135]\tvalid's auc: 0.534046\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[48]\tvalid's auc: 0.490698\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[42]\tvalid's auc: 0.571354\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.53231\n",
      "Early stopping, best iteration is:\n",
      "[134]\tvalid's auc: 0.535014\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalid's auc: 0.490029\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.570646\n",
      "Early stopping, best iteration is:\n",
      "[94]\tvalid's auc: 0.570943\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.514481\n",
      "[200]\tvalid's auc: 0.521393\n",
      "Early stopping, best iteration is:\n",
      "[258]\tvalid's auc: 0.522646\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[64]\tvalid's auc: 0.488557\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.557449\n",
      "[200]\tvalid's auc: 0.560474\n",
      "Early stopping, best iteration is:\n",
      "[174]\tvalid's auc: 0.561055\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.531036\n",
      "Early stopping, best iteration is:\n",
      "[156]\tvalid's auc: 0.532462\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.491048\n",
      "Early stopping, best iteration is:\n",
      "[110]\tvalid's auc: 0.491299\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.572044\n",
      "Early stopping, best iteration is:\n",
      "[140]\tvalid's auc: 0.573233\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.533056\n",
      "Early stopping, best iteration is:\n",
      "[147]\tvalid's auc: 0.534972\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.490211\n",
      "Early stopping, best iteration is:\n",
      "[131]\tvalid's auc: 0.491317\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.570516\n",
      "[200]\tvalid's auc: 0.573174\n",
      "Early stopping, best iteration is:\n",
      "[205]\tvalid's auc: 0.573299\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.534235\n",
      "Early stopping, best iteration is:\n",
      "[100]\tvalid's auc: 0.534235\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.494214\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.571104\n",
      "Early stopping, best iteration is:\n",
      "[163]\tvalid's auc: 0.572636\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.528622\n",
      "Early stopping, best iteration is:\n",
      "[112]\tvalid's auc: 0.529504\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.490183\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.569523\n",
      "Early stopping, best iteration is:\n",
      "[128]\tvalid's auc: 0.570052\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.532149\n",
      "Early stopping, best iteration is:\n",
      "[92]\tvalid's auc: 0.532716\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.491653\n",
      "[200]\tvalid's auc: 0.494657\n",
      "Early stopping, best iteration is:\n",
      "[258]\tvalid's auc: 0.496947\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[63]\tvalid's auc: 0.571198\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid's auc: 0.517531\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.491603\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[60]\tvalid's auc: 0.56819\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.530898\n",
      "Early stopping, best iteration is:\n",
      "[139]\tvalid's auc: 0.533496\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.489671\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.572153\n",
      "[200]\tvalid's auc: 0.573897\n",
      "Early stopping, best iteration is:\n",
      "[201]\tvalid's auc: 0.574156\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[61]\tvalid's auc: 0.518741\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.496003\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[65]\tvalid's auc: 0.568448\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.531266\n",
      "Early stopping, best iteration is:\n",
      "[140]\tvalid's auc: 0.533872\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.489719\n",
      "Early stopping, best iteration is:\n",
      "[79]\tvalid's auc: 0.490836\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.571679\n",
      "Early stopping, best iteration is:\n",
      "[156]\tvalid's auc: 0.573595\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.529677\n",
      "Early stopping, best iteration is:\n",
      "[80]\tvalid's auc: 0.530274\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.493909\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.570303\n",
      "Early stopping, best iteration is:\n",
      "[94]\tvalid's auc: 0.571012\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.530349\n",
      "Early stopping, best iteration is:\n",
      "[104]\tvalid's auc: 0.530918\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[52]\tvalid's auc: 0.489625\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid's auc: 0.571667\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.533249\n",
      "Early stopping, best iteration is:\n",
      "[108]\tvalid's auc: 0.534037\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.492908\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[57]\tvalid's auc: 0.571036\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.530854\n",
      "[200]\tvalid's auc: 0.533084\n",
      "Early stopping, best iteration is:\n",
      "[171]\tvalid's auc: 0.533245\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.489514\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.570188\n",
      "Early stopping, best iteration is:\n",
      "[155]\tvalid's auc: 0.571627\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.529106\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid's auc: 0.530745\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[27]\tvalid's auc: 0.491991\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.569863\n",
      "Early stopping, best iteration is:\n",
      "[128]\tvalid's auc: 0.570962\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[59]\tvalid's auc: 0.518063\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.494699\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[58]\tvalid's auc: 0.56688\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.533957\n",
      "Early stopping, best iteration is:\n",
      "[110]\tvalid's auc: 0.534885\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[60]\tvalid's auc: 0.491052\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[60]\tvalid's auc: 0.5729\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid's auc: 0.531456\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.4937\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[53]\tvalid's auc: 0.569821\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.529219\n",
      "Early stopping, best iteration is:\n",
      "[119]\tvalid's auc: 0.53053\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[27]\tvalid's auc: 0.489177\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.568517\n",
      "Early stopping, best iteration is:\n",
      "[112]\tvalid's auc: 0.569116\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.53194\n",
      "Early stopping, best iteration is:\n",
      "[107]\tvalid's auc: 0.532123\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.492102\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.571972\n",
      "Early stopping, best iteration is:\n",
      "[73]\tvalid's auc: 0.572608\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.532858\n",
      "Early stopping, best iteration is:\n",
      "[131]\tvalid's auc: 0.533492\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid's auc: 0.490381\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[62]\tvalid's auc: 0.574072\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.51623\n",
      "[200]\tvalid's auc: 0.521544\n",
      "[300]\tvalid's auc: 0.523135\n",
      "Early stopping, best iteration is:\n",
      "[331]\tvalid's auc: 0.52398\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[60]\tvalid's auc: 0.488769\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.5571\n",
      "[200]\tvalid's auc: 0.559569\n",
      "Early stopping, best iteration is:\n",
      "[252]\tvalid's auc: 0.560413\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.531185\n",
      "[200]\tvalid's auc: 0.533866\n",
      "Early stopping, best iteration is:\n",
      "[200]\tvalid's auc: 0.533866\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[21]\tvalid's auc: 0.493877\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.569387\n",
      "Early stopping, best iteration is:\n",
      "[87]\tvalid's auc: 0.570241\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.532826\n",
      "Early stopping, best iteration is:\n",
      "[77]\tvalid's auc: 0.533886\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.490087\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid's auc: 0.570973\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[64]\tvalid's auc: 0.518053\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.488797\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[67]\tvalid's auc: 0.568066\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.532503\n",
      "Early stopping, best iteration is:\n",
      "[76]\tvalid's auc: 0.532681\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.49231\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.569104\n",
      "Early stopping, best iteration is:\n",
      "[75]\tvalid's auc: 0.570125\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.532971\n",
      "Early stopping, best iteration is:\n",
      "[139]\tvalid's auc: 0.535553\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.487478\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[63]\tvalid's auc: 0.570844\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.515961\n",
      "[200]\tvalid's auc: 0.519871\n",
      "Early stopping, best iteration is:\n",
      "[260]\tvalid's auc: 0.522027\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[63]\tvalid's auc: 0.489639\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.5578\n",
      "Early stopping, best iteration is:\n",
      "[123]\tvalid's auc: 0.559706\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.515776\n",
      "[200]\tvalid's auc: 0.520766\n",
      "[300]\tvalid's auc: 0.522871\n",
      "Early stopping, best iteration is:\n",
      "[285]\tvalid's auc: 0.523306\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[39]\tvalid's auc: 0.490757\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.557822\n",
      "[200]\tvalid's auc: 0.560285\n",
      "[300]\tvalid's auc: 0.561497\n",
      "Early stopping, best iteration is:\n",
      "[279]\tvalid's auc: 0.561864\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.527616\n",
      "Early stopping, best iteration is:\n",
      "[131]\tvalid's auc: 0.528906\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.490324\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.569718\n",
      "Early stopping, best iteration is:\n",
      "[121]\tvalid's auc: 0.570243\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.532019\n",
      "Early stopping, best iteration is:\n",
      "[84]\tvalid's auc: 0.532911\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.48932\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[42]\tvalid's auc: 0.570102\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.515081\n",
      "[200]\tvalid's auc: 0.521881\n",
      "Early stopping, best iteration is:\n",
      "[213]\tvalid's auc: 0.522417\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[48]\tvalid's auc: 0.490333\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.55867\n",
      "Early stopping, best iteration is:\n",
      "[127]\tvalid's auc: 0.560641\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.533025\n",
      "Early stopping, best iteration is:\n",
      "[133]\tvalid's auc: 0.53402\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.489486\n",
      "[200]\tvalid's auc: 0.491913\n",
      "Early stopping, best iteration is:\n",
      "[260]\tvalid's auc: 0.493948\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.570836\n",
      "Early stopping, best iteration is:\n",
      "[88]\tvalid's auc: 0.571344\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.530604\n",
      "Early stopping, best iteration is:\n",
      "[105]\tvalid's auc: 0.530802\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.492105\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.569022\n",
      "Early stopping, best iteration is:\n",
      "[109]\tvalid's auc: 0.569663\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.5313\n",
      "Early stopping, best iteration is:\n",
      "[82]\tvalid's auc: 0.532356\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.494737\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.571311\n",
      "Early stopping, best iteration is:\n",
      "[96]\tvalid's auc: 0.571834\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.516117\n",
      "[200]\tvalid's auc: 0.520755\n",
      "[300]\tvalid's auc: 0.522655\n",
      "Early stopping, best iteration is:\n",
      "[353]\tvalid's auc: 0.523575\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[32]\tvalid's auc: 0.48986\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.556608\n",
      "[200]\tvalid's auc: 0.559844\n",
      "Early stopping, best iteration is:\n",
      "[253]\tvalid's auc: 0.560603\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.5301\n",
      "Early stopping, best iteration is:\n",
      "[93]\tvalid's auc: 0.530465\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.491967\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.570919\n",
      "Early stopping, best iteration is:\n",
      "[102]\tvalid's auc: 0.571039\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.530095\n",
      "Early stopping, best iteration is:\n",
      "[162]\tvalid's auc: 0.534699\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.489516\n",
      "Early stopping, best iteration is:\n",
      "[90]\tvalid's auc: 0.490549\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[60]\tvalid's auc: 0.569108\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.53324\n",
      "Early stopping, best iteration is:\n",
      "[146]\tvalid's auc: 0.534311\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid's auc: 0.490629\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.569112\n",
      "Early stopping, best iteration is:\n",
      "[80]\tvalid's auc: 0.570714\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.53173\n",
      "Early stopping, best iteration is:\n",
      "[133]\tvalid's auc: 0.532842\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid's auc: 0.490561\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.569613\n",
      "Early stopping, best iteration is:\n",
      "[80]\tvalid's auc: 0.570255\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.53224\n",
      "Early stopping, best iteration is:\n",
      "[109]\tvalid's auc: 0.53284\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.491567\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.570082\n",
      "Early stopping, best iteration is:\n",
      "[70]\tvalid's auc: 0.570843\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.52973\n",
      "[200]\tvalid's auc: 0.533476\n",
      "Early stopping, best iteration is:\n",
      "[181]\tvalid's auc: 0.533873\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid's auc: 0.490764\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.570119\n",
      "[200]\tvalid's auc: 0.570532\n",
      "Early stopping, best iteration is:\n",
      "[182]\tvalid's auc: 0.571343\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.533309\n",
      "Early stopping, best iteration is:\n",
      "[79]\tvalid's auc: 0.534664\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.491615\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[68]\tvalid's auc: 0.569341\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.53094\n",
      "[200]\tvalid's auc: 0.534242\n",
      "Early stopping, best iteration is:\n",
      "[213]\tvalid's auc: 0.534838\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.486628\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.571719\n",
      "[200]\tvalid's auc: 0.573138\n",
      "Early stopping, best iteration is:\n",
      "[170]\tvalid's auc: 0.573402\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.526945\n",
      "[200]\tvalid's auc: 0.530846\n",
      "Early stopping, best iteration is:\n",
      "[262]\tvalid's auc: 0.532191\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[48]\tvalid's auc: 0.489824\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.567534\n",
      "[200]\tvalid's auc: 0.570387\n",
      "Early stopping, best iteration is:\n",
      "[178]\tvalid's auc: 0.57071\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[62]\tvalid's auc: 0.531571\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.494968\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.572106\n",
      "Early stopping, best iteration is:\n",
      "[85]\tvalid's auc: 0.572883\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.529535\n",
      "Early stopping, best iteration is:\n",
      "[107]\tvalid's auc: 0.529671\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid's auc: 0.490965\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.570692\n",
      "Early stopping, best iteration is:\n",
      "[102]\tvalid's auc: 0.5707\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.530894\n",
      "[200]\tvalid's auc: 0.534285\n",
      "Early stopping, best iteration is:\n",
      "[230]\tvalid's auc: 0.535062\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.489156\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.570532\n",
      "Early stopping, best iteration is:\n",
      "[89]\tvalid's auc: 0.570863\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.532841\n",
      "Early stopping, best iteration is:\n",
      "[111]\tvalid's auc: 0.53362\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.490022\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.571163\n",
      "Early stopping, best iteration is:\n",
      "[80]\tvalid's auc: 0.571604\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.527621\n",
      "[200]\tvalid's auc: 0.531886\n",
      "Early stopping, best iteration is:\n",
      "[196]\tvalid's auc: 0.532431\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.491068\n",
      "Early stopping, best iteration is:\n",
      "[79]\tvalid's auc: 0.492139\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.567409\n",
      "[200]\tvalid's auc: 0.570703\n",
      "Early stopping, best iteration is:\n",
      "[179]\tvalid's auc: 0.571064\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.52902\n",
      "[200]\tvalid's auc: 0.533091\n",
      "Early stopping, best iteration is:\n",
      "[242]\tvalid's auc: 0.533798\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.491999\n",
      "Early stopping, best iteration is:\n",
      "[135]\tvalid's auc: 0.493281\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.568615\n",
      "[200]\tvalid's auc: 0.571838\n",
      "Early stopping, best iteration is:\n",
      "[201]\tvalid's auc: 0.571907\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.528399\n",
      "Early stopping, best iteration is:\n",
      "[121]\tvalid's auc: 0.529175\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.48896\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.569607\n",
      "Early stopping, best iteration is:\n",
      "[85]\tvalid's auc: 0.570365\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.515997\n",
      "[200]\tvalid's auc: 0.520206\n",
      "[300]\tvalid's auc: 0.522804\n",
      "Early stopping, best iteration is:\n",
      "[303]\tvalid's auc: 0.523266\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[49]\tvalid's auc: 0.489011\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.557627\n",
      "[200]\tvalid's auc: 0.559939\n",
      "Early stopping, best iteration is:\n",
      "[197]\tvalid's auc: 0.560222\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.528908\n",
      "Early stopping, best iteration is:\n",
      "[130]\tvalid's auc: 0.529961\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.488104\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid's auc: 0.488566\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.568571\n",
      "Early stopping, best iteration is:\n",
      "[124]\tvalid's auc: 0.570075\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.531684\n",
      "Early stopping, best iteration is:\n",
      "[113]\tvalid's auc: 0.532138\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[53]\tvalid's auc: 0.489386\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.571492\n",
      "Early stopping, best iteration is:\n",
      "[150]\tvalid's auc: 0.572707\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.51447\n",
      "[200]\tvalid's auc: 0.520581\n",
      "[300]\tvalid's auc: 0.522214\n",
      "[400]\tvalid's auc: 0.522781\n",
      "Early stopping, best iteration is:\n",
      "[393]\tvalid's auc: 0.523136\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[56]\tvalid's auc: 0.489605\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.55706\n",
      "[200]\tvalid's auc: 0.559987\n",
      "[300]\tvalid's auc: 0.560984\n",
      "Early stopping, best iteration is:\n",
      "[281]\tvalid's auc: 0.561295\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.527759\n",
      "Early stopping, best iteration is:\n",
      "[152]\tvalid's auc: 0.529486\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.490766\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.567934\n",
      "Early stopping, best iteration is:\n",
      "[152]\tvalid's auc: 0.569214\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.533217\n",
      "Early stopping, best iteration is:\n",
      "[164]\tvalid's auc: 0.533951\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.490615\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.570893\n",
      "Early stopping, best iteration is:\n",
      "[81]\tvalid's auc: 0.57207\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.533711\n",
      "Early stopping, best iteration is:\n",
      "[95]\tvalid's auc: 0.534184\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.490031\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.570128\n",
      "Early stopping, best iteration is:\n",
      "[130]\tvalid's auc: 0.570765\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.52735\n",
      "Early stopping, best iteration is:\n",
      "[106]\tvalid's auc: 0.527964\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalid's auc: 0.492076\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.569689\n",
      "Early stopping, best iteration is:\n",
      "[113]\tvalid's auc: 0.570037\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.515727\n",
      "[200]\tvalid's auc: 0.520559\n",
      "Early stopping, best iteration is:\n",
      "[237]\tvalid's auc: 0.521925\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.488726\n",
      "[200]\tvalid's auc: 0.490176\n",
      "Early stopping, best iteration is:\n",
      "[228]\tvalid's auc: 0.490369\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.557107\n",
      "Early stopping, best iteration is:\n",
      "[156]\tvalid's auc: 0.559702\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.531704\n",
      "Early stopping, best iteration is:\n",
      "[132]\tvalid's auc: 0.53321\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.488759\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.571018\n",
      "Early stopping, best iteration is:\n",
      "[83]\tvalid's auc: 0.571403\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.531446\n",
      "Early stopping, best iteration is:\n",
      "[129]\tvalid's auc: 0.53281\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.492536\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.569026\n",
      "Early stopping, best iteration is:\n",
      "[86]\tvalid's auc: 0.569932\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.530014\n",
      "Early stopping, best iteration is:\n",
      "[115]\tvalid's auc: 0.530759\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.493314\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.569393\n",
      "Early stopping, best iteration is:\n",
      "[119]\tvalid's auc: 0.569586\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.530741\n",
      "Early stopping, best iteration is:\n",
      "[132]\tvalid's auc: 0.532125\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.487954\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.570104\n",
      "Early stopping, best iteration is:\n",
      "[122]\tvalid's auc: 0.571308\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.532214\n",
      "Early stopping, best iteration is:\n",
      "[118]\tvalid's auc: 0.533053\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.49458\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid's auc: 0.570854\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.534191\n",
      "Early stopping, best iteration is:\n",
      "[83]\tvalid's auc: 0.534789\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid's auc: 0.489506\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.570674\n",
      "Early stopping, best iteration is:\n",
      "[71]\tvalid's auc: 0.572544\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.531459\n",
      "Early stopping, best iteration is:\n",
      "[122]\tvalid's auc: 0.5329\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.489391\n",
      "[200]\tvalid's auc: 0.490771\n",
      "Early stopping, best iteration is:\n",
      "[232]\tvalid's auc: 0.492029\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[63]\tvalid's auc: 0.5707\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.532874\n",
      "Early stopping, best iteration is:\n",
      "[95]\tvalid's auc: 0.533006\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.48871\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.570696\n",
      "Early stopping, best iteration is:\n",
      "[85]\tvalid's auc: 0.5711\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.531991\n",
      "Early stopping, best iteration is:\n",
      "[106]\tvalid's auc: 0.533141\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.490472\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.571392\n",
      "Early stopping, best iteration is:\n",
      "[112]\tvalid's auc: 0.571868\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.5334\n",
      "Early stopping, best iteration is:\n",
      "[110]\tvalid's auc: 0.533906\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[47]\tvalid's auc: 0.489311\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.570863\n",
      "Early stopping, best iteration is:\n",
      "[109]\tvalid's auc: 0.571129\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.531783\n",
      "[200]\tvalid's auc: 0.532985\n",
      "Early stopping, best iteration is:\n",
      "[183]\tvalid's auc: 0.533465\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalid's auc: 0.490524\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.570071\n",
      "Early stopping, best iteration is:\n",
      "[78]\tvalid's auc: 0.570703\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.535324\n",
      "[200]\tvalid's auc: 0.537773\n",
      "Early stopping, best iteration is:\n",
      "[221]\tvalid's auc: 0.537927\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid's auc: 0.490911\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.570443\n",
      "Early stopping, best iteration is:\n",
      "[76]\tvalid's auc: 0.571578\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.529778\n",
      "Early stopping, best iteration is:\n",
      "[169]\tvalid's auc: 0.533038\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[21]\tvalid's auc: 0.48876\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.569418\n",
      "[200]\tvalid's auc: 0.571161\n",
      "Early stopping, best iteration is:\n",
      "[222]\tvalid's auc: 0.571991\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.533222\n",
      "Early stopping, best iteration is:\n",
      "[81]\tvalid's auc: 0.533835\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.488699\n",
      "Early stopping, best iteration is:\n",
      "[78]\tvalid's auc: 0.48936\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.571331\n",
      "Early stopping, best iteration is:\n",
      "[131]\tvalid's auc: 0.572197\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.530023\n",
      "[200]\tvalid's auc: 0.533908\n",
      "Early stopping, best iteration is:\n",
      "[206]\tvalid's auc: 0.534036\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[49]\tvalid's auc: 0.488073\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[57]\tvalid's auc: 0.570038\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.532918\n",
      "Early stopping, best iteration is:\n",
      "[122]\tvalid's auc: 0.533873\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[62]\tvalid's auc: 0.489188\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.569755\n",
      "Early stopping, best iteration is:\n",
      "[81]\tvalid's auc: 0.570937\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.531552\n",
      "[200]\tvalid's auc: 0.533449\n",
      "[300]\tvalid's auc: 0.533756\n",
      "Early stopping, best iteration is:\n",
      "[272]\tvalid's auc: 0.534356\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.486491\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.569731\n",
      "[200]\tvalid's auc: 0.571464\n",
      "Early stopping, best iteration is:\n",
      "[170]\tvalid's auc: 0.572315\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=LGBMClassifier(metric='None', n_estimators=5000,\n",
       "                                            n_jobs=4, random_state=314),\n",
       "                   n_iter=100,\n",
       "                   param_distributions={'colsample_bytree': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001899AB13A48>,\n",
       "                                        'min_child_samples': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001899AB25E08>,\n",
       "                                        'min_child_weight': [1e-05, 0.001, 0.01,\n",
       "                                                             0.1, 1, 10.0,\n",
       "                                                             100.0, 1000.0,\n",
       "                                                             10000.0],\n",
       "                                        'num_leaves': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001899AB25B08>,\n",
       "                                        'reg_alpha': [0, 0.1, 1, 2, 5, 7, 10,\n",
       "                                                      50, 100],\n",
       "                                        'reg_lambda': [0, 0.1, 1, 5, 10, 20, 50,\n",
       "                                                       100],\n",
       "                                        'subsample': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001899AB25BC8>},\n",
       "                   random_state=314, scoring='roc_auc', verbose=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(*train, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(colsample_bytree=0.7756038066515227, metric='None',\n",
       "               min_child_samples=424, min_child_weight=10000.0,\n",
       "               n_estimators=5000, n_jobs=4, num_leaves=30, random_state=314,\n",
       "               reg_alpha=0, reg_lambda=100, subsample=0.9348658101450167)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#gs.best_estimator_\n",
    "#LGBMClassifier(colsample_bytree=0.6433117836032942, metric='None',\n",
    "#               min_child_samples=224, min_child_weight=1e-05, n_estimators=5000,\n",
    "#               n_jobs=4, num_leaves=20, random_state=314, reg_alpha=10,\n",
    "#               reg_lambda=10, subsample=0.8945613420997809)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.511248\n",
      "[200]\tvalid's auc: 0.515878\n",
      "[300]\tvalid's auc: 0.517861\n",
      "[400]\tvalid's auc: 0.518498\n",
      "[500]\tvalid's auc: 0.519077\n",
      "Early stopping, best iteration is:\n",
      "[487]\tvalid's auc: 0.519322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.022175233657982574"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf = lgb.LGBMRegressor(colsample_bytree=0.7756038066515227, metric='None',\n",
    "               min_child_samples=424, min_child_weight=10000.0,\n",
    "               n_estimators=5000, n_jobs=4, num_leaves=30, random_state=314,\n",
    "               reg_alpha=0, reg_lambda=100, subsample=0.9348658101450167)\n",
    "\n",
    "cf.fit(dataset_train[feature_names],dataset_train['return'] > 1, **fit_params)\n",
    "cf.score(dataset_test[feature_names],dataset_test['return'] > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 14, 11, 22, 11, 22, 18,  5,  8, 40,  9,  0, 39, 20, 24, 16, 22,\n",
       "        3, 25,  9,  2, 35,  0, 37, 16, 11,  3, 26,  8,  0, 19])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Value', ylabel='Feature'>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_imp = pd.DataFrame(zip(cf.feature_importances_, feature_names), \n",
    "                           columns=['Value','Feature']).sort_values('Value', ascending=False)\n",
    "feature_imp\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Training until validation scores don't improve for 30 rounds.\n",
    "[100]\tvalid's auc: 0.551389\n",
    "Early stopping, best iteration is:\n",
    "[89]\tvalid's auc: 0.553699\n",
    "0.003857956228475623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(cf.fit(*train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import lightgbm as lgb\n",
    "#gbm = lgb.LGBMClassifier(n_estimators=100, random_state=5, learning_rate=0.01)\n",
    "#gbm.fit(dataset_train[feature_names], dataset_train['return'] > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tqdm\n",
    "#\n",
    "#n = 3\n",
    "#\n",
    "#X = []\n",
    "#y = []\n",
    "#indexes = []\n",
    "#dataset_scaled_x = dataset_scaled[feature_names]\n",
    "#\n",
    "#for i in tqdm.tqdm_notebook(range(0, len(dataset_scaled)-n)):\n",
    "#    X.append(dataset_scaled_x.iloc[i:i+n].values)\n",
    "#    y.append(dataset_scaled['return'].iloc[i+n-1])\n",
    "#    indexes.append(dataset_scaled.index[i+n-1])\n",
    "##dataset_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#X = np.array(X)\n",
    "#y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import lightgbm as lgb\n",
    "#cf = lgb.LGBMRegressor(colsample_bytree=0.7740467183023685, metric='None',\n",
    "#               min_child_samples=395, min_child_weight=0.01, n_estimators=5000,\n",
    "#               n_jobs=4, num_leaves=9, random_state=314, reg_alpha=5,\n",
    "#               reg_lambda=10, subsample=0.4643892520208455)\n",
    "#    \n",
    "#cf.fit(dataset_train[feature_names].astype(float), dataset_train['rank'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "cf2 = RandomForestRegressor(n_estimators=100)\n",
    "cf2.fit(dataset_train[feature_names].astype(float), dataset_train[predi_target])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 參數優化_1110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy.stats import randint as sp_randint \n",
    "#from sklearn.model_selection import RandomizedSearchCV \n",
    "## build a classifier \n",
    "#clf = RandomForestRegressor(n_estimators=100) \n",
    "## specify parameters and distributions to sample from \n",
    "#param_dist = {\"max_depth\": [3, None], \n",
    "#              \"max_features\": sp_randint(1, 11), \n",
    "#              \"min_samples_split\": sp_randint(2, 11), \n",
    "#              \"min_samples_leaf\": sp_randint(1, 11), \n",
    "#              \"bootstrap\": [True, False], \n",
    "#              \"criterion\": [\"mse\", \"mae\"]} \n",
    "## run randomized search \n",
    "#n_iter_search = 20 \n",
    "#rs = RandomizedSearchCV(clf, param_distributions=param_dist, \n",
    "#                                   n_iter=n_iter_search) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rs.fit(dataset_train[features], dataset_train['return'] > 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split Train Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Value', ylabel='Feature'>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_imp = pd.DataFrame(zip(cf2.feature_importances_, feature_names), \n",
    "                           columns=['Value','Feature']).sort_values('Value', ascending=False)\n",
    "feature_imp\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = dataset.index.get_level_values('date') < '2021'\n",
    "dataset_train = dataset[select]\n",
    "dataset_test = dataset[~select]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_drop = dataset.dropna(subset=feature_names+['return'])\n",
    "\n",
    "vals = model.predict(dataset_drop[feature_names].astype(float))\n",
    "dataset_drop['result1'] = pd.Series(vals.swapaxes(0,1)[0], dataset_drop.index)\n",
    "\n",
    "vals = cf.predict(dataset_drop[feature_names].astype(float))\n",
    "dataset_drop['result2'] = pd.Series(vals, dataset_drop.index)\n",
    "\n",
    "vals = cf2.predict(dataset_drop[feature_names].astype(float))\n",
    "dataset_drop['result3'] = pd.Series(vals, dataset_drop.index)\n",
    "\n",
    "dataset_drop = dataset_drop.reset_index().set_index(\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 把量加進來做篩選\n",
    " * https://hahow.in/courses/5b9d3a6dca498a001e917383/shapeussions/60c96f5b018697e8a6131cbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把量加進來\n",
    "vol=data.get('成交股數')/1000\n",
    "vol_ma5=vol.rolling(5).mean()\n",
    "\n",
    "vol_filter=vol_ma5>1000\n",
    "vol_filter=vol_filter[vol_filter].fillna(0)\n",
    "#vol_filter\n",
    "t1 = vol_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_filter = t1.reindex(every_month, method='ffill')#.loc['2010-02-15']\n",
    "#vol_filter.loc['2010-02-15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#t1 = vol_ma5.iloc[-1].dropna()\n",
    "#t1.to_csv('./tmp/132.csv')\n",
    "#t1.hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='date'>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "dates = sorted(list(set(dataset_drop.index)))\n",
    "\n",
    "rs = []\n",
    "for d in dates:\n",
    "    #print(d)\n",
    "    dataset_time = dataset_drop.loc[d]\n",
    "    \n",
    "    dataset_time = drop_extreme_case(dataset_time , feature_names , thresh=0.01)\n",
    "    \n",
    "    #print(dataset_time)\n",
    "    \n",
    "    predi_target = dataset_time['result1'] + dataset_time['result2'] + dataset_time['result3'] \n",
    "            ###\n",
    "    predi_target = predi_target * (dataset_time['vol_ma5'] >vol).astype(float)\n",
    "            ###\n",
    "\n",
    "    condition = (predi_target >= predi_target.nlargest(20).iloc[-1]) \n",
    "    \n",
    "    #print(vol_filter.loc[d])\n",
    "    #print(condition)\n",
    "    \n",
    "    r = dataset_time['return'][condition].mean()\n",
    "\n",
    "    rs.append(r * (1-3/1000-1.425/1000*2*0.6) )#+ 0.05)\n",
    "\n",
    "rs = pd.Series(rs, index=dates)['2016':].cumprod()\n",
    "\n",
    "s0050 = close['0050']['2016':]\n",
    "\n",
    "pd.DataFrame({'nn strategy return':rs.reindex(s0050.index, method='ffill'), '0050 return':s0050/s0050[0]}).plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#\n",
    "#return_history_1026 = pd.Series(rs, index=dates)['2021':].cumprod()\n",
    "##eq = (gain[hold == 1].mean(axis=1)).fillna(1).cumprod()\n",
    "#\n",
    "#pickle.dump(rs, open('return_history_1026.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyfolio as pf\n",
    "#import pandas as pd\n",
    "#\n",
    "#close.index = close.index.tz_localize(\"Asia/Taipei\")\n",
    "##pf.create_returns_tear_sheet(close['0050'].pct_change())\n",
    "#\n",
    "## 得到 上一個單元的 回測結果\n",
    "#ret = pickle.load(open(\"return_history_1026.pkl\", \"rb\"))\n",
    "#\n",
    "## 將回測報酬率取出來\n",
    "#ret = ret.pct_change().dropna()\n",
    "#ret.index = pd.to_datetime(ret.index).tz_localize('Asia/Taipei')\n",
    "#\n",
    "## 利用pyfolio 比較報酬率\n",
    "#pf.create_returns_tear_sheet(ret, benchmark_rets=close['0050'].pct_change())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 當月持股狀況"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the latest dataset\n",
    "last_date = dataset.index.levels[1].max()#\"2022-10-15\"\n",
    "is_last_date = dataset.index.get_level_values('date') == last_date\n",
    "last_dataset = dataset[is_last_date].copy()\n",
    "\n",
    "\n",
    "last_dataset = drop_extreme_case(last_dataset, feature_names , thresh=0.01)\n",
    "t1 = last_dataset\n",
    "\n",
    "# remove NaN testcases\n",
    "last_dataset = last_dataset.dropna(subset=feature_names)\n",
    "\n",
    "# predict\n",
    "\n",
    "vals = model.predict(last_dataset[feature_names].astype(float))\n",
    "last_dataset['result1'] = pd.Series(vals.swapaxes(0,1)[0], last_dataset.index)\n",
    "\n",
    "vals = cf.predict(last_dataset[feature_names].astype(float))\n",
    "last_dataset['result2'] = pd.Series(vals, last_dataset.index)\n",
    "\n",
    "vals = cf2.predict(last_dataset[feature_names].astype(float))\n",
    "last_dataset['result3'] = pd.Series(vals, last_dataset.index)\n",
    "\n",
    "\n",
    "# calculate score\n",
    "\n",
    "predi_target = last_dataset['result1'] + last_dataset['result2'] + last_dataset['result3']\n",
    "\n",
    "#\n",
    "##predi_target = predi_target * vol_filter.iloc[-1] #******加上量的濾網\n",
    "predi_target = predi_target * (last_dataset['vol_ma5'] >vol).astype(float)\n",
    "\n",
    "\n",
    "\n",
    "condition = (predi_target >= predi_target.nlargest(20).iloc[-1])\n",
    "#vol_filter\n",
    "\n",
    "# plot rank distribution\n",
    "predi_target[predi_target!=0].hist(bins=20)\n",
    "\n",
    "\n",
    "# show the best 20 stocks\n",
    "slist1 = predi_target[condition].reset_index()['stock_id']\n",
    "\n",
    "#https://keras-cn.readthedocs.io/en/latest/models/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date = dataset.index.levels[1].max()#\"2022-10-15\"\n",
    "is_last_date = dataset.index.get_level_values('date') == last_date\n",
    "last_dataset = dataset[is_last_date].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank.sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 平均分配資產於股票之中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "股票平分張數:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "stock_id\n",
       "1419    0.072639\n",
       "1777    0.035253\n",
       "2303    0.067492\n",
       "2387    0.073260\n",
       "2546    0.058824\n",
       "3022    0.037547\n",
       "3090    0.055762\n",
       "3231    0.102916\n",
       "3501    0.057692\n",
       "3706    0.100840\n",
       "5225    0.074906\n",
       "5464    0.114286\n",
       "6142    0.294118\n",
       "6227    0.074534\n",
       "6248    0.135440\n",
       "6456    0.031381\n",
       "6579    0.033708\n",
       "6670    0.013514\n",
       "8234    0.085349\n",
       "9904    0.090909\n",
       "Name: 2022-12-14 00:00:00, dtype: float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close = data.get(\"收盤價\")\n",
    "\n",
    "money = 60000\n",
    "stock_prices = close[slist1].iloc[-1]\n",
    "\n",
    "\n",
    "print(\"股票平分張數:\")\n",
    "money / len(stock_prices) / stock_prices / 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "[移動窗格最佳化](https://hahow.in/courses/5b9d3a6dca498a001e917383/discussions/61b4c90147843d0006cf2593)\n",
    "***************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def select(df):\n",
    "\n",
    "    rank = df['pre']\n",
    "\n",
    "    condition1 = (rank >= rank.nlargest(1).iloc[-1])\n",
    "\n",
    "    return df['return'][condition1].mean() * (1-3/1000-1.425/1000*2*0.6)\n",
    "\n",
    "end = 5\n",
    "\n",
    "cf = lgb.LGBMRegressor(n_estimators=500)\n",
    "\n",
    "\n",
    "\n",
    "train_time = ['2015','2016','2017','2018','2019']\n",
    "\n",
    "s_time = ['2007','2008','2009','2010','2011']\n",
    "\n",
    "test_time = ['2016','2017','2018','2019','2020']\n",
    "\n",
    "#dataset_copy = dataset_dropna.copy()\n",
    "\n",
    "store_mse = []\n",
    "\n",
    "\n",
    "for time in range(end):\n",
    "\n",
    "    print('%d 次執行中'%(time))\n",
    "\n",
    "    dataset_dropna2_train = dataset_copy.loc[s_time[time]:train_time[time]] #2007~ 2015   2008~2016   2009~2017  2010~2018  ....\n",
    "\n",
    "    dataset_dropna2_test = dataset_copy.loc[test_time[time]:test_time[time]]#            2016                2017               2018              2019  .....\n",
    "\n",
    "    \n",
    " cf.fit(dataset_dropna2_train[feature_names].astype(float), dataset_dropna2_train['rank'])\n",
    "\n",
    "    \n",
    " predict = cf.predict(dataset_dropna2_test[feature_names])\n",
    "\n",
    "    dataset_dropna2_test['pre'] = predict\n",
    "\n",
    "dates = dataset_dropna2_test.index.get_level_values('date')\n",
    "\n",
    "b = dataset_dropna2_test.groupby(dates).apply(select).cumprod()\n",
    "\n",
    "s0050 = close['0050'][test_time[time]:test_time[time]]\n",
    "\n",
    "s0056 = close['0056'][test_time[time]:test_time[time]]\n",
    "\n",
    "pd.DataFrame({'Best 1 stocks return(include handling fee)':b.reindex(s0050.index, method='ffill'), \n",
    "\n",
    "              '0050':s0050/s0050[0],'0056':s0056/s0056[0]}).plot()\n",
    "\n",
    "plt.ylabel('return')"
   ]
  }
 ],
 "metadata": {
  "@deathbeds/ipydrawio": {
   "xml": "<mxfile host=\"17-0536659-02\" modified=\"2022-10-27T03:01:05.740Z\" agent=\"5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\" etag=\"8bODyUWCdQaexky56D9k\" version=\"20.2.8\" type=\"embed\"><diagram id=\"9nsO6hMlLNMTvIbReD-d\" name=\"第1頁\"><mxGraphModel dx=\"1458\" dy=\"721\" grid=\"1\" gridSize=\"10\" guides=\"1\" tooltips=\"1\" connect=\"1\" arrows=\"1\" fold=\"1\" page=\"1\" pageScale=\"1\" pageWidth=\"827\" pageHeight=\"1169\" math=\"0\" shadow=\"0\"><root><mxCell id=\"0\"/><mxCell id=\"1\" parent=\"0\"/><UserObject label=\"Tree Root\" treeRoot=\"1\" id=\"2\"><mxCell style=\"align=center;collapsible=0;container=1;recursiveResize=0;\" parent=\"1\" vertex=\"1\"><mxGeometry x=\"40\" y=\"40\" width=\"120\" height=\"60\" as=\"geometry\"/></mxCell></UserObject></root></mxGraphModel></diagram></mxfile>"
  },
  "kernelspec": {
   "display_name": "finlab",
   "language": "python",
   "name": "finlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
