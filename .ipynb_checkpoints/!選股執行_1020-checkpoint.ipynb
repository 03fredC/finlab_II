{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 移除不必要的警告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 移除不必要的警告\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 獲取歷史資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from finlab.data import Data\n",
    "from finlab.ml import fundamental_features\n",
    "fdf = fundamental_features()\n",
    "\n",
    "data = Data()\n",
    "\n",
    "close = data.get(\"收盤價\")\n",
    "open_ = data.get(\"開盤價\")\n",
    "high = data.get(\"最高價\")\n",
    "low = data.get(\"最低價\")\n",
    "vol = data.get(\"成交股數\")\n",
    "\n",
    "PB = data.get(\"股價淨值比\")\n",
    "pe = data.get(\"本益比\")\n",
    "\n",
    "#close = data.get_adj(\"收盤價\").round(2)\n",
    "\n",
    "#財務指標\n",
    "rev = data.get(\"當月營收\")\n",
    "l_rev = data.get(\"去年當月營收\")\n",
    "\n",
    "#t123 = data.get('土地')\n",
    "\n",
    "#bargin_i=data.get(\"投信買賣超股數\")\n",
    "#bargin_f=data.get(\"外資自營商買賣超股數\")\n",
    "#bargin_s=data.get(\"自營商買賣超股數(自行買賣)\")\n",
    "#\n",
    "\n",
    "rev.index = rev.index.shift(5, \"d\")         #每月頻率\n",
    "t123.index = t123.index.shift(5, \"d\")       #季頻率\n",
    "#周頻率"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "```\n",
    "https://www.twblogs.net/a/5d3f3173bd9eee517422735f\n",
    "W-WED\n",
    "https://docs.python.org/zh-tw/3/library/calendar.html\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MFI  = data.talib(\"MFI\")\n",
    "##MFI.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ub,mb,lb =data.talib(\"BBANDS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################　　　自己加入的　　　##############################################\n",
    "import pandas as pd\n",
    "from finlab.__init__ import talib_all_stock\n",
    "from talib import abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 營收相關"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finlab.ml import fundamental_features\n",
    "\n",
    "df1 = fundamental_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias(n):\n",
    "    return close / close.rolling(n, min_periods=1).mean()\n",
    "\n",
    "def acc(n):\n",
    "    return close.shift(n) / (close.shift(2*n) + close) * 2\n",
    "\n",
    "def rsv(n):\n",
    "    l = close.rolling(n, min_periods=1).min()\n",
    "    h = close.rolling(n, min_periods=1).max()\n",
    "    \n",
    "    return (close - l) / (h - l)\n",
    "\n",
    "def mom(n):\n",
    "    return (rev / rev.shift(1)).shift(n)\n",
    "\n",
    "def yoy(n):\n",
    "    return (rev.shift(n) / rev.shift(12+n)) -1\n",
    "\n",
    "\n",
    "features = {\n",
    "    'mom1': mom(1),\n",
    "    'mom2': mom(2),\n",
    "    'mom3': mom(3),\n",
    "    'mom4': mom(4),\n",
    "    'mom5': mom(5),\n",
    "    'mom6': mom(6),\n",
    "    'mom7': mom(7),\n",
    "    'mom8': mom(8),\n",
    "    'mom9': mom(9),\n",
    "    \n",
    "    'bias5': bias(5),\n",
    "    'bias10': bias(10),\n",
    "    'bias20': bias(20),\n",
    "    'bias60': bias(60),\n",
    "    'bias120': bias(120),\n",
    "    'bias240': bias(240),\n",
    "    \n",
    "    'acc5': acc(5),\n",
    "    'acc10': acc(10),\n",
    "    'acc20': acc(20),\n",
    "    'acc60': acc(60),\n",
    "    'acc120': acc(120),\n",
    "    'acc240': acc(240),\n",
    "    \n",
    "    'rsv5': rsv(5),\n",
    "    'rsv10': rsv(10),\n",
    "    'rsv20': rsv(20),\n",
    "    'rsv60': rsv(60),\n",
    "    'rsv120': rsv(120),\n",
    "    'rsv240': rsv(240),\n",
    "###############################################\n",
    "    \n",
    "    'yoy': yoy(1),\n",
    "    'delta_yoy':(yoy(1)/yoy(2))-1,\n",
    "    \n",
    "    'PB':PB,\n",
    "    'PE':pe,       \n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stock_id  date      \n",
       "1101      2005-02-15         NaN\n",
       "          2005-03-15         NaN\n",
       "          2005-04-15    0.570159\n",
       "          2005-05-15    1.599304\n",
       "          2005-06-15    0.965557\n",
       "                          ...   \n",
       "9962      2022-06-15    0.678559\n",
       "          2022-07-15    0.917472\n",
       "          2022-08-15    1.694666\n",
       "          2022-09-15    0.641536\n",
       "          2022-10-15    1.280940\n",
       "Length: 427065, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mom(1).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 財報指標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finlab.ml import fundamental_features\n",
    "F_dataset = fundamental_features()\n",
    "F_dataset = F_dataset.dropna(thresh=int(len(F_dataset)*0.5), axis=1).dropna(how='any')\n",
    "F_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 技術指標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://zhuanlan.zhihu.com/p/342075180 talib函数功能一览表\n",
    "\n",
    "def bias(n):\n",
    "    return close / close.rolling(n, min_periods=1).mean()\n",
    "\n",
    "def acc(n):\n",
    "    return close.shift(n) / (close.shift(2*n) + close) * 2\n",
    "\n",
    "def rsv(n):\n",
    "    l = close.rolling(n, min_periods=1).min()\n",
    "    h = close.rolling(n, min_periods=1).max()\n",
    "    \n",
    "    return (close - l) / (h - l)\n",
    "\n",
    "def mom(n):\n",
    "    return (rev / rev.shift(1)).shift(n)\n",
    "\n",
    "\n",
    "def bi_(n):\n",
    "    return (bargin_i / vol.shift(1)).shift(n)\n",
    "\n",
    "def bf(n):\n",
    "    return (bargin_f / vol.shift(1)).shift(n)\n",
    "    \n",
    "def bs(n):\n",
    "    return (bargin_s / vol.shift(1)).shift(n)\n",
    "\n",
    "def rsi(n):\n",
    "    #return talib_all_stock(ndays=10000, func=abstract.RSI, timeperiod=n)\n",
    "    return data.talib(\"RSI\",timeperiod=n)\n",
    "\n",
    "def MFI(n):\n",
    "    return data.talib(\"MFI\",timeperiod=n)\n",
    "\n",
    "def obv(n):\n",
    "    return data.talib(\"OBV\",timeperiod=n)\n",
    "\n",
    "\n",
    "\n",
    "features = {\n",
    "    \n",
    "#    'ATR14':data.talib(\"ATR\",timeperiod=14),\n",
    "#    'NATR14':data.talib('NATR',timeperiod=14),\n",
    "#    'TRANGE':data.talib('TRANGE'),\n",
    "#    'Adosc3':data.talib('ADOSC',timeperiod=3),\n",
    "    \n",
    "#    \"MFI5\":MFI(5),\n",
    "#    \"MFI10\":MFI(10),\n",
    "\n",
    "#    'rsi6': rsi(6),  #DataFrame\n",
    "#    'rsi10': rsi(10),  #DataFrame\n",
    "#    'rsi14': rsi(14),  #DataFrame\n",
    "#    'rsi20': rsi(20),  #DataFrame\n",
    "#    'rsi50': rsi(50),  #DataFrame\n",
    "#   \n",
    "    'mom1': mom(1),\n",
    "    'mom2': mom(2),\n",
    "    'mom3': mom(3),\n",
    "    'mom4': mom(4),\n",
    "    'mom5': mom(5),\n",
    "    'mom6': mom(6),\n",
    "    'mom7': mom(7),\n",
    "    'mom8': mom(8),\n",
    "    'mom9': mom(9),\n",
    "    \n",
    "#    'yoy': yoy(1),\n",
    "#    'delta_yoy':(yoy(1)/yoy(2))-1,\n",
    "    \n",
    "#    'ff':ff,\n",
    "    'PB':PB,\n",
    "    'PE':pe,   \n",
    "#  \n",
    "    'bias5': bias(5),\n",
    "    'bias10': bias(10),\n",
    "    'bias20': bias(20),\n",
    "    'bias60': bias(60),\n",
    "    'bias120': bias(120),\n",
    "    'bias240': bias(240),\n",
    "    \n",
    "    'acc5': acc(5),\n",
    "    'acc10': acc(10),\n",
    "    'acc20': acc(20),\n",
    "    'acc60': acc(60),\n",
    "    'acc120': acc(120),\n",
    "    'acc240': acc(240),\n",
    "    \n",
    "    'rsv5': rsv(5),\n",
    "    'rsv10': rsv(10),\n",
    "    'rsv20': rsv(20),\n",
    "    'rsv60': rsv(60),\n",
    "    'rsv120': rsv(120),\n",
    "    'rsv240': rsv(240),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認各指標清單"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PB',\n",
       " 'PE',\n",
       " 'acc10',\n",
       " 'acc120',\n",
       " 'acc20',\n",
       " 'acc240',\n",
       " 'acc5',\n",
       " 'acc60',\n",
       " 'bias10',\n",
       " 'bias120',\n",
       " 'bias20',\n",
       " 'bias240',\n",
       " 'bias5',\n",
       " 'bias60',\n",
       " 'mom1',\n",
       " 'mom2',\n",
       " 'mom3',\n",
       " 'mom4',\n",
       " 'mom5',\n",
       " 'mom6',\n",
       " 'mom7',\n",
       " 'mom8',\n",
       " 'mom9',\n",
       " 'rsv10',\n",
       " 'rsv120',\n",
       " 'rsv20',\n",
       " 'rsv240',\n",
       " 'rsv5',\n",
       " 'rsv60']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1=sorted(features)\n",
    "list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t1 = data.talib(\"NATR\",timeperiod=14)\n",
    "#t1.to_csv('myfile.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 製作dataset\n",
    "\n",
    "##### 設定買賣頻率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2005-02-15', '2005-03-15', '2005-04-15', '2005-05-15',\n",
       "               '2005-06-15', '2005-07-15', '2005-08-15', '2005-09-15',\n",
       "               '2005-10-15', '2005-11-15',\n",
       "               ...\n",
       "               '2022-01-15', '2022-02-15', '2022-03-15', '2022-04-15',\n",
       "               '2022-05-15', '2022-06-15', '2022-07-15', '2022-08-15',\n",
       "               '2022-09-15', '2022-10-15'],\n",
       "              dtype='datetime64[ns]', name='date', length=213, freq=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rev.index = rev.index.tz_localize(\"Asia/Taipei\")\n",
    "every_month = rev.index\n",
    "every_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 將dataframe 組裝起來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features['bias20'].reindex(every_month, method='ffill')\n",
    "\n",
    "for name, f in features.items():\n",
    "    features[name] = f.reindex(every_month, method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name, f in features.items():\n",
    "    features[name] = f.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################　　　自己加入的　　　##############################################\n",
    "dataset.index = dataset.index.set_names(['stock_id','date'], level=[0,1])\n",
    "\n",
    "\n",
    "#dataset.index.levels[1].name = 'date'\n",
    "#dataset.index.levels[0].name = 'stock_id'\n",
    "\n",
    "#因為你pandas更新到新版了\n",
    "## profit.index.levels[0].name = 'year'\n",
    "## profit.index.levels[1].name = 'month'\n",
    "#這兩行的語法被棄用，請改成\n",
    "#profit.index=profit.index.set_names('year', level=0)\n",
    "#profit.index=profit.index.set_names('month', level=1)\n",
    "#or profit.index=profit.index.set_names(['year','month'], level=[0,1])\n",
    "#直接一行\n",
    "#就可以了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 新增 label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finlab import ml\n",
    "\n",
    "ml.add_profit_prediction(dataset)\n",
    "ml.add_rank_prediction(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 刪除太大太小的歷史資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(433881, 31)\n",
      "(376660, 31)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "\n",
    "def drop_extreme_case(dataset, feature_names, thresh=0.01):\n",
    "    \n",
    "    extreme_cases = pd.Series(False, index=dataset.index)\n",
    "    for f in feature_names:\n",
    "        tf = dataset[f]\n",
    "        extreme_cases = extreme_cases | (tf < tf.quantile(thresh)) | (tf > tf.quantile(1-thresh))\n",
    "    dataset = dataset[~extreme_cases]\n",
    "    return dataset\n",
    "\n",
    "dataset_drop_extreme_case = drop_extreme_case(dataset , list1 , thresh=0.01)\n",
    "\n",
    "print(dataset_drop_extreme_case.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dropna = dataset_drop_extreme_case.dropna(how='any')\n",
    "dataset_dropna = dataset_dropna.reset_index().set_index(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2005-02-15', '2005-03-15', '2005-04-15', '2005-05-15',\n",
       "               '2005-06-15', '2005-07-15', '2005-08-15', '2005-09-15',\n",
       "               '2005-10-15', '2005-11-15',\n",
       "               ...\n",
       "               '2021-09-15', '2021-11-15', '2021-12-15', '2022-02-15',\n",
       "               '2022-03-15', '2022-04-15', '2022-06-15', '2022-08-15',\n",
       "               '2022-09-15', '2022-10-15'],\n",
       "              dtype='datetime64[ns]', name='date', length=376660, freq=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_drop_extreme_case.index.get_level_values(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################　　　自己加入的　　　##############################################\n",
    "\n",
    "dataset_dropna.index = pd.to_datetime(dataset_dropna.index)\n",
    "dataset_dropna = dataset_dropna.sort_index()\n",
    "\n",
    "#修復＜class ‘numpy.ndarray‘＞　https://blog.csdn.net/lxbin/article/details/114005757"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset_dropna[:'2020']\n",
    "dataset_test = dataset_dropna['2021':]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神經網路模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################　　　自己加入的　　　##############################################\n",
    "\n",
    "\n",
    "#pip3 install --upgrade protobuf==3.20.1 --user\n",
    "#\n",
    "#python AssertionError: ＜class ‘numpy.ndarray‘＞\n",
    "#\n",
    "#If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n",
    "#\n",
    "#https://github.com/SeldonIO/MLServer/issues/615"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Downgrade the protobuf package to 3.20.x or lower.\n",
    "##卸载protobuf\n",
    "#!pip uninstall protobuf\n",
    "##安装低版本protobuf\n",
    "#!pip install protobuf==3.19.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 100)               3000      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 13,201\n",
      "Trainable params: 13,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "start fitting\n",
      "Epoch 1/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.284 - ETA: 0s - loss: 0.274 - ETA: 0s - loss: 0.266 - ETA: 0s - loss: 0.253 - ETA: 0s - loss: 0.236 - ETA: 0s - loss: 0.216 - ETA: 0s - loss: 0.194 - ETA: 0s - loss: 0.178 - ETA: 0s - loss: 0.166 - ETA: 0s - loss: 0.157 - 1s 6ms/step - loss: 0.1552 - val_loss: 0.0713\n",
      "Epoch 2/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0736 - val_loss: 0.0714\n",
      "Epoch 3/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0736 - val_loss: 0.0715\n",
      "Epoch 4/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0714\n",
      "Epoch 5/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0736 - val_loss: 0.0713\n",
      "Epoch 6/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0714\n",
      "Epoch 7/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 8/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 9/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0712\n",
      "Epoch 10/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 11/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0712\n",
      "Epoch 12/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0712\n",
      "Epoch 13/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0714\n",
      "Epoch 14/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 15/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 16/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 17/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 18/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0734 - val_loss: 0.0713\n",
      "Epoch 19/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 20/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 21/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 22/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 23/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 24/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 25/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 26/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0713\n",
      "Epoch 27/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 1s 6ms/step - loss: 0.0733 - val_loss: 0.0712\n",
      "Epoch 28/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0712\n",
      "Epoch 29/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0712\n",
      "Epoch 30/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 31/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 32/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 33/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 34/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0712\n",
      "Epoch 35/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0734 - val_loss: 0.0713\n",
      "Epoch 36/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0712\n",
      "Epoch 37/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 38/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0711\n",
      "Epoch 39/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0712\n",
      "Epoch 40/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0712\n",
      "Epoch 41/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0714\n",
      "Epoch 42/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0712\n",
      "Epoch 43/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0712\n",
      "Epoch 44/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0713\n",
      "Epoch 45/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 46/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0712\n",
      "Epoch 47/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 48/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0715\n",
      "Epoch 49/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 50/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 51/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 52/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 53/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 54/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0712\n",
      "Epoch 55/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0713\n",
      "Epoch 56/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0712\n",
      "Epoch 57/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 58/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 59/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0713\n",
      "Epoch 60/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 1s 5ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 61/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 62/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0712\n",
      "Epoch 63/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 64/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 65/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0732 - val_loss: 0.0712\n",
      "Epoch 66/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 67/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 68/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 69/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 70/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 71/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 72/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0731 - val_loss: 0.0712\n",
      "Epoch 73/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 74/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0712\n",
      "Epoch 75/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 76/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 77/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 78/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 79/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 80/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 81/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0712\n",
      "Epoch 82/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 83/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 84/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 85/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0712\n",
      "Epoch 86/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 87/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 88/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0712\n",
      "Epoch 89/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0712\n",
      "Epoch 90/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 91/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0712\n",
      "Epoch 92/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0712\n",
      "Epoch 93/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 94/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0713\n",
      "Epoch 95/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0710\n",
      "Epoch 96/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0709\n",
      "Epoch 97/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 98/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 99/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 100/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 101/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 102/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0712\n",
      "Epoch 103/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 104/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0712\n",
      "Epoch 105/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0713\n",
      "Epoch 106/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 107/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0715\n",
      "Epoch 108/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0713\n",
      "Epoch 109/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0713\n",
      "Epoch 110/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0713\n",
      "Epoch 111/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 112/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0729 - val_loss: 0.0714\n",
      "Epoch 113/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0712\n",
      "Epoch 114/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0712\n",
      "Epoch 115/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 116/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0710\n",
      "Epoch 117/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0714\n",
      "Epoch 118/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0709\n",
      "Epoch 119/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0713\n",
      "Epoch 120/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0712\n",
      "Epoch 121/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 1s 6ms/step - loss: 0.0729 - val_loss: 0.0713\n",
      "Epoch 122/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0712\n",
      "Epoch 123/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0710\n",
      "Epoch 124/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0729 - val_loss: 0.0710\n",
      "Epoch 125/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 126/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0709\n",
      "Epoch 127/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0729 - val_loss: 0.0710\n",
      "Epoch 128/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 129/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0712\n",
      "Epoch 130/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 131/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 132/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0712\n",
      "Epoch 133/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0712\n",
      "Epoch 134/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0713\n",
      "Epoch 135/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 136/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 137/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 138/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 139/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0709\n",
      "Epoch 140/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 141/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 142/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 143/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0710\n",
      "Epoch 144/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 145/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0712\n",
      "Epoch 146/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 147/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0712\n",
      "Epoch 148/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 149/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0712\n",
      "Epoch 150/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 151/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 152/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0709\n",
      "Epoch 153/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 154/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0709\n",
      "Epoch 155/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.080 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0709\n",
      "Epoch 156/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0712\n",
      "Epoch 157/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0710\n",
      "Epoch 158/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0710\n",
      "Epoch 159/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0712\n",
      "Epoch 160/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0713\n",
      "Epoch 161/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 162/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0709\n",
      "Epoch 163/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 164/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0710\n",
      "Epoch 165/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 166/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0713\n",
      "Epoch 167/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0710\n",
      "Epoch 168/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0718\n",
      "Epoch 169/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0715\n",
      "Epoch 170/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0708\n",
      "Epoch 171/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0712\n",
      "Epoch 172/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 173/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 174/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0714\n",
      "Epoch 175/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0710\n",
      "Epoch 176/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0709\n",
      "Epoch 177/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0709\n",
      "Epoch 178/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0710\n",
      "Epoch 179/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 180/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0709\n",
      "Epoch 181/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0713\n",
      "Epoch 182/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0714\n",
      "Epoch 183/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0715\n",
      "Epoch 184/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 185/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 186/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0708\n",
      "Epoch 187/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 188/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 189/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0716\n",
      "Epoch 190/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0710\n",
      "Epoch 191/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 192/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0715\n",
      "Epoch 193/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 194/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 195/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0713\n",
      "Epoch 196/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 197/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 198/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0709\n",
      "Epoch 199/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 200/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 201/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0710\n",
      "Epoch 202/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0710\n",
      "Epoch 203/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0710\n",
      "Epoch 204/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 205/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 206/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 207/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 208/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 209/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 210/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0715\n",
      "Epoch 211/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 212/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0725 - val_loss: 0.0713\n",
      "Epoch 213/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0713\n",
      "Epoch 214/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0710\n",
      "Epoch 215/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0710\n",
      "Epoch 216/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0710\n",
      "Epoch 217/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 5ms/step - loss: 0.0723 - val_loss: 0.0709\n",
      "Epoch 218/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 219/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0715\n",
      "Epoch 220/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0725 - val_loss: 0.0710\n",
      "Epoch 221/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 222/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0724 - val_loss: 0.0710\n",
      "Epoch 223/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0714\n",
      "Epoch 224/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0710\n",
      "Epoch 225/225\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0709\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Dense(100, activation='relu',\n",
    "                      input_shape=(len(feature_names),),\n",
    "                      kernel_initializer=initializers.he_normal(seed=0)))\n",
    "model.add(layers.Dense(100, activation='relu',\n",
    "                      kernel_initializer=initializers.he_normal(seed=0)))\n",
    "model.add(layers.Dropout(0.7))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=\"adam\",)\n",
    "\n",
    "print('start fitting')\n",
    "history = model.fit(dataset_train[feature_names], dataset_train['rank'],\n",
    "                    batch_size=1000,         #1000  #每一个batch的大小\n",
    "                    epochs=225, #225          #迭代次数\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    #validation_data =        #(测试集的输入特征，测试集的标签），\n",
    "                    #validation_split =       # 从测试集中划分多少比例给训练集，\n",
    "                    #validation_freq = 20        #测试的epoch间隔数                     \n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22808b78a08>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj0AAAGdCAYAAAD5ZcJyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAACdpUlEQVR4nO2dd3gUdf7H39s3vTdIIBB6CxAgBFFBEVAUI4jIcbbDjvfzRD3FU/DOwp2I5YSzo5zlQE4PFRBFmiABpPdeAqTXTd/s7vz++O53yrbspi7J5/U8++zu9NnZmXnPp6oEQRBAEARBEATRzlG39QYQBEEQBEG0BiR6CIIgCILoEJDoIQiCIAiiQ0CihyAIgiCIDgGJHoIgCIIgOgQkegiCIAiC6BCQ6CEIgiAIokNAoocgCIIgiA6Btq03wJ+w2WzIyclBSEgIVCpVW28OQRAEQRBeIAgCKioq0KlTJ6jV7u05JHpk5OTkICkpqa03gyAIgiCIRnDx4kUkJia6HU+iR0ZISAgA9qOFhoa28dYQBEEQBOENJpMJSUlJ4n3cHSR6ZHCXVmhoKIkegiAIgrjCaCg0hQKZCYIgCILoEJDoIQiCIAiiQ0CihyAIgiCIDgGJHoIgCIIgOgQkegiCIAiC6BCQ6CEIgiAIokNAoocgCIIgiA4BiR6CIAiCIDoEJHoIgiAIgugQkOghCIIgCKJDQKKHIAiCIIgOAYkegiAIgiA6BCR6WoPTPwOrnwBO/tjWW0IQBEEQHRYSPa3BmU3A7qUkegiCIAiiDSHR0xokjWDvl3a17XYQBEEQRAeGRE9rkDicvecfAcxVbbstBEEQBNFBIdHTGoR2AkITAcEGXN7b1ltDEARBEB0SEj2tReIw9k4uLoIgCIJoE0j0tBZiXM/utt0OgiAIguigkOhpLXhcz8VdgCC07bYQBEEQRAeERE9rkZAKaPRAdRFQer6tt4YgCIIgOhwkeloLrQGIH8Q+X/qtbbeFIAiCIDogJHpaEx7Xc26L/X0r8I9kYN/nbbZJBEEQBNFRINHTmvS5mb0fWQXUVQKbFwA1pcCO99p0swiCIAiiI0CipzXpOgqITAHMdsFz4Vc2PP8QUH6pbbeNIAiCINo5JHpaE5UKGPJ79jlrsXIc9eUiCIIgiBalUaJnyZIlSE5OhtFoRHp6Onbt8lxwb+XKlejTpw+MRiMGDhyItWvXKsarVCqXr4ULF4rTTJ48GV26dIHRaERCQgLuuusu5OTkiOPPnz/vchk7duxozC62HIN/B6g00veU69m7o+ix1LXeNhEEQRBEB8Bn0bNixQrMmTMH8+fPx969e5GamooJEyagoKDA5fTbt2/HjBkzMGvWLOzbtw+ZmZnIzMzE4cOHxWlyc3MVr6VLl0KlUmHq1KniNGPHjsVXX32FEydO4Ouvv8aZM2dw++23O63v559/ViwrLS3N111sWULigZ7j2eeYPsD4l9jnc1sAczX7vOtD4NXOwIa/tc02EgRBEEQ7RCUIvlXKS09Px/Dhw7F4MXPP2Gw2JCUl4Y9//COeffZZp+mnT5+OqqoqrF69Whw2cuRIDB48GO+95zqANzMzExUVFdiwYYPb7fjuu++QmZmJuro66HQ6nD9/Ht26dcO+ffswePBgX3ZJxGQyISwsDOXl5QgNDW3UMrwi9wDw3f8B170A9LgeeGsQUJ4NTFjA6vhsXcSm0wcDTx4HDCEtty0EQRAEcYXj7f3bJ0uP2WzGnj17MG7cOGkBajXGjRuHrKwsl/NkZWUppgeACRMmuJ0+Pz8fa9aswaxZs9xuR0lJCb744guMGjUKOp1OMW7y5MmIjY3F6NGj8d1333ncn7q6OphMJsWrVUhIBR7aAvQcx+J8ek1gw3+cqxQ85krg0MrW2SaCIAiCaOf4JHqKiopgtVoRFxenGB4XF4e8vDyX8+Tl5fk0/bJlyxASEoIpU6Y4jXvmmWcQFBSEqKgoZGdn49tvvxXHBQcHY9GiRVi5ciXWrFmD0aNHIzMz06PwWbBgAcLCwsRXUlKS22lblBEPAp2GAFE9gLgBwOR3gDFz2bjdnwAFx4H//gE4+VPbbB9BEARBtAO0bb0BjixduhQzZ86E0Wh0Gvf0009j1qxZuHDhAv7617/i7rvvxurVq6FSqRAdHY05c+aI0w4fPhw5OTlYuHAhJk+e7HJdc+fOVcxjMpnaRvjE9AIe3KwcVl3CYnryDgIfjgXqq4GzW4AnDgO6gNbfRoIgCIK4wvFJ9ERHR0Oj0SA/P18xPD8/H/Hx8S7niY+P93r6rVu34sSJE1ixYoXb9UdHR6NXr17o27cvkpKSsGPHDmRkZLicPj09HevXr3e7PwaDAQaDwe34NiUwEuh3K3DoKyZ4ABbvc2A5MOy+tt02giAIgrgC8cm9pdfrkZaWpggwttls2LBhg1vhkZGR4RSQvH79epfTf/zxx0hLS0NqamqD22Kz2QCwuBx37N+/HwkJCQ0uy2+56nEgKAYYeg8w7kU2LGsxYLMBFjN1aycIgiAIH/DZvTVnzhzcc889GDZsGEaMGIG33noLVVVVuO8+Zn24++670blzZyxYsAAA8Pjjj+Paa6/FokWLMGnSJCxfvhy7d+/GBx98oFiuyWTCypUrsWjRIqd17ty5E7/99htGjx6NiIgInDlzBi+88AJSUlJE8bRs2TLo9XoMGTIEAPDNN99g6dKl+Oijj3zdRf8hfgDw9Gn2ua4C2PomUHwa+PQm4NJuoMtI4Lb3gbDObbudBEEQBHEF4LPomT59OgoLCzFv3jzk5eVh8ODBWLdunRisnJ2dDbVaMiCNGjUKX375JZ5//nk899xz6NmzJ1atWoUBAwYolrt8+XIIgoAZM2Y4rTMwMBDffPMN5s+fj6qqKiQkJGDixIl4/vnnFe6pl156CRcuXIBWq0WfPn2wYsUKl7V8rkgMIcCwe4Ff3way7Zlv57cC740GRj4CxPYFul0DGMPadDMJgiAIwl/xuU5Pe6bV6vQ0luoSYO3TrMBhynXAhr+ymj+czsOA+39mafAEQRAE0UHw9v7td9lbhAcCI4HbP5a+J48G9nwKXN4LHP0WuLwbOLMB6DHO7SIIgiAIoqNCDUevZLQGIP0hYMr7wLA/sGG/LAJsVuDwN0DuQWlaczVQkQ/U13pe5umfgbObW2yTCYIgCKKtIEtPe2HUY8CuD4Ds7cD71wL5hwC1Drh1CUt5//EvQH0Vm7bvZGD6Z87LKDkLfDENgAr44x4gslur7gJBEARBtCRk6WkvhHZiHdwBJnhUasBWD/zvQWD1nyTBAwDHvgNKzzsvY++/AcEGCFZg+zvKcb997DyMIAiCIK4gSPS0J65+EgjtDHQZBTy2Gxj1RzZcowcmvAq8UAx0vYoNO/EDez/yP9bewloP7PtCWta+z4HKAva56DSwZg7w0/NA0anW2x+CIAiCaEbIvdWeiOgKPHFEyt4a/zLQ52YgOBaI7M6G9ZkEXPgVOL4G6JwGrLyXDR84DagqAIJigfAk4PIeYMe/WFHEPZ9I67iwHYju2Zp7RRAEQRDNAll62huO6epdRkqCBwB638jeL2wHfnpBGs67uQ+ZySxGALDrI6DgGLD/S2m6C9tdr9dmBeprmrbtBEEQBNGCkOjpaER2B2L6sridizsAlYYFNnOG3g30uhFISgfMFcDH44GaEjYdwAKlXfH5FGBhT+Yuq68Btr3Jgqct7tuEEARBEERrQu6tjkifm4DCY+xz6p3A5HeAbW8AgVGSVWjaMuCDa4FKe7PYjEeBrCVAWTZQflnZ+qK6REpzX3kv6xdWVci+h3cF0h9sjb0iCIIgCI+Qpacj0mcSe1dpmCtLrQGueVqq9QMAoQnAHZ+xtHeNARj5KBA/iI3jbTA4l/eyd62RvVcVArog9vnXtyRrj83GKkjLXWSlF4BNC4DVTwA/PAPUmpp1VwmCIAiCQ5aejkinocCNrwFB0UBUivvpuqQDD2xkrrDQTkDXUUDufiZaBsp6ml36jb33ywQGTQMKTzIL0rujANNlYNeHQEUusP8LoKaUTXvT60D/24ClE9g4jloLTHilufeYIAiCIKj3lhy/773V1hz9DvjqLua+MoYBugDgvh+Ar+4GzmxkQmbEA9L0Wf8CfpyrXIbWCFhqmbiJHwjk7GMutZTrgd8+ZOn1j/0GRCS36q4RBEEQVy7e3r/JvUV4T5cM9l5VCBSfBvIOAQe/Ai7tYcMThyunT7sXCIxmn8O7AjOWA3MvMQuPzcIEjzYAmP4FMOl1oPtYwGoGNr7cartEEARBdBxI9BDeExzDYoB6jAP6T2HDtrwG1JUz8RI3QDm9PhC4exULlH50B0uX1+iAyYuB2H5smpsWAnH2zzf8lb0fWgkcXNkqu0QQBEF0HMi9JYPcWz5QVQy80YdZZgBW6fm+td7PX1/LMsFieimHf/dH1g4DAIb8Hpj0BmusShAEQRBuIPcW0bIERbFqz5zEYb7NrzM6Cx4AmPQmcO0zAFSsFcbmvzdpMwmCIAiCQ6KHaDxp90qfE0c0zzI1WmDsc8DtH7Pv299hvb8IgiAIoomQ6CEaT/LVQOdhLFi566jmXXb/KSx2yFYPrHsGIC8sQRAE0URI9BCNR60G7l0D/OkQEBjZvMtWqVgtIY0eOP0zcOx7Nry2nFWGXnkfsPRGIP+Icr5TPwOv95KmJwiCIAg7JHqIpqEzsiytliAqBRj1f+zz2qcAUy7w71uBH58DjnzD+oBtdChkuGMJa53xwzMsWJogCIIg7JDoIfyba54GonoyIfNuBqvtExAJjJ7Dxp/8ASi7yD7XmoBzW9ln02Vgz6dtsskEQRCEf0Kih/BvdEbg1sUAVKyFhVoLTP8MGDefxRQJNmDvMjbt6Z9ZDBDvCL91EWCuYp/PbQUW9mA9vupr2mRXCIIgiLaFRA/h/3QZCVz1OKBSAze/CSSPZsOHz2Lve5YBFjNw4gf2Pf0hVgG6qoD1/QKATa+yStK7lwIfXgeUnGv9/SAIgiDaFBI9xJXBuBeBZ7OBoXdLw/rcDATHMXHz61vAqZ/Y8H63AmOeZZ9/fQvI3sHif9RaICgWKDjKiiASBEEQHQoSPcSVgUoFGEKUwzQ6IGM2+7zpFaC2jKXPJw4HBk0Honsxl9h/ZrBp+twM3L8eUOuA81uB7J3K5VFaPEEQRLuGRA9xZTPq/4DxrzArDgD0mgCoNew1xt7hvaaEvQ+/n3VvT72Tfd/6urSc7B0s5mf5TCkwmiAIgmhXUO8tGdR76womeyew798sqysqhQ2z2YD3rwbyDwPRvYHZO5nFqPgMsHgYC4J+6Bc27t1RQMkZNp8uCEgZC4QlMndaXP+22y+CIAiiQby9f5PokUGipx2SvQP4/nFg3F+B3hOl4f+dBRz+Lwt47pIBHFwOBMcDkd2A7CxpuohuwGO/MVcaQRAE4Zd4e//WtuI2EUTr02Uks/A4cv0LwKVdQNkF9gKAmxayuJ9zm5k1aPPfgdJzwP4vgbR7vFtffS0TUDF9gS7pzbYbBEEQRNOhmB6iYxKRDDy0Feg7mX3vOxnoewtrrZFyHTDiAeBqewHEXxaythYfjAU2vswCnk05LP5n91JpmRd3MXfa948DX04DLHWtvlsEQRCEe8i9JYPcWx0QQQCKTwOR3Vnws5z6GuCfQ4CKXOXwIXex7K/S84AxDHj6LFB+EViSDlhlQmfGcqD3jS2+CwRBEB0db+/fZOkhOjYqFRDd01nwAIAuALjmKft0GskqtO8zJngA1gA1Zx+zBFnrgE5DgMG/Z+OO/K/FN58gCILwHorpIQhPDJsFBEaxmj9x/Vk/r+//BET1AELimcXnzEb2DgCpM5jw2f85cHwti/HRGdtyDwiCIAg7JHoIwhMqFdD/Nul72r1Aj3FAUAwLcD6/FTj6LVB4nI3vOZ5lhIUmAqZLwJZ/AGc2sKapUz9i02x7EwiMZMsiCIIgWg0SPQThK2GJ7D1lLHsvOMLeo3uzlHcA6J8JZC0Gtr3BvuceAAZMZS6wDX9lw3qMk5bVGC7vBcqy2boIgiCIBqGYHoJoLBHJQGSK9L3XBOnzgCnS56ge7H3jS8D6edLww197Xv5vHwFbFrpvj/HV3cDKe4ALWa7HEwRBEApI9BBEU+DWHkApejqnAXd+Cdz9LXD/z4AhjDU6LcsGoGLTHFwpTW+uAr68k6XB22ysZ9iap4BNL7OK0o5UFrKMMQA49FWz7xZBEER7hEQPQTSFlOvYuyEMSHIoRthnEtB9DBAQAVz1f9LwiQtY09P8Q0DBMcBmBb55EDj5A3B8NRM5F38DYLfwnPvFeb3cpQYAR1YB1vpm3CmCIIj2CcX0EERT6DURGP0E0HmY51YV6Q8Dp9YDwTHAiIeYkDmxFti+mI0/vlqa9vxWoLpY+n52i9RNnlNwTPpcU8Km6Tmu6ftDEATRjiHRQxBNQa0Bxr3Y8HSGYGDWj9L3gdOY6Nn/uTQs+WomeM5tBeoqpOEXfmWWHLmoyrdbejR6wGpmfcRI9BAEQXiE3FsE0Rb0vpEFQesCgV43AtO/AMa/xMZd+BW4vJt9VusAcyUrgCin4Ch7H/YH9n5sNasJRBAEQbiFLD0E0RboAoDHdgOCVbLg2KysrUVtOfseEAkkX8WqPZ/dAiSNsE9nAwrsdYHS7gOOfgdU5ADZ26UYI4IgCMIJsvQQRFuhVitdVmoN0PUq6XtSOguEBoBzW5glx2ZlXeHrq5hrK6oHE0YAa3hKEARBuIVED0H4E8lXS5+7jAS6jWGfz28FXokD3hzA2l4ArBiiRitljV3cyd4rC4Ezm4AqWTC0HEud1DvME3mHWcuNinzf94MgCMIPIfcWQfgT3a6RPncZCUSlsF5ePKanIgf48S/sc1w/9s5Fz6XdzBL0xe1A7n42LH4QMOUDILavtNyvZzGX2R9+Aro4pNnL+XEuyzKz1AG3vdssu0cQBNGWkKWHIPyJ2H5A4nD23mkI6/0162dgzjH2rtYClhppWv6uDwbqTMDBr+yCx14AMe8g8MmNwKU97PulPUzwAMDp9e63o7IQOL+NfT70FVB+ubn3lCAIotUh0UMQ/oRazSo4P5oFaA1smEYLhHYCkoazmkAcLno0WiBxGPvM21z0mww8eZJVhq4pBZbdwlxev7wmzX95j/vtOL4aEGzss80C7PhX8+wfQRBEG0KihyCuJK55mlmAAiIkoQNILq6qAvaeOgMIiQPu/g7odi0LfP5iGnBynTTP5T3u+3odXcXeubttz6dATRn7fPJH4J9DqecXQRBXHCR6COJKQmtgsThzjgOBkdJwns4OAIFRrIM7wIoizlwJ9L0FsNlbVfS/DdAYWGp8yVnndVQVswKJAHDzW0Bsf1YraMe/AKsFWPs0UHIGOLjCt23PPcjE0u6lvs1HEATRTDRK9CxZsgTJyckwGo1IT0/Hrl2eU2VXrlyJPn36wGg0YuDAgVi7dq1ivEqlcvlauHChOM3kyZPRpUsXGI1GJCQk4K677kJOTo5iOQcPHsTVV18No9GIpKQkvPbaayCIdodWD+iMymGJwyHG8QycpkyF1xqAacuAkbOBhFTg+vnsHVC6uLa9CbwcB3wwhtUPih/EAqmv/TMb/+s/gV/fYinzAFB0yrftXj+PiaUfngEKT/g2L0EQRDPgs+hZsWIF5syZg/nz52Pv3r1ITU3FhAkTUFBQ4HL67du3Y8aMGZg1axb27duHzMxMZGZm4vBhqXN0bm6u4rV06VKoVCpMnTpVnGbs2LH46quvcOLECXz99dc4c+YMbr/9dnG8yWTC+PHj0bVrV+zZswcLFy7Eiy++iA8++MDXXSSIKw9jGJA8mlVwHnKX83i1Bpj4KvDQL0BkNxbrA0iipyIP2PwPwFILlGezYf1vY+/9bgW6jGIB1BtfkpZZdNL77bu0Bzi7iX22moHv/siKLBIEQbQiKkFw59R3TXp6OoYPH47Fi1mjRJvNhqSkJPzxj3/Es88+6zT99OnTUVVVhdWrpYaKI0eOxODBg/Hee++5XEdmZiYqKiqwYcMGt9vx3XffITMzE3V1ddDpdHj33Xfxl7/8BXl5edDr9QCAZ599FqtWrcLx48e92jeTyYSwsDCUl5cjNDTUq3kIwm+oKWNBy5HdGp724Ergm/uZhej+n4G1fwZ2vc8ap2bMBqoKgbR7pWDqnH3AB2MBCIAhlGWKAcAz51l8UUP8ZwbrNdbjBiB7B2CuAG5cCKQ/2Lh9JQiCkOHt/dsnS4/ZbMaePXswbpzU2FCtVmPcuHHIynId1JiVlaWYHgAmTJjgdvr8/HysWbMGs2bNcrsdJSUl+OKLLzBq1CjodDpxPddcc40oePh6Tpw4gdLSUpfLqaurg8lkUrwI4oolINw7wQMAnYey99yDQMk5YM8n7Pv1LwADpgDpD0mCB2DB00PtFqRR/weEdmafi043vK5zW5nggQqYuAAYN58N//lFoCzbu+0lCIJoBnwSPUVFRbBarYiLi1MMj4uLQ15enst58vLyfJp+2bJlCAkJwZQpU5zGPfPMMwgKCkJUVBSys7Px7bffNrgePs4VCxYsQFhYmPhKSkpyOR1BtDsiuwPGcMBaB7w3mrmcuo5mmV7umPQGcN864OongeiebBh3cVWXMHFz6L9Sh3hBALYvBj7LZN8HTGXzDZsFdMlgGWXf/8l9BllT2P8f4JSHOkQEQXRI/C57a+nSpZg5cyaMRqPTuKeffhr79u3DTz/9BI1Gg7vvvhs+eucUzJ07F+Xl5eLr4sWLTdl0grhyUKlYDBDAMrMCo4EJr7Dh7tDogK4ZrJZQdC82rOgkcPgbYGEKsOxmVu1506ts3MEVwE9/YXV++t8G3PwmG65WA5PfYRlkZzYA+79s3n0rPAGsehhYeR/FDREEocCnNhTR0dHQaDTIz1f24snPz0d8fLzLeeLj472efuvWrThx4gRWrHCdChsdHY3o6Gj06tULffv2RVJSEnbs2IGMjAy36+Hb4AqDwQCDweByHEG0eya9wYKUo3sCcQOUGV8NIYqeU6zRqWBjsT01payw4YRXmRgCgJGPsu9yQRXdExjzDLDhb8CaOUBMb2XdoaZw3p5ub65gdYtCXJ//BEF0PHyy9Oj1eqSlpSkCjG02GzZs2ICMjAyX82RkZDgFJK9fv97l9B9//DHS0tKQmpra4LbY7E9wdXV14np++eUX1NfXK9bTu3dvRER4EWhJEB2NkDhg0B0sXscXwQNI7q2LO4Fse3zeH35i2WNl2UD+Eda3CwAGz3RtQbrqT0DPCSxj7D93AqUXGr0rCi5slz5TzBBBEDJ8dm/NmTMHH374IZYtW4Zjx47hkUceQVVVFe677z4AwN133425c+eK0z/++ONYt24dFi1ahOPHj+PFF1/E7t278dhjjymWazKZsHLlStx///1O69y5cycWL16M/fv348KFC9i4cSNmzJiBlJQUUTz97ne/g16vx6xZs3DkyBGsWLECb7/9NubMmePrLhIE0RDc0lNdBEBgFaFjejH3F8BS2y01QEgnIK6/62WoNcDtS4H4gSxbbNnNUmB0ZSErhOgrgqCsFE2ihyAIGT53WZ8+fToKCwsxb9485OXlYfDgwVi3bp0YNJydnQ21WtJSo0aNwpdffonnn38ezz33HHr27IlVq1ZhwIABiuUuX74cgiBgxowZTusMDAzEN998g/nz56OqqgoJCQmYOHEinn/+edE9FRYWhp9++gmzZ89GWloaoqOjMW/ePDz4IKXEEkSzE5LAmpyaK9l3XtOnxzhm4eHtLnpc7zlOyBAM/O4r4NNJrDr00vFMKOUfYi0wfv+Nb1ao0vOsEz2nrJmsRwRBtAt8rtPTnqE6PQThAx+MYfV7ANYFPrQTc2u9O0qa5o7PWPPThqgsBL643d4hXsbwB4BJr3u/Tfu+AL59VPqedi9wy9vez08QxBWJt/dvny09BEEQAJiLK2cfkDSSCR6AdX4PSQAqcgG1Fug+xrtlBccA964GdrzL5tcagG8eBH77kLXMGHoXc3f9shCIH8B6iVnrge3vsBiekrNAl5FAfQ1bHt8Gf3VvnfgB2PUhkPkvCrQmiFaERA9BEI1jwFTWcf2qx6VhKhWQcj2w/3Mmhow+WEwNIVKfL4AFNm96GVj/AnOfHf4vsOXvbNyEV5VuNID19eIMnAZs/6d/ip6qYuB/D7GGrwdXKH8/giBaFL+r00MQxBVCrwnAsxeAPjcph2fMBjoNBa55qmnLv3oOEJnC0uB/+wjY9pY07sfnmODRGpkAun0pEBRjH6liWWkAUHbRfa2eugrW8f2oVOQU3z8OfDkdsJibtu2e2PQKEzyA701bCYJoEmTpIQiieYnrBzy4qenLUWuA0X9izUk3vsSKHAZGsRT47f8E9CHA71YAyVex6TsPA757jHWHj+kLqDSs4rRjrZ7KAmDne0xI1ZYDUAF/Osje93zKprmwDUi5jjVkLT0P9Mtk2yNHEDwHabsi77DU8gMAir1o40EQRLNBoocgCP9l0J3ApgVSRtbIR4BrngZ63wiEJQLhXaRpI7oC93wvfQ/tzDrGl15gosdqAX6ez2JprHWylQjA6Z9ZDBLn1M/MPffv24C6ciDpfSDzXSAqBbi0G/jhGaD8IgvU7pLu/f5sepUVcoztBxQc9a1TPUEQTYbcWwRB+C9aPTDqj+yzPoRlcwFA11FKweMKPr4sG7DUASvvAbIWM8GTOByY/gUw5jk2zekNwBmZder0etYktc7uhrq4E3hnKPD3LsBH1wOXdwOV+cBntwFnt3i3LxV5UgxS5r/Ye3Ux61tGEESrQKKHIAj/ZvgsVr156oesk7y3cNFTeg5YPpO1x9AYmHVm1nqg781Ar/FsmrOb2YtTdBL49S32efDvpSw0HouT+jug+1jWNPWLaUDBsYa358ByQLAyC1KnIUBoon1dp1j22ZaFJIAIooUh9xZBEP6N1gDc8Fff5+OiZ+f7rHK0LhC480sgZaw0TXwqC4CuKmTf9cFAbF/g0m9A3iE27Kr/Y73BzNUsvkdnZF3qLXXAl3cwsfTjX4C7vpGWW1MKlJwDOg9l3wUB2PcZ+zzk9+w9uidgusQE1rY3gZM/AGc3AXd/63tbEIIgvIIsPQRBtE+46KkuYu/jXlQKHoB1fE+5XvqefDWLF+IkpDLBAwD6QBakHdmdfdcaWNNWtY51iz/1szTfV/cAH44Fjqxi3y/uZEHLuiCgfyYbxvuX5R2SrEwXfgXWPdv4fSYIwiMkegiCaJ9EdJU+xw8Chs1yPV2PcdLnlLFAjxuk74Ome15HVAqQ/hD7/NNfWLB08RngnD3OZ/08ZhHa/g773v82Vo8IkPqXHVzO+pTpQwCoWFbZ4W9AEETzQ6KHIIj2SUQ3+wcVcPObgMaNNz/lOkBlvxR2H8saoMb2B4zhwIDbG17PNU8DAZFA4XFg76fA/i+lcWUXgE9uZPFEKg0wQtZQmVt6eJzQoGlSbaMNf2MVpwmCaFYopocgiPZJWGfWd0sfDCQOcz9dUBRw2weAuYJ1igeAWT+yAoVBUQ2vJyAcGDMX+OFplpKuNbLhPScAp35ktX4A4KaFLICZE9VTuZxeE4GuV7FaQaXnmHhKu8fbvSUIwgvI0kMQRPsl7V5goBfWmkHTgGF/kL4bQrwTPJxh9zF3VXUxYLoMGMOAaZ8wixEAjJzNstDkhHZiMT4AoA1gXeUNwcDoOWzYlteYa4wgiGaDRA9BEERT0eiA8S9L3wfcDuiDgN//l9UDGv+S8zwqFRDdg33ufi2gC2Cfh/0BCOnEMrv2/psNO74GeL03cGKd83IIgvAaEj0EQRDNQc/xQJ+bmXuLW3VCO7F6QI4tLDhdMtj7gKnSMJ2Rtd8AgB3/YrE9Pz0PVOYBP/xZGetjymUVq6mHF0F4hUoQBKGtN8JfMJlMCAsLQ3l5OUJDfegOTRAEAbDsrfpq77vLm6tYYcPOaco+XnWVwJv9WJDz4N+zrvWcW95mbruKfBYkXXIGCIgA7vqfMmbIHReyWOq9McynXSMIf8bb+zdZegiCIJoLjdZ7wQMwF1jiMOfGpYZgYKg9iJkLHl4faMtC1jbjs9uY4AFYMcRlk4FLezyv7/ga4JOJwNf3e56OINopJHoIgiD8kfSHWJo7wNpn3P0tEJLAYn0+nwIUHAGC44CHfmFZX3Um4JsHgPpa98s8/DV7P/UTa31BEB0MEj0EQRD+SFiiVL158O9YhekbXmI1hSKSWeHEe9eyqtEzlgPB8czys+0NIO8w8P2fWHuMfV8wIWStV1aN5kHSBNGBoJgeGRTTQxCEX1FTChz6L5A6g7m8AMBmdR0YfWQV6ySv1rJeX4JVGtfjBtat/t+TAagACKzn2BNHWSd7grjCoZgegiCIK52ACGDEA5LgAdxngvW7lWWQ2SxM8PS5GUh/mPUGO70eWP8Cm27QHUBQLGuyevKHlt8HgvAjqCIzQRBEe0ClAm5dwjq2dx8L9BrPhqu1QNZiIPcA+973FiC0M3OD7fqQiSVfqSkDlv+OudkmL2aNWwniCoD+qQRBEO2F4Fhg4gJJ8ADA1U+yPmIAC4juPpYVQNTogfNbgbNbfF/Pj39hHeH3fwH89mGzbDpBtAYkegiCINozgZHAtc+wzz1vYK6y8CRW6wcANr7MYoA8UVcJfHoz8MUdQNYSZd2g9fNYrSGCuAIg0UMQBNHeGfkI8PtvgMnvSMOufor1/Lq0i6Wwe+LUT8wqdOpH4Mfn7Mt8FOgxDrDUAqseaVg4EYQfQKKHIAiivaNSAT2uZ1YfTkgckP4g+/zT80B9jfv5z29j72FdWMp8TB/guhdYDJHGAOTsk2KGCMKPIdFDEATRUbnqT6zAYdFJYIOLpqic81vZ+41/B548CTywEdAHAiHxQO8b2bhDK1t8cwmiqZDoIQiC6KgERkourx3/As5slMaZcpj1pyKfiSKoWIPU4BjWPoMz6A72fui/rIYQQfgxlLJOEATRkek1ARh6N6vQ/NltrL5PZT5w6TcgcTgw4iE2XfwApXuM0+MGlh1WmQfs+ZRZfAQbyxDrfxugNbTm3hCER0j0EARBdHQmLADqKoAj/wOOr5aGX/oNKMtmn5OvcT2vVs/EzZ5PgDVzpOEXd7LU9rR7gd43ATojEN5VWWgRALa+AVzYDkT1ALpfK7nLCKIFoDYUMqgNBUEQHZr8o6z2TnAs69W1URbnc+d/gD43uZ7vwnbgE7tY6TIKSLmOiSDTZeV0hjDg5jeAgbez7wXHgH+NVE7z+AFW9JAgfMDb+zdZegiCIAhGXD9gwivss83KrD45+wCogK4Z7ufrksFS4HUBLDhaowVGPwGcWAPsXgoUnQbMFUBtOfD1LBY7NPkdFkcEAEkjgYpcoOwCWx+JHqKFINFDEARBOKPWADe/BXxyExM8ARHup1WpgOtfUA7TaFmLC97mwmoBtr4ObPkHsyZpdMCBFWzcDX9lw/b+m3WI73+bfZ565nKrKQOGz1L2HTNXAzUlrBs9QXgJiR6CIAjCNZ0GA08cBvTBDU7aIBotMOZZFrvz9SwW9AwAnYYCSelA7kH2Pf8wez/1M7DmCSmmqL6KWY84K2ayFhoPb2MWKnfUlgP5R5g1SqVq+n4QVzSUsk4QBEG4JzCSBSs3FwNvB0b9n/Q9YzYTI/ED2Pe8w8wq9PUsJngM9viMja8AOfvZ50t7mItMsAJnN3te3w/Psnijo9+y74LAlt9UbFbg+BoWAE5cMZDoIQiCIFqXcS+yNPn+t0nur7j+7N10ibW9qC0DAiKBJ0+wNHpbPfDNA0xk7HxXWlbufvfrEQSp9tDxNew9azHwcgyQvaNp+7D/S9Zpfv38pi2HaFXIvUUQBEG0LmqNsg8YABjDgPAuzLrz61tsWMp1rPLzLf8ELu1mRRI/mwLk7JXm89T+wnSZ1Q8CgHNbmAja+T6rI3RyHdBlpPt5G4Kv99wvjV8G0eqQpYcgCILwD+IGsveLO9l7zxvYe1AUMOM/zNV1aRdgswDRvdm4opOAuQr47SPgoxuAkrPS8i7tlj5X5rPCieUX2ffiM03b1rIL9uWcAmpKHcZdBCzmpi2faBFI9BAEQRD+AY/r4aRcJ33uPBSY+V9AZ2+BMfY5IDieWW1y9gEbX2aC6JuHpHYYl/colyd3RcnFkZzacmDXh0BdpedtLb0gfb4sszydWg+8NYA1cSX8DhI9BEEQhH8QJxM9CYNZkUQ5XdKBWT8Bt73PYoE6DWbDf31bsrZc2gVst7vOuOiJ7sXeK3KkZRWfAWw2521YPw9Y+xSwdZH77RQEKatMvh5Ayko79j2bjvArSPQQBEEQ/oHc0tNjnPtpUu9kGV8JqWzYqZ/Ye2QKe9/0CpB3yF5YEcBoWXsMlYa9LDWsIKIcmxU4Zm/DcW6L++2sLGDzc7gbrdbELD0AE1jclUb4DSR6CIIgCP8gPJkFNANAz/ENT89FD+fWxUCviYDVDHxxB1BfzeKABt4uLTf5KiCiK/tccoZldb0zjGVzXdwJVBexcbkHmItLEIDCE8oO8jyeB/a6P5d3s+lOrgOsddJ0Tc0QI5odEj0EQRCEf6BWA1OXAje9DiSNaHj6hMHS55BOrJ3FLW+zru/cldVpCKv+3MMeFN3/NskiVHwa2PEuC0Ze8yRzSXFsFtZwdce7wJIRLNWdw+N5OqcBah1QXQyUnmfVowFAF8jes7N8/AGIloZED0EQBOE/9BwHjHjAu+rJoZ2AwGj2ecAUJppC4oFJsnicxGHs/aaFwB2fAUPvZVWhAVapmWeK5R9mAcwAEBTD3i/8KtUEOvyNtMyy8+w9uhcQb884O/Y9cPpn9plXjiZLj99BoocgCIK4MlGpgIHTmAtr6D3S8AFTgdTfASo10GcSGxYYCfSbzIRRlN3Sc+i/zBXGsdUDWqMkWnZ9KAUs5x4AqorZZ27piegqiar1L7BlRfcG0u5jwwqOOqezE20KiR6CIAjiyuXGvwPPXABieknDVCog81/As9nMBeVIZHf2XlvG3vtOlixG3cdKQdR8PABAAM5tZh95TE94V6BfJqDWsuDo8K72VPoYyZp0cVeTd5FoPkj0EARBEFc2ahe3MpUKMIS4np5bejh9JgET/86Ez8iHmdsqMEoa3+1a9s5bWsgtPclXAXMvAS8UAn86CPTPZON4tefzWxu1Sy1KB06lJ9FDEARBdCzCkgCNrIlqt2uBQdOAP58Buo9hgqlLBhvXaYjUIPXMZtastPwS+x5uzwLTBbDWGnK4UMpawuoItZbQyD0IfH478M2DrusQ2WzAZ7exjLWGCjC2QxolepYsWYLk5GQYjUakp6dj1y7P5ruVK1eiT58+MBqNGDhwINauXasYr1KpXL4WLlwIADh//jxmzZqFbt26ISAgACkpKZg/fz7MZskXe/78eZfL2LGDAskIgiAIGWoNENGNfY7uBYQmOE8zfBbLCBv7F6DrKCaSTJeYi0uwsu8hLubj9J8CDJ7JKkavnwe8HAu8lgJk/cv19NUlLIPswPLG79fmfwAfXAucXg8cXOFckRpgNY3ObmIZa+e3NX5dVyg+i54VK1Zgzpw5mD9/Pvbu3YvU1FRMmDABBQUFLqffvn07ZsyYgVmzZmHfvn3IzMxEZmYmDh8+LE6Tm5ureC1duhQqlQpTp04FABw/fhw2mw3vv/8+jhw5gjfffBPvvfcennvuOaf1/fzzz4plpaW58OcSBEEQHRvu4uIWGUdSrgOePMb6f+kDJXfVL/bMsLAk1241jkYL3LqEpd9rjSzIuboI+PE54PyvQN5h4L3RwMp7WZbXsltY/7D/PaRMnfeWkrPA5leZyAqIZMOOu1jOtjelz/7oemthVILgm80tPT0dw4cPx+LFrGaBzWZDUlIS/vjHP+LZZ591mn769OmoqqrC6tWrxWEjR47E4MGD8d5777lcR2ZmJioqKrBhwwa327Fw4UK8++67OHuW9U85f/48unXrhn379mHw4MG+7JKIyWRCWFgYysvLERoa2qhlEARBEFcAZzYyy8itS4DoHg1P/9vHwBpZZeeU64C7/ufduszVQE0JsPEV4MCXQFgXwFzJhslR61gGmS6Itdtw7EXmiZ3vAz/8GUi+Ghj2B+C/97F6RH/cw1xrgpVVjv5kojRPwmDgIQ+Vp68gvL1/+2TpMZvN2LNnD8aNk8qDq9VqjBs3DllZroswZWVlKaYHgAkTJridPj8/H2vWrMGsWbM8bkt5eTkiIyOdhk+ePBmxsbEYPXo0vvvuO4/LqKurg8lkUrwIgiCIDkDKdcCsH70TPAATEre8zVxegFSfxxv0gUBYInDjP5iFqDybCZ5OQ6XK02FdgEe2s5ii+irg35OBs5ulZdisrKfY7k8Aa73zOngrjp7jmXVKY2AVpy/9Brx/DfByHPDFNDZNrxvZe95BoKbM+/1wR/YOYO2fgbqKpi+rhfFJ9BQVFcFqtSIuLk4xPC4uDnl5eS7nycvL82n6ZcuWISQkBFOmTHG7HadPn8Y777yDhx56SBwWHByMRYsWYeXKlVizZg1Gjx6NzMxMj8JnwYIFCAsLE19JSUlupyUIgiA6MCoVkHYv8H/7gLtWAWPm+r4MYyhLpdcFskDpu1cBM1cCj+4AHvmVpd3f/gkQP4hVef7sNmDz35nIWTeXdW5f/SfmFsveKS3XXC3F5/S8gWWtdR/Dvn9+O5B/iFl6zBUsFmnCK8wKJNiap4DixpeBXe8DR79t+rJaGG1bb4AjS5cuxcyZM2E0Gl2Ov3z5MiZOnIhp06bhgQceEIdHR0djzhzJ9Dh8+HDk5ORg4cKFmDx5sstlzZ07VzGPyWQi4UMQBEG4R2cEUsY2fv5u1wBPnmDChFedju0rjQ+MZK6ttU8B+z4HNi9g77x5qTEcKDwOfHkH8NRJQGtggsdSy6xIMX3YdH1vBk79CNSVM6vPjP+wdhxBMSyeKXk0swSd3wr0ngiXHFnFlj3hVUCrdz0NwNp5yN/9GJ8sPdHR0dBoNMjPz1cMz8/PR3x8vMt54uPjvZ5+69atOHHiBO6//36Xy8rJycHYsWMxatQofPDBBw1ub3p6Ok6fdn8QDAYDQkNDFS+CIAiCaFGMoZ7bbOgCWKzRlA+ZyOGCZ8KrwOP7gaBYVjiR9/YSXVs3SMvtdSMrmAgAN78B9LieCS4usJKvZu/yDK7z24C9n7HPgsCE128fAsc8hIrUy7rVF5/xYufbFp9Ej16vR1pamiLA2GazYcOGDcjIyHA5T0ZGhlNA8vr1611O//HHHyMtLQ2pqalO4y5fvowxY8YgLS0Nn3zyCdSeoubt7N+/HwkJHlIKCYIgCMJfGXQHMHsnMGwWMPEfQMZsICCCiRsAOLWeiZPT69l33lQVYFWhp3/OhNOQ3zsvO/kq9p53kGWL7Xwf+PRm4LvHWBXpsgtAVSGb5uwm99tYel76XHLO9TSHvwYWD2c1hNoYn91bc+bMwT333INhw4ZhxIgReOutt1BVVYX77mO9Ru6++2507twZCxYsAAA8/vjjuPbaa7Fo0SJMmjQJy5cvx+7du50sNSaTCStXrsSiRYuc1skFT9euXfH666+jsLBQHMctRsuWLYNer8eQIUMAAN988w2WLl2Kjz76yNddJAiCIAj/ICSeWWrk9LwB2P8FEz29b2LCQ2Nglhw5fW5yv9zQTszac34rsMJBFJ3dLNUxAoAzm5i4cmWdkgudkrPO01nrgR+fZ13vs5YAU973tLctjs+iZ/r06SgsLMS8efOQl5eHwYMHY926dWKwcnZ2tsIKM2rUKHz55Zd4/vnn8dxzz6Fnz55YtWoVBgxQpuItX74cgiBgxowZTutcv349Tp8+jdOnTyMxMVExTp5x/9JLL+HChQvQarXo06cPVqxYgdtvv93XXSQIgiAI/6X7WOa6KjoB/PAMGzZkJmAI9m05M5azuj3b/8nqCHXJYC6zc78AVUXSdKbLQNEpZX8zTqlM9NRXAZX5TKhxjn3PBA8AnFgLWOpYHFIb4XOdnvYM1ekhCIIgrgiW3ghkb2efVRpWjyeym+d53FF+mXWD1+iBJcOZ1SiqB1BwhA2zmpl7beTDzvOueZIVVeTc9wOrYM35eDxwUZZpNmOF+8DpJtAidXoIgiAIgvADesridwbe3njBAwBhnVkhxOieQHA8YK1jggcAht7D3nmzVUcc43jkwcyX9zLBo9axbvRAm6e1k+ghCIIgiCsNXtQQAEY/0TzLVKmAbldL34PjgKF3s8/ntwEWs/M83L0VY88KKzkrjfvtY/be/zYg3V5X78Qa18tpJUj0EARBEMSVRvwAlsI+ebGyzk9TSZaJnsThQNwAliJfX8WqO8uxWoCybPa5x/XsvcRu6bHWA8ft7afS7gGSRjIrUm25stJ0K0OihyAIgiCuRDJmA0Pvat5ldnMQPWq1FKOT7dA+ynQJsFlYDFBXewo8t/Sc38pqCQXFsABptRroewsbd2Jt826zD5DoIQiCIAiCEdENiOzOPiePZu+8w/zFXcppucCJ6MrigQCg2J62ftRe0LDPJEBtL5I44gHg918DNy1sue1vAL9rQ0EQBEEQRBuhUgF3fskCkhOHsWFJI9j7xZ2AzcYsNUe/BcLtbZsiugHhXQGVmrnBKnKB42vYOG7dAYCY3uzVhpDoIQiCIAhCIravMk4ofhCgDWDuqoKjwOongKoCaXxkN9abKyyJVXLOWsLGG8OA5GucFt+WkHuLIAiCIAj3aHRA5zT2eePLSsEDSO4w7uLKWszee9/kuVFpG0CihyAIgiAIz3AX18kf2PvQu4Fu17IaPDyI+boXgD43A4FRgFoLpN3bJpvqCXJvEQRBEAThGR7MzMl4DIjuBdRXA/ogNqzTYODOL1ggs7Xe76w8AIkegiAIgiAaInG47PMIKSCZCx45KpVfCh6A3FsEQRAEQTREYKRUdXnI7z1P68eQpYcgCIIgiIa5dTFrRzF4ZltvSaMh0UMQBEEQRMMkDpNq91yhkHuLIAiCIIgOAYkegiAIgiA6BCR6CIIgCILoEJDoIQiCIAiiQ0CihyAIgiCIDgGJHoIgCIIgOgQkegiCIAiC6BCQ6CEIgiAIokNAoocgCIIgiA4BiR6CIAiCIDoEJHoIgiAIgugQkOghCIIgCKJDQKKHIAiCIIgOAYkegiAIgiA6BCR6CIIgCILoEJDoIQiCIAiiQ0CihyAIgiCIDgGJHoIgCIIgOgQkegiCIAiC6BCQ6CEIgiAIokNAoocgCIIgiA4BiR6CIAiCIDoEJHoIgiAIgugQkOghCIIgCKJDQKKHIAiCIIgOAYkegiAIgiA6BCR6CIIgCILoEJDoIQiCIAiiQ0CihyAIgiCIDgGJHoIgCIIgOgQkegiCIAiC6BCQ6CEIgiAIokNAoocgCIIgiA4BiR6CIAiCIDoEJHoIgiAIgugQNEr0LFmyBMnJyTAajUhPT8euXbs8Tr9y5Ur06dMHRqMRAwcOxNq1axXjVSqVy9fChQsBAOfPn8esWbPQrVs3BAQEICUlBfPnz4fZbFYs5+DBg7j66qthNBqRlJSE1157rTG7RxAEQRBEO8Rn0bNixQrMmTMH8+fPx969e5GamooJEyagoKDA5fTbt2/HjBkzMGvWLOzbtw+ZmZnIzMzE4cOHxWlyc3MVr6VLl0KlUmHq1KkAgOPHj8Nms+H999/HkSNH8Oabb+K9997Dc889Jy7DZDJh/Pjx6Nq1K/bs2YOFCxfixRdfxAcffODrLhIEQRAE0Q5RCYIg+DJDeno6hg8fjsWLFwMAbDYbkpKS8Mc//hHPPvus0/TTp09HVVUVVq9eLQ4bOXIkBg8ejPfee8/lOjIzM1FRUYENGza43Y6FCxfi3XffxdmzZwEA7777Lv7yl78gLy8Per0eAPDss89i1apVOH78uFf7ZjKZEBYWhvLycoSGhno1D0EQBEEQbYu392+fLD1msxl79uzBuHHjpAWo1Rg3bhyysrJczpOVlaWYHgAmTJjgdvr8/HysWbMGs2bN8rgt5eXliIyMVKznmmuuEQUPX8+JEydQWlrqchl1dXUwmUyKF0EQBEEQ7ROfRE9RURGsVivi4uIUw+Pi4pCXl+dynry8PJ+mX7ZsGUJCQjBlyhS323H69Gm88847eOihhxpcDx/nigULFiAsLEx8JSUluV0nQRAEQRBXNn6XvbV06VLMnDkTRqPR5fjLly9j4sSJmDZtGh544IEmrWvu3LkoLy8XXxcvXmzS8giCIAiC8F+0vkwcHR0NjUaD/Px8xfD8/HzEx8e7nCc+Pt7r6bdu3YoTJ05gxYoVLpeVk5ODsWPHYtSoUU4Byu7Ww8e5wmAwwGAwuBxHEARBEET7widLj16vR1pamiLA2GazYcOGDcjIyHA5T0ZGhlNA8vr1611O//HHHyMtLQ2pqalO4y5fvowxY8YgLS0Nn3zyCdRq5aZnZGTgl19+QX19vWI9vXv3RkREhC+7SRAEQRBEO8Rn99acOXPw4YcfYtmyZTh27BgeeeQRVFVV4b777gMA3H333Zg7d644/eOPP45169Zh0aJFOH78OF588UXs3r0bjz32mGK5JpMJK1euxP333++0Ti54unTpgtdffx2FhYXIy8tTxOr87ne/g16vx6xZs3DkyBGsWLECb7/9NubMmePrLhIEQRAE0Q7xyb0FsBT0wsJCzJs3D3l5eRg8eDDWrVsnBg1nZ2crrDCjRo3Cl19+ieeffx7PPfccevbsiVWrVmHAgAGK5S5fvhyCIGDGjBlO61y/fj1Onz6N06dPIzExUTGOZ9yHhYXhp59+wuzZs5GWlobo6GjMmzcPDz74oK+7SBAEQRBEO8TnOj3tGarTQxAEQRBXHi1Sp4cgCIIgCOJKhUQPQRAEQRAdAhI9BEEQBEF0CEj0EARBEATRISDRQxAEQRBEh4BED0EQBEEQHQISPQRBEARBdAhI9BAEQRAE0SEg0UMQBEEQRIeARA9BEARBEB0CEj0EQRAEQXQISPQQBEEQBNEhINFDEARBEESHgEQPQRAE4bcs3XYOD/x7N8wWW1tvCtEOINFDEARB+C0fbzuH9UfzcehyeVtvCtEOINFDEARB+C1mK7PwVNVZ2nhLiPYAiR6CIAjCb6m3i55qM4keoumQ6CEIgiD8FotVAABU1VnbeEuI9gCJHoIgCMJvMZOlh2hGSPQQBEEQfgt3b1WZydJDNB0SPQRBEIRfYrUJEJh3C9UUyEw0AyR6CIIgCL+EW3kAsvQQzQOJHoIgCMIvMctED8X0EM0BiR6CIAjCL+GZWwBlbxHNA4kegiAIwi+pJ0sP0cyQ6CEIgiD8Enm/LbL0EM0BiR6CIAjCL7HYJPcWWXqI5oBED0EQBOGXUPYW0dyQ6CEIgiD8Erl7i+r0EM0BiR6CIAjCL5G7t8jSQzQHJHoIgiAIv0Th3qqzQBAED1MTRMOQ6CEIgiD8ErnosdgERbFCgmgMJHoIgiAIv6TeqrTsVFPaOtFESPQQBEEQfkm9RWnZqaK0daKJkOghCIIg/BKLTSl6qimYmWgiJHoIgiAIv8Ts4N6q8uO09WqzBR9tPYsLxVVtvSmEB0j0EARBEH6Jo3vLny09PxzKw8trjuHtn0+19aYQHiDRQxAEQfglju4tf7b0lFabAQBlNfVtvCWEJ0j0EARBEH6Jo3vLny09tfVs28wWSqv3Z0j0EARBEH7JlZS9VVvPtrXO4r/CjCDRQxAEQfgpTtlbflynhyw9VwYkegiCIAi/xLE4oV9beuwWnjoSPX4NiR6CIAjCL3G0mvh3TA/bVmqV4d+Q6CEIgiD8kispe4vcW1cGJHoIgiAIv4S7t9Qq9v2KsPSQ6PFrSPQQBEEQfgnvsh4WoAPg35aeOorpuSIg0UMQBEH4JY6ix58tPXVk6bkiINFDEARB+CX1FubeCgvUA7gysrcokNm/aZToWbJkCZKTk2E0GpGeno5du3Z5nH7lypXo06cPjEYjBg4ciLVr1yrGq1Qql6+FCxeK07zyyisYNWoUAgMDER4e7nI9rpaxfPnyxuwiQRAE0cbU2wOZw7ml5wqo02O1CbDapFT7iyXVuPq1jVi67VxbbRohw2fRs2LFCsyZMwfz58/H3r17kZqaigkTJqCgoMDl9Nu3b8eMGTMwa9Ys7Nu3D5mZmcjMzMThw4fFaXJzcxWvpUuXQqVSYerUqeI0ZrMZ06ZNwyOPPOJx+z755BPFsjIzM33dRYIgCMIP4IHMYkyPP1t66iULj9zFtetcCS6W1OCHw7ltsVmEAz6LnjfeeAMPPPAA7rvvPvTr1w/vvfceAgMDsXTpUpfTv/3225g4cSKefvpp9O3bFy+99BKGDh2KxYsXi9PEx8crXt9++y3Gjh2L7t27i9P89a9/xRNPPIGBAwd63L7w8HDFsoxGo6+7SBAEQfgBvA1FeGDrBzLX1lux4Vi+aMHxZnqOXPRwoVbj5XJaA0EQYLMJDU/YDvFJ9JjNZuzZswfjxo2TFqBWY9y4ccjKynI5T1ZWlmJ6AJgwYYLb6fPz87FmzRrMmjXLl00TmT17NqKjozFixAgsXboUguD+wNbV1cFkMileBEEQhH/A6/SEizE9rSccPtp6FrOW7cbSX71zS8lFT51V+lxld8n5UxD2g5/twfVvbPFa0LUnfBI9RUVFsFqtiIuLUwyPi4tDXl6ey3ny8vJ8mn7ZsmUICQnBlClTfNk0AMDf/vY3fPXVV1i/fj2mTp2KRx99FO+8847b6RcsWICwsDDxlZSU5PM6Cf/FVFvf1ptAEEQTMDu4t8wWm5jR1dIcvFQOALhcWuPV9LUy606dzNXFrVO1fiR6tpwsxLmiKlwqrW7rTWl1/C57a+nSpZg5c2aj3FIvvPACrrrqKgwZMgTPPPMM/vznPyuCoR2ZO3cuysvLxdfFixebsumEH7H6YA4GvfgTlu/KbutNIVqA2nqrRysu0T4Q3Vt20QO0nsXkTGElAO9cajaboHBpyTO4Kuv8y71llW1rjbnjZZr5JHqio6Oh0WiQn5+vGJ6fn4/4+HiX88THx3s9/datW3HixAncf//9vmyWW9LT03Hp0iXU1dW5HG8wGBAaGqp4Ee0D/pR2wP5OtB9KqswY8crPePSLvW29KUQLw91bQQYNdBpWlrm6FYKZ6602XChmVpBKLzLGHAsSygUQ315/cW/JXVqt8Vv6Gz6JHr1ej7S0NGzYsEEcZrPZsGHDBmRkZLicJyMjQzE9AKxfv97l9B9//DHS0tKQmprqy2a5Zf/+/YiIiIDBYGiW5RFXDtKFpuOd1O2dU/kVMNVakHW2uK03hWhhuHtLq1YjUK8FIMXItCQXS6phsQf6emPpcYyNUQQy10mVmv0heFghevzE+tSaaH2dYc6cObjnnnswbNgwjBgxAm+99Raqqqpw3333AQDuvvtudO7cGQsWLAAAPP7447j22muxaNEiTJo0CcuXL8fu3bvxwQcfKJZrMpmwcuVKLFq0yOV6s7OzUVJSguzsbFitVuzfvx8A0KNHDwQHB+P7779Hfn4+Ro4cCaPRiPXr1+PVV1/FU0895esuthhmiw1bThZiVEoUggw+//SED3Czrb88XRHNB3cTlFXXo95qg07jd156opng7i2dVo0gvQblNfWt8iBzprBK/OxNmryTpceFewtg/922vvbL3WxNjTPadqoIqw/m4Pmb+yH4Crmn+byV06dPR2FhIebNm4e8vDwMHjwY69atE4OVs7OzoVZLF6FRo0bhyy+/xPPPP4/nnnsOPXv2xKpVqzBgwADFcpcvXw5BEDBjxgyX6503bx6WLVsmfh8yZAgAYNOmTRgzZgx0Oh2WLFmCJ554AoIgoEePHmJ6vT8gCAKeXHkA3x/IwaNjUvDniX3aepPaNTX1dj86iZ52h/yYllaZERtKZSnaK9y9pVOrYNRrALTOgwyP5wGUosUdniw9cpHmD6JH6d5q2m+5eNMp7Dhbgmt6xeCmgQlN3bRWoVG//mOPPYbHHnvM5bjNmzc7DZs2bRqmTZvmcZkPPvggHnzwQbfjP/30U3z66adux0+cOBETJ070uI625L97LuH7AzkAgL3ZpW28Ne0ffjL7czEzonHIL9SFlXUketoxvDihTquG3m7Ra43srbMy0eOVe8viXvTIY4L84SFMHrzcVPdWWTXLkK2svXKus2QXbgXOFlZi/ndHxO8n8ioo86SF4RcXf7jIEM2L3DxfXGluwy0hWhouHnQaNQxatWJYS6Jwb3kRQySvxgxIHdcBZ0tPW9Oc7i1uBXMUff4MiZ5W4OU1x1BttmJ4cgRUKqC0uh5FdLFuUfiJTTE97Q+5kC2qdJ2ZSbQPuHtLq1aJsVutI3pklh6zpcGHVEf3Vp3FuU4P4B8PYTXN6N4S0/H9YL+8hURPK7Dw9kHIHNwJ78wYiuSoIADAyfyKNt6q9g0/mSl7q/0hv1A3l6Xn8eX7MPOjHX6RXUNIcPeWXquGnlt6Wti9VVJlFt02ACAIDYsDTzE98pggf3gIU2ZvNf76KAiC6NbyBwuWt5DoaQWigg14684hiA8zoldcMADm4moqgiDgq98u4hQJKCdqzGTpaa/IL9TNYemxWG34dn8Ofj1djIIKshz5Ezx+R6eRiZ4WtvRwK0/n8ACoWWmgBuN6HN1bXJhZbYJinD+0fahtJvdWncUmpvWT6CHc0jsuBEDzWHp2nivBn78+iLnfHGrystobcvcWPb23L2oV7q2mW3oUMQ5X0MW7I8BFj1atEgOZW9rSc6aAiZ7uMUFiplVFA6Knzk0gs2MihT88hMldUU3ZHnmbn7r6K6eyM4meVqZXPBM9x5vB0pNdUq14JyTkbq0rKciOaBiFe6vKs2WmvKa+wXgDuei5kp5YOwIu3VstbOk5W8SCmFNigsXaMw1betyIHof5/OH/VdNMxQnlGVsU00O4pY9d9JzKrxAtELvPl2DYyz/jq9986/1VaDfFF1eZYW2n1gybTcDlMu8a/snnkZuU/eHpimg+5BdqT+6t2norxr6+Gbcs3uYxELXW7F/uB4JhswnidU2nkVLWW9y9Zbf0pMQGi5aehmr1OGdvcdGj/D/V+EGMYXNlbzkWXbxSINHTynSNCoJeo0aV2SrezBf+eAJFlXX4eu8ln5bFRY/VJqC0un1mg7324wlc9feN2HSiwOt5HC071a1Qtp5oPWq8DGTOK69FSZUZpwsqYapxf7NRureuHDN9e6feJh0LrUYlWnpauk4Pj+lJiZbcWw2lrV9Jlp7meiBUWHo87Je/PZCT6GlldBo1usdIGVyHL5dj57kSAMCJfN/q9xTKgi4LTO0zAPPAxTIAwL7sMq/ncTyRm5KhQPgfjqLH3TkjfxK9WOreBawQPeQK9Ru4awsA9K0UyFxnseJiKXsYTYkNRrCBVYFuyL3lrg2Fk+jxg67mzdV7Sx7n5M5Cumz7eQx68Ue/KshLoqcN6G13ce06X4Kl286Jw8uq633KHimoqBU/F7bTeiX59n28XOq9i8vRv0zurfaF/EJtttpgclMNVi56Lnn4/8jjv+r84EmcYFhkFh25e6uuAUtPdnE1vtp9UTG/t2QXV8NqExBs0CI2xIAgvbfuLXeBzP73ACa/PjbJvVXbsOj59XQRqsxW7L1AoqdDM6xrBADg/S1n8c2+ywAgBswdyzV5vRylpafW7XRmiw33fbILb6w/2ZjNbVMK7RasHB/iehxNreTeal84xkUUuxH88qdsT3FhtRTI7Jdwa4lKBWjUKq8tPX9bfRR//u9B/HKq0Od18krM3WOCoFKpfAhkdhfTo5yvqRWQmwNlIHPjRZg3MT38gdOfAp1J9LQBM0Z0wUPXdofGXgRiWNcIXNsrBoBv9XvkoseTpWdfdik2nSjER1vPNnKL24aqOotoQvUlmNnJveUHwYNE8+F4gXWXtq609HhwbykCmdve/UAwxL5bdguPtxWZ+QNSY1z+YjxPDKunFuSt6LG7RQPtTVH5NjpaiPzB6qzIVmyuQGY3y+Ep+03t8dWckOhpA7QaNebe2Bdr/m80HrymO16flipmdXkreqrqLArTqacT/LT9RK42W1Ehq63g78hdfbnlNV7X23EUOf5woSGaD36BDbHfkNxbeqTj7sk9SnV6/BPuntLZHw69DWQur2HXOEfXkjdIoofFXUrZW94FMocF6ABIVirHa5E/WBLl1qamiB55nR53Dwvcyk6WHgIA0Cc+FM/d1BfJ0UFinM8xL0VPoUPsjydLz+kCqY9M/hUU8Jwvc9nVWwWv45Yopqd9w49nYmQgAPdp65V10kXZU0yP3F1Glh7/QazGbBc73jYcFUWPF93RHTlbKNXoAeB9ILP9fxNq1Nm/s/8oF0s8HskfRLU8WL+63tro5tfeZG9xSw+JHsKJvgmhAFiNCG9SMh0FgKMIkqMUPe5jf/wNx2311sXlFNND7q12gyAI4vFNjAgA4Mm9Jf0PPLq3KKbHrxAEATabALNF6d7ypvdWvdUmul0cqyF7s15u6enu4N6qbGBZXMyEBmgV28jFUnSwHoB/PIDJBYggOGeeeUulF9lbYg9EPzqvSPT4CZ3DAxCk18BsteG8vSIop7LOgmf+exC/ni4Sh3F3ltZu+vUkevjTC3BliR5Hl523GVzOMT3+c8IRTaPOYgN/MO1it/S4q8osfzo31VoU5ng58pgeyt5qex77zz5c/dom0WLD3VvexPSYaqRj7GsCQ2FlHSpqLVCrgK5R7L/la0xPiN3Sw7eRP3BFhxgA+IeornGwZjbWCuNo6XFlMeK/mz8UZeSQ6PET1GqV2KLC0cW1cvdFrNh9EX/7/qg4rNCeyt0jNtj+3f2FX24hybuCRI+jQPM2g4vcW+0X+bEULT0Vri09jjcqd6LZm5ie8pp65JVfOefOlcym4wW4XFaD43ksk5W7t6TeW+7dMeUy0eOre4s/HCZFBsKoY24tX7O3Qo12S48YyMz+T9HBdtHjB9cix/94Y60w8jo9rixGFqtNHOYPYo9DoseP6BPPXFzvbj6Dx5fvwz57QacdZ4sBsOKFXNxw91b/TmEAmDXIlRtHbuUBrqwihjyQOcB+AWqse8ufnjKIxrHjbDFyymrE/7hBq0ZsiBGAe0uPY5NId6JH0XXaTUzPvZ/swvWLNpPwaWHqLFZR2PJq207uLQ8FJMvkosfH8150bUUHicN8DWQOdQhkdnRv+cPN31F4NYelB/AspvzpwZNEjx+RmsgEzLFcE77dn4MXvzsCm03ALnvFZgDYfoa5uLh46RYdKIoCV9ae04VKq9GVdNHmlp5B9t9Fbum5WFKNN9afREmV81O+4wnWmCwOT5gttkYH/7V3WuJ32X2+BHd+sANPfnVAvLAG6DWIst9ILhRXu8xKdHw6dxfX46o57YZj+fjPrmwAQGmVGfuyy1BltooPIETLILfU8AB1rUP2lif3Vnm1zL3l43l/TqzREywOCxZFj+esV1H0OLi3JNFjaNQ2tQTOD4VWbDyej39tPu1TQUfHdHxP9dH8wcLFIdHjR9w2tDMWTBmIZ2/sA7UKOHCpHJtOFKBUdiJvP80uutzSExtiRGwoO6Fcih57EDM/6fIr2lb0WGSBhg3BLT1D7cUc5Rk4Szadxj83nMKXOy84zVcjswaw7813wuWU1SDtpfX4y6rDzbbM9sL2M0UY/soG/HAot1mXu/5YPgD2JM5vGoE6DfomhCIsQIeCijr8/qOdKHPoP8dvOJ3DmRvMXQaXPMaB/1eeWnkAc785hJP5FTh0uVwcv9/eFqWlsNkEXCxxH3Td3pGLFh6gzsWON4HMTXFvcSsRF9OA3L3l+RrC3ThiyrpYkVkpevyhOCEXaPz3rDZbMPebQ3ht3Qks/PGE18txvI47WknlljZ/sHBxSPT4EQatBjNGdMHD16YgvVsUAODlNccASCffr3ZLDxc4MSEGxNhPKFctLM4UsKeXUSlseW3t3vrTiv1Ie2k9TuZ7Ts0XBEG09AxJCgegtPScsou588XONwh+Y5SerprPvbX/Yhkq6iyKoHKC8cvJIhRV1vnUHNYbtp5kv3VptVk8tgF6DcICdPh8VjoiAnU4cKkcd36wQyH8uUuCl4Nw5x5VlOW32GCzCeKDxs6zxQrRs6+FRc/iTadx9WubsOZg8wrHtkQQBK8tgKXVzpYe0b3lRSCzXPg2JFQcqZEJak6QPWXd2y7rPHuLiyBu7XAVyCwIAr7ecwmnGrgWNic2myBuW2QgE3cVtRbx3vH+L2e9emgRBMHJuuoUS1lH7i3CB24aGA8AOGfP5Loroyu0ahUuldYgu7ha/JPGhBgQE+LB0mP3U4/uEQ2AuYy8LfInZ/f5EmxtRFl3xbYUVGL1wVzUWWxY8dtFj9OyGCV2ogzpwiw9plqLeKLx38WVy4JfWPgTm6/ureLKOredgflv7Mqt1tHh2VEVbnphNYbCijoctbdmqbcKUpyXvfLtwMQwrHgoA7EhBhzPq8Ad72eJ4oa7JLjocWfpqXUIZJY/oe48V4JDlyTRcyzHhLoWbErKXdn7/KhBY1Ow2QRM/2AH7vxgh1fXHblo4bFaju6teo+BzNKx8zWmhz8cBdr7bQHSw6bZYvNYSqTOwb1V51CRmcf0WGyCKNp2nSvBkysP4NEv9vq0nU1BXqMnMoht0+WyGsg16VMrD6C0getbncUmHge+HE+lQsi9RTTIhAHxUKmk79f3icVgu8Xjl1OFYhXa2BADYkO4pUfpuqqXpb9npERBpWInXUm1bzfsyjoL7vp4F+775DePPb4a4t9Z58XPaw7merwI8iKKIUYtYkIMotk4p6wWZdVmUXS4enrnJ1gUPxm9OOHMFhve3XwG49/cgrSXf8bfvj/icjoueipqLV7VU+pI8HRhd6nhjcHRosaDkQN10o2pV1wIvnooA53DA3CuqEq8ifAn/d5xUrXz29/djsUbTymW6dhwVG4h2HWuRGHpMVttOJrjfX88Od7c9LmYzyn3vu2KP3OxtBq7zpVg57kS5Hpx7ZAHIhc7ure8sfTUSNc2X60LouvUILf0SP8zT+4yLibEQGb7dz4Pt8YDkjg4az/WpwoqW9ylabMJsFhtimshFyvZ9nVHBumREGZEldnaoCVebvni11mnQGZ55ecmFEFsbkj0+CmxIUaMSI4EABh1agxKDMcou7Xmy53ZsAmsEV9kkN6tpedIjgkWm4BAvQaJEQGICmLT+RrMnHWmGDX1VlhsgpN5v8BU69I8m1teo4h/qKitx9d7LgFgzQPzTLXY7aHzLhdXXNDxuIzLZdXijQEAcstqnYLv+MkWZb/QePPE98PhXPxj3XGczGeWsW/2XnZ5cZVX/y31UTy2FTvOFuPvPxxvUQsFALHbeXNaehybRl4uYxdobunhJEcHYdkfhgMADl0qg9UmiBfm/p1CEWLQwmy1YfeFUrz+00mHZomymJ56q2JcQUWdKKx5o+ADjXBxLd54CkNeWo/TBe5vJrX1VnFdl8uaP/auLW46p/KlwqhnZEVS3eEqENkxe8tTMT15TI+3sYMcLkYCZf8tnUYtrtfd8qw2QbR6iIHMVuYm5RlMYYE6sdciFx65suvw5pNNs6I3xB3vZ+GGN38Rz1GDVi3uJxc9sSEGxIWyjEi5+HQFz9wKNmgRaBeGjqJHft212gSPsVitCYkeP+aW1E4AgBHdoqDXqjFpYAJ0GpVo7o8KMkCrkVJ35aJHEAT8/QcWD3R93zioVCrEh7m2CDXElpNSjIb8gr83uxTXv7EFk/65TRFvIwgC7vp4F6b861dREH295xKqzFb0jA3GrYPZfq0+mON2ndyNwU/CTnbRc7GkBueLJdFjsQnIdxB7ju4tbyw93PVxba8YRAcbUFFnUWTNceS/cWlVwxYNm03A0ysP4F+bTzc4bUvx4ndH8N6WMy0eh8QtPc0legRBwNZTbJu5i0O09DiIHgDoGsVSjW0CO05ctMaGGPHDn67Gx/cMQ5B9Prnwd0xZd/VE3z06CKN7soeOxgQzrzuSh/KaemSdcZ/9Jf9fe1uTyltO5FVg2Ms/t3rT4VMyocNTwj0ht9RwnLO33J/PctHUkEvKEX7cA2RWRKDhYGb5/0esyGyx2a0b0jJ4rBC/PuXJrHlbmjkOTk5VnQW7L5TiXFGVmNhi1GnEc4hbmWJCDAgPZKKtvCHRUyeJHiNPGPGQvQX4j4uLRI8fM2NEF7w2dRBevW0AABab8Ol9I8RGi9zCE2PP3jpXVCU+za09lIcdZ0tg0KrxzMTeAIA4uzjKK/ctmPmXk9LN8sClMgDAb+dLcNdHO1FRa4HZasMW2ZPKwUvlOF1QCZsA/HaeWXNW2q08d2d0FcXc2kO5blMkeRAzFz39OrEaRvuyS8XUUo5j/RV+csX4kCbKxUy/TqEY1zcWAPCzPWtIMZ3M0uNNXM/pwkqs3HMJb/98qsFpW4Laeqt44/FUtbs54G4tUwMXTE/zz/5yL348kgdAqksVoNNguN3qyS0hATpn0aPTqMWL9gWZgAgyaJAYEYjr+8YhwS6e5YUvFYHM9VaXomdA5zDRvdwY0ZNjt9zkerCyyv/XhRV1zWqZ++FwLoqrzFjtJkC6uLIOE9/6Be9vOdNs6wSAUzLLlmPNMLPF5nQjLKt2/u84Fydk1wxBEJysL44WCl9cXGIgs4OgbiiYWS56eEVmmyCdD2oV+78a7cvl7lT5f2H7mWLU1lvx9Z5LOJIjuVObA/l5z606AToNAuyxS6LoCZbCCMpdHAc5/MEm2KgVra6Ox9LRwu4vGVwkevwYjVqFO4YnITEiUBx2VY9ofPVwBtK7ReKejK4AgKFdIhCo1+B8cTW2nS5CtdmCV9aw6s2PjEkR548LYwJCfsG/WFLtMU7nfFGVeKIAwMGL5aitt+Lhz/agymwVn5y3yawIcgvO4ZxyVNVZcMxunRrfPx6je0QjPFCHokozNh53/YTDY3p4On56N3bT23muBGcc2nQ4BjPziwr3WXuTvcXFTEywAeP6xgEA1h/Nd3IJKCw9Xri3uEWhzmJrk2aDJ/MrxKDs0gYuZE3FVNM099a6w3lYczAX/9rMbrz8qbR/p1DE2/+73CLn6N7i8Iy9C/asPoNWDa1GuszFh3LhL/3nFXV6ZO4ttSymblCiJHrOF1c3GOgpp9psEQWyJ9fyWYf/dW4zurgO2oOx3cWO7DhbguN5FQ0mGPjKaQ+Wninv/ooxr29SnBeu3CrOXdbZ//np/x7E0JfWKwSuo4XCl7T1ahfuLQAI0nuuylxrtyjqtWoYddJ/jR/zIL0WKpVKXG6taOmR/wetmPnRTjy58gCeXnlQsXxBEPDZjgv4pZEuMPmDGj/+AXqN+ODAEz2iQwwID/Dd0sOX4ymmx9X3toJEzxVI34RQrHgoA3eO6AKA1Ya4Y1gSAODDrefw6tpjyCmvRefwADx0TYo4H7f0cNFztrASN7y5Bbcs3ub2hsxjKoYnR8CoU6OizoIPfzmL4ioz4kONeP+uYQCA7aeLYLOxRoHydNsjl8tx6HI5bAKQEGZEXKgROo0adw5n2/73H447xc6UVJlFVwx33Q3pEg6tWoXc8lrstBeI4xkRl0prkFtegxe/O4J8U62YPspjeuqtQoNm7iJZNtxVPaJh1KntpfClJ1WbTVDE9Hhj6ZELTFdPsb6wL7tUtIJ4yxFZ0G1LxyDxJ1uztXEC74T9t+YinJdXiAszIiJQ2bDRlXsLkIIqL5SwG2GwQemq4JZDeTsWeX2RWotNfEIdmBguCp8BncMQHqgXY8vOFSsFiidyZOLFo6XHQfQ0l4tLEARR9BRXmV1aLPj/lGXyNE/sjyAIbkWP2WLD4csm5JvqFA8triwMjinrVpsAq03AgYtlMFts2CvLdHM8x3wpVyEvhyCnoVYU/L9u0KrFbQQk9zcPjObigPd64/8FLqb32GMcHdvvbDxegBdWHcaTKw94vS9y5A9qXPTI3Vuc6GC9aOlx5WaUwzMjQ4yS6HG05Dj+XuTeIpqVWaO7Qa0CfjlZiM93sEqy/5g6SHEC85geflIt+OE4auttyDfVua0LsuUEEz1j+8RiYGdWGZk/id82tDPSu0ci2KBFaXU9juaasO9iGXLKa0U//LG8CvFk5ic3AMwem4LoYD3OFlUpsrqyi6sx5V+/4kR+BUKNWozvx6wugXotBtorM/OiZTwN/3JpDRauO4FPt5/Hx9vOSU3+ZEXGGnrKEC09IQYE6DUY3SMGAPDzUcnFVV5Tr0iX9eZpX147qaELiSOVdVKKviAIeODfe/DQZ3t8CkSXZxqVeRGD5Ir1R/Nx/aLNOHzZvdm9tt6qEK+NyeDiGSMFFXWw2QSxkGZciBGRQTrFtAF6rdP8gFQPhddvCnISPfa4Nvs5YLHaFAGWZotNtFTFhxowe2wP3DQwHmn2IGZvYx7k5HjZ++6sXRTwrE1v2640RG55rUKsZ7uobcV/6zqLrdnKMeSU16LabBUDePNNdaLgktd44VlagOtzROdQnBBgx4mf09y9LQiC6Frl6/S2Vo9Vlkoe6PDfklpReBY9Rp0GWo1aFMo8S5bPz/t5VZvZec2Xd+fwJMXyHC2lH287B0AZp9YQFqtUNV5x7EXRo3YSdzEhBoTZHy7kqf+u4IHMIUat6LZzLE7oeM0l9xbRrCRFBmLigHjx+72jksXAS06s/Sk3t7wW288UYb3shv6lveR+jdmKAxfLsPZQLh75fI9YaO6anjFITQxn09j/vFOHJkKnUWNkd+Z62nqqSHRtTRqUwDJmLDYxa0suekKMOjw1nsUavb3hlJiC/5dVh3C+uBqJEQH45tFRSIqUXHsj7C4ugF3UMuwFFy+WVuMXe8Dr2cIq8WQLNepE8dXQEx9/GuLuER7XI8+qkF88AHiV+l/QSEuP1SbgtiW/4vpFW1BttsBUYxHX70s6szw+oDGWHkEQsPDH4zhTWIWVu927PhxFTmNcXNzSY7UJKK4yo1Dm4owMMiimdRXTAwDR3NJT7NrSw91kXHzUuriJ8JtwkEGLJ8f3xr9mponWBp6d40vckly85Ja7t6RwSw9/uMhpJvfWwUtKsZrtwsUlL1raXOvlSQzdo4PE84oLO7loLJaJLJcxPQ7uLUAKFAak37em3ioKWO7G9OTeqrG3FbHaBMX1wdEC0rClh62Tu7b4dvKHIj4/X25NvVV8cAk1ajFlaCLuzuiKlzJZ7KbcUnokpxzbZcHvDQnSVfsu4/cf7UT/+T/ilsXbYLUJSktPqSymR+do6ZHcW47VzR0xybK3jFrvLD3k3iKanYevTYFGrULP2GA8M7GP0/gE+wX/eF4F7vp4FwBg0sAEaNUq7LlQiv/sysbof2zErUt+xaNf7MUPh/NgE5i46d8pFKky0TKkS7jY4Z1bXD7fcQFf7GTi6ZZBncTgYx6rIBc9ADBtWBL6JYSiotaCFbsvorCiTnRr/fsPI9AjNkQx/Uh7lWoASIoIQLI9W2f3hVJREJwvrhJTWgP0kgnX0wlXW28Vb9I8ODy9O1vXocvl4tOVYyCw3NJjttgw+4u9+OTXc4pp8mU3E19Ez77sUpwqqERBRR3OFFSJFyvH9XrCahMU7rnGiJ4jOSYxjf+wh/o0JocnQ19FT2mVWWEVyzfVSpaeUIOTpcede8sxpse9e4uty5XJnRfFc5wXkNoM+CJ65Jae2nqbSytRaZVZjLniYr653FsH7ckHnGy76+/w5XJxW+QuFV8tTFtOFrq0AnLXVs+4YKTEsHOVu7hMsv+HXPR4cm9pZUFWZqtNFCo8zovvi1atEi1+ngqTvvnzSdz5wQ58u/+y+D9Qq6T2NRweyOxuWTzgnN/8uYuLn2/8vyq5t6yiayshLAB6rRp/u3UAZo7oIlr5+PmzdNt5xbocH7zklFWbMeer/dh2ugh1dvdhTlmN4rrFBVqAS/eWLJDZ65geHQL0rtv9OFl6SPQQzc2gxHBsmHMtvnl0lMtAz16xIZiWlogAnQZWm4BQoxZ/u7U/JvRnFqK53xxCcZUZEYE6DE4Kx+9HdsGPf7oGi+5IhUqlUoiWqUMTxc/conS5rAZmiw0T+sdhbJ9YDLA/sQLMMsPdU/Jh916VDABYufsS1h3OhU1gQaPypn+ctOQI8aKQHB2ERLsVSG7ylcdFBOo1oqnaMX1SDr8o6LVqhBrZ9MlRgQgP1MFssYlB2IVOlh7pwrD7QgnWHMrF6z+eUBShk/c6K/fBvfXzMSnA+0JJlaKasLfuh/PFVYoLT2MCmb/Ze1n8fDTHpKhUbaqtx7rDucwS5WDp8TWDy7EYWkFFrWh9iA2RYno47gKZeRwXv2kEGZTTcQtAvv2mI8/Y0WnYn0tu6XGEpySbfBB1jtmFruJ6eIxQfKgRPWM9t83wFV5ckQvC7JJqbD9dhJvf2YYX7D3k5KIn1wdLYkFFLe77ZBdmLfvNaRyv0dMjNgQp9gcknsFlUhQhZMe53mpDhf1mKrfqcPeWSqUSh9fWW8UbOP+d+ENFeKAOwQZlppQr+H/ufHG1KGgC7UHHchpyb9WJlh676LGLHx6Izv+7AS4sPdzyCABqtUoU2qbaepRUmfHdAXb+8eGO1yA5p+wZs7EhBjH27GJJtcusTaNe49K95XXKujx7y00gs3P2VvPV72oKJHraGcnRQWLapCNqtQoLp6Vi37wb8O8/jMCq2VchKtiAmeldxGmu7RWDbc9ch1Wzr8LLmQPFEv4AkBgRgEGJYegUZsQtgzqJw1NigtEtmj3JzR6bgndnpkGjVqG/3dIDsKq5jr5ygFmaAvUanCuqwj83slo2Nw9KcLn9oUYd+iWwZXaLDkJciEHx9AdAcVM2auWWHvcnXJEsc4tf8FQqldjziwdK8osHvwDJLS7cslBltuKCzH1Q0EhLzwZZuvyF4mpFsKe3Fhsez8O3tyGTtSMWq0286ALsYn2uiN3ItpwsxPg3fsHDn+/F0m3nnESOr5YeR9GTb6qTlS0wiJl4HHfuLXmzSMBZuPCbTKG91Yi8IB1/Uuf/B0+WHl9iehzFi6uYLJ6u3i06SLxhNYelRx7EzFvbXCiuFkU1/28r3VverzevvBY2gR0vxxR7nq7eMzYY3aMdLT3S78dFvPw3TbT/BoDk3gIAg92KIp8/xx58zc+vsACdeK3xFNMjVVevF68PrsQ071HlGGDMkWJ62LZxSxEPpuf/uQCdZHWWLD1GxbK4+7Si1oJzRZWotwroHB6AIV3CAUgJF67glrU+CaGiyLxYWu1SKDm6t9QqJs7EQOYGrlVcAIYYtKLYa6hOD7m3iDbDqNPgml4xojUlIyUK94/uhoeu7Y6P7hnm8gkXYELg60dGYeNTYxAWqFMM//KBdKz+42g8PaEP1PaLlNzSMzgpzGl5ALspTRrIRA6/CN000LXoAYDbhnQGAIztHQutRq14UpKbpQN0GqjVKjFzotpDEJ0YzxOijBsZau/5tS+7TDFdrzj2u5W4ED2AFEdjswmKQpDe3iizi6sVRd2yi6sdLD3eLYdnbvGYq7LqepfxJFV1FicXCMBitIoqzYgK0iPVbqU7fNmEb/Zewj1Ld4lxMWcLq5xEjq+BzCccRM+F4mrRmhITYnQSPQ25tzghRq3TeLXKHjdUWSdeqI2yOiqipcfFOpoS08OfouWWHptNwJ4LJWJWXreYIFn18aZnUmWXVKO8ph56jRo32JMCLpZUY9d5FieSU1aD8pp60cLChnkf0yO/Oco/yzO3esYFizdh3gBZ7g7lvzefP9SoRYTseOtkGVHc0iN3g9XWs+Brfn6FBegajMMBlC1l3NXoAaTebcdyXVfT5i0o+M2fX4d4wDi3LgbIUta5NS3eQfTw/2tFbb2Y/RUdrBdrjhVVun9w4b93j5hgJEVIxVxdWnp0asVDaGSQARq1Sryum2rrPbZNcVWnx52lh//vyb1F+A0qlQrP39wPc2/sq7jAuEKnUYsnt5yEsACFyAFYACN/+nGM55Fzhyx7YXBSuKIukSOzRnfD0b9NwDW9WHZVov3kjg81YlhyhDgdPxF5jyaP7i2ZpUcOb3S676LS0sMvgnKLC4+TACSxUVptVmR78Roke7NLcTzPfXwML4rIM1CYe8v3mB5euXtUCnM/WmyC4ubGeWn1UUxe/Cs2yWomCYKAD+3Ve29J7SQev0OXy/H+Fjacx2nkV9S6CGT2UfTYY4+4xZALR6OOuRzDAnSKXnTu6/Q4WHocrIsatUqM28oz1YoX4gCdRvyvckuPK/Ef5mP2ltUmiJYdLqLllXj/8eNxTH03Cz/Zkwp6x4UgLswAlap5Mqm4a6tvp1Ck2B9yLpXWiFZAmwDsdWgH44tbTf47yLe1sLIOploL1CogOSoIKdFs3eeKq2C1CYr/C4+h4u7f8EC96GYGJPcWIAkgx9//clmNYn4uXty1oLHZg+UBbumR/geO9LVbl08XVLjMnuJuNgOP6bFvL+81Jlp6ZEX83Fl6uOgx1VjE60V4oF58IPMU0yOKnthgMQEku6Ta5TysOKH0u/Lzhlt6BMGztZanrMvr9NQ4ZG/xc4s/iJDoIdo9Wo0aNw/qhMggPcb0jnU73bCuEeLNbpIHKw8Ae5Ev6YLIBdLVPaPRJTJIHM5PxAAv3FuFsho9clKTwqBSSU9LXBzxmItqs1V8upFbevgNpcDhCau8uh7lNfW484MdmOGi6/T200V4Z8MpsUDcRHuslZOlx42b6pNfz+Ghz3aLbgaewTSgc5j4e7hKW+etHnbK2m58fzAX288Uw6BV4w9XdUN/u6D9/kAOTuRXQK9V4+kJLPsu31TnUyBzeXU9nvzqgNjmQxAEUfRcbY8P4y6Z2BAjVCoVtBq1eEEGnNOKOY6WHlfCRV6gkMcZBMjcW/LMFEdES4+Xoq6wog4WmwCNWiVmP8otPTyDMqN7FJ69sQ/uGJYEg1YjCnBvrS5VdRZ8+us5p0Kj/H/TPToIcaFG6DVqWGwC5H+9nQ7tVnxxb5W5ET3ni9j50DkiAEadBp0jAqBWsfi74so6h5gepaUnPFCnONZyFzYXFI5FDC+X1igsPfy4u3OplFabRVd4Ra3FY/2nxIgAhBi1qLcq6w5xeHwLv9bwbeRGOtHSo5OsznmyQGY5knurXnRHhwfqRFHiSfRw12GP2GAk2a+Lhy+Xiw9e8v8zc29J3/m1z6CV3F6ehD1PaQ8xSu6tWjcVmXntLE/W9taERA/Rorw+LRV7X7hBzJpxhUqlwqI7UvHwtSm4y15l2lvuHZWMsb1j8PCYFHSNkixE/OLFA1kd/c3M/M6e3NyJnhCjDr3sAmdfdqk4XfeYINEKU1pthiAIiton3NLjGANQVmPGxZJqmC02lFbXK0RRZZ0F9376GxatPym6ev4wOhkAe2KUiypXlp46ixWvrTuBH4/kY+fZEgiCIK4/PtSICLuFwlEwlVWbxSd7fkGvqK3Hy6tZRe/ZY3ugS1SgmEbNt3l8vzgxuy7f5Gzp8eT++WzHeXy99xIW2HvD5ZuYVUCjViHDnjXHL7i8rg4gxVYA7mN6AvUaRVVcV8KF/xfzTbVioThm6XFsP9D0mB7eIDU+1IjOdqskdwsWV9aJgb3v/n4oHr42RbxxdpI12PWGP399EC9+fxR//f6oYjj/z8aGMPdFYmSA07y7zjFXF8/GLPChBYbJjejhsV88w1KjVokuyuIqs8uYHnlMjlz0yIOaRfeWC0uPMqbHc+sIeZxLRa1FFL+ujrlKpRKtPTypQQ6PXepmv/7oHazl3NIjVmQ2S+4td5aeilqLJAIDdKKYL3bj3qoxS81qmaWHHWeeORseqFOsy6hXZm/JrdzcHeWprli5/ToSEah3W5yQW9fJ0kMQLhjaJQLP3tjHpevMEwM6h+GT+0YgJSYYXWU1ffjNgz/NyAMabTYBL3x7GOPe+AWvrj0mBTI7iB4AYgDhvotl4nTybKISe6pxhax1QVFlHQpMUvYRzwoqq65XPOXL09AP2qvLhgfqMGNEEv4+ZSCGdolAkF4DQVBeUFxZevZcKJUaGZpqYaq1iGb32FADwu3b6xgELa/YzJ8U399yFgUVdegWHYQHr+kOgF1I5TefqWmJoiCpqLWI2VD8AujJ0sNblhy8VI7KOguO2V193aKDFHWZ2LZLF2p5XI+7mB6VSqWw9gQbXVh6ZLV6+G8mL8vP8Zi91UDxNg7vlt45PEC86fCnfF60s2dssHh8ODyuZ+upIkVcz4XiKry69pjC3fnDoVyxuOiG4/mKOBZHQd9F9vvG2oeJLrCEUPEY57voz+cqvkgeHC//b52zW3q4BReQjl9xpVnx+5XYrS5yd45c9ChieuyfHQNtLzlYerjYrXYneirkoqdevD64E9M8geKoC9HDY324MNI7pLzHOVh6iqrMojXROaZHZumxi46wQL2YlejO0nOmsBKCAEQE6hAZpBctPZzoYIPi+sbcW9K+yuMZvQlm5scqIlAvWXpk1yhBEERLT7QPjZ9bAxI9RLuhi8zSwy8wYkEw+wlotQl45uuDYtXqH4/kidaLGId4EEASPT8eyRNjAGJCpLoxpVX1Ynfs+FCjGBx+JMckWlq62+MZyqrrFfEc8hvXfnsg8VUp0VgwZRDuHNEFKpXKSQSwdTqLHu6iAthNlbs5wgJ0MOo0iAhyXXRMXrzwQnEVauut2HySxfb83/U9xAuaTqNGX3ssU2yIAVf3iGado+2/72m7YOLWDHcp3dVmC/ZeYPtqtQnYfb5E7Ck0OClc7LXGiZVdjOXBre5iegApbR1wLVwkS48UyByg08CgU14OmyN7i6erd44IkMSWXfTstoueYcmRTvON68fcwV/szMYzXx9Ebb0V+aZa/O7Dnfjgl7NYtv08ACa6X/iWpZ2rVCy+RN7PzlH0yB8MptjLTnD3R5ws1florgmzPv0Nn9mrpZtq6zH6H5vw5/8qWyG4i+k5b7cwcEsPAETZC0wWV9UpLD2CwP6X3HoQHqBDaAPuLcfyD5fLamSiSZa95eZGqxQ9ngOZAUn0OFp6LFabmHnYx4XoiQjUiecQf+dFG4MNWqdMW3lJBKWlx7N7S+7aAnjavsx95SB6jI6iR3bta+g/Xmexiu7AsECdIhVfmsYmulD5+UjuLYJoZuRPsfziFehQWOydjaewcs8lqFUQe3nxGBxXlp7x/eIRFaTH2cIqCAKz5EQG6SVLT7VZdG11iQoU0/SP5JSLNXp62cVCeY2DpadEEkD77RlijgHfcpcdD9ouq6lXpOYDwNZTUuXo3PJasSgit8aIlh6HmJ7Dl6WLuE1gF3X+5DpCVgwSANK6spvz1LREaDVqqFQqUUBw11iiKHpcXzB3nStRtH3IOlOMHw+zzKUJ/eMRZc8i4cjdonL3lrubEyBVZQYg1muRI3dv1SoCmR0tPe6ztyoayG7h8PiYTuFGMbajwt5e5LfzLJZmuCwAn3PbkES8lDkAahXw1e5LGP2PTZj+fpbowuD/o5W7L6Ko0oxeccG4f3Q3AFC0lHEM0uciOj7UKMZPyX+XTuFsG//+wzFsOF6AD7eyYpvHcky4XFaDH4/kK+aRWwPkYpw/CCgsPcGSpcfxhlpcZVaIllB37i03lp6cshqx3k94oE4qKOjG0lPk4N6S+m65jhXjhVaP5poUFi9eDDVQrxEFpdy9Jf//ctdVrosaPdI0UswY38eIIJ14/EqqzE7nPqAMYgaYxZOfiwC7tsldWI4p6/Jrn+Tecn0O82OnVrGUdVd1euS/exRZegiiZQgx6kQTOn/Si7Y/Xa47nIdfTxdhsb0W0Gu3pyLdnsrNKzjHBDtfhCKC9Hh1ykDxO0/t5CdyaZVZjLfpGikXPSbRvdXbnuJeWWfBxVJnS48gCNh/sQwAMNhuWeJ0lT0p87gaQVA+hRVV1inES76pVlbjhu0TFwzuLD38aXrVvsuw2gTEhRrQyeGi/Pi4nlg0LRV/GtdTHMYtMfymwS0F7txbvOI2jzFasfsicsprEajX4Oqe0dCoVYqnTneWHh507Aq5e8sxewtQBjLLm0w6ih6Xgcz2m7FNACq9aGbJRUrn8EAEGbRiVtKF4mqxivFwF5YeALhrZFd8dM8wJIQZUVRZJ/YTA6TYDr788f3icdsQZrnZdKJAjGVxtPSM6R0Dg1aNO4YlIlkmSADmBu1kD6zl6+L/F/5/K6+pV2QwuWonYbMJouiRr4OL0ZIqs1PMV3Gl2W1Mj1btHNPDp+X/t1MFlWJAdu+4UKkzuheWHrPVJrqS3InpHrHB0KhVzFori9U7an9A6B0fIpbqMMj+R/I4mqt6RGNyaif0iQ9BQpgRM0ZI9dE4ipgeno0WoEdkkB4qFfvfucro46InRVbUVW4ljglxtvToNGrR9S4/ZxqqOl4uO05qtUqMoZOLGnk2HD+PqDghQbQA3NrDb2DThiUiOSoQl8tqMPOjnbDYBIzvF4epQzvjqh7KJ93oEGf3FsAsEFPs9YG45UQe08OLkCVHB6FfAhMmey6Uij2O5O00TsraQnBLT255LQoq6qBRqzCgkzLtX269YoUnteJ6OVxIcANJbrnUwoF3qXcVyFxVZxEDHXkJgFX7We+0IUkRTpVpwwJ0mJqWKKbmst9DKYy4e8tdyvq20yxo9uFrUwBIN6+xvWPFYyZfpvwzzwIx6tTiDcYV8gKFLrO3wmQp6zL3ltGp/YDzvEadRrzxelOrh9/8eWApz9b57kAO6q1MXMqfyB25rk8cfvnzWLw5PRVThnTGCzf3AyBZKuSipm9CCLpHB6HOYsOGY/mos1hFUcJveD1iQ3D8pYl44oZeSAg1KqwosSFGMYCaU1FnUcTbAMrYnXIXw/MralFbb2OB07J9i1S4t5QZcsVVdeL8EU4xPa7cW/X2/WE3ebPFBkFgPf/6dQqV6nN5EdMDSMUZ3Ykeo06DHnZBIW/ie9zu7uLxPIDS0iO35oQYdfjnjCFY96drkDX3esyyW+bkKLO37OIiUAetRi1ec1y5uBwtPQAUcT0xIQaFsOEuKb4++XkW7uYBiVMqZtnZK03LApm5FYzH8wQZpIcJsvQQRAvA3UH84hUeqMfH9w4XL6KhRi1ezhwAlUqFq+2d1AHY41Ncm7YB4MVb++PeUclimja3KJXK3VuRgUjrGoHYEAMKKurEvledwo3iEz73vQPAJXtmDrfy9I4LcYpVkbu3kiICxfXKL0i/nGSi5+qebH/yymvEi7iTe0vmFjieZ4JgL1vPCxjym8kQB4uTO+IcYnD4k7crQVBYUSfGRExNS1Ts2/j+ceJnLtTYZ2dLj6fjBCifWh2LEwJAfJhkjeI3vwCHsvx6rdptzSpv43rqrTbxv8Fjvbj4+eAXVutoWHKkk7h0RKdR47YhiXhj+mBc1YO5HLlVRR6Er1KpcKO96vLmE4ViITudRqUQESqVCiqVCmq1SiGq40KlmB4Oq9dSrygGKBcMypge9pm3gkmKCFD8hlEy9xb/f3D3l7y4YHigTrwZA8o6PXqHOj2xIQbx3NJr1HjW3nNQtPR4kb0FSBl1nmLF+iawhxe56OH/576yyvVyIekpa9UVijo9spgeAG7jeuqtNlFcK0SPLFPPKabHvo3P39wXj43tgZ6y+Rr6f/NrD5+OF/W0CVJ8GA8MD9RLcX9UkZkgWgDeOkIeS5ASE4yP7xmGjO5RePvOIWJGUL9OoaL/2rGonSOhRh1enNxfrDektPTY3VtRgQjQazD3JmWz19gQo1jUziLzx+eW1cJiteGAG9cWoLT0JEYEKNYLsKJ+aw+xGI47hrEij6XV9aKViV90XQUyc5fYgM5hYu0hDi/M2BCOF3VeN6myzuKU7ZN1lll5+iaEIjrYIDaQ1WvUuK6PVMdJLqSU2VtsH9xl2HAasvQEG7Sii2u/vfCk0SGmx5VrixMquzF54mJJNSw2AQE6DRLs6/vzxD5I7ya5s0Z2j3I3u0t4MHBptRkWq1RugQu9gZ3DAbBu5qIVSNZexRF5YHNsqBE94yRxxm9WZdX1ivRld13ReUwPr9Hj6D7jlrrc8lrRpczP0yKZeys8UKeo+K5z6d6S3FE8geHeq5JFlw4/7t64twCpvIQrdyiHu47kXeodM7cAZWV4x5T0huAxPaXVZtFFyc95MRDcIW39pyP5qLcKiArSi+5JwNnSo8jesh/b24Yk4qkJvRX/j4ayt+SxV4DyfOSWU14XLVCWGu+Y0t5WeH5kIogrjLsykjGqR7RoiuYMS47Efx4cqRimUaswKiUKaw/luQxi9gS3uFwolhr6dbUXR8wc3Bmf78jGngulUKmYoAoP0OMipHgenUaFequAPFMt9nHR46JqdafwAHHapMhAhYWpvKYeD3++BzX1VlzVIwoTB8TDoFWjzmIT20q4CmQ+XVCBz3dki1k+AzqFKp4QNWqVGD/UELFOoodddG0Cu+HIxQN/Kh5qF3fj+sVhxe6LGNcvVpHFwoWUQdYAFgB6x4dCq1Yp+sG5Qh6w6U689E0IQZ6pFmfsdXICdBrUyS7KroKYOa6ehPNNtThdUKlwmZ6R9dPi7rhecSFY8VAGdpwtxvFcE+6UVSP3Bh7bIQjMVekYs8NFxLmiKjGDz7G9ihweMxZs0CLYoMWQLhH44K409E0IxZ0f7EC1mWVFKWJ37JaGOotVWUqhitWsEuN5ohxEj/248MwulUoS9QWmWnFfnFPWVbLPTFBUyQKPn5nYB5uOF+L/rpdizYIaaDjK16VVq2CxCaJl1JOlh1sa+W9RWmUWLUR95O6tJlh6wuzZW/K4IR5H5q4q88fbmNVwZnoXhdvXU0yPpweHBgOZHSxQOo1a/B1X7buMn4/liy1PguS9ufzE0kOih2hXaNQq9IrzfFOUM6F/PNYeymvwRuoIvwDyGifyp1OVSoW/Tu6PKf/ajh6xwdBq1OKFBGA3rrAAHc4VVeF8UTUO2asPuxI9Oo0af508ALnlNUiOCpRZeurx/KrDOFdUhc7hAfjnnUOgUauQEGbE+eJq0bUR6yKQ+cmvDuDAJSlVfVSPaHQOD4BRp0ZtvQ19E5zdbO6Ik11ImcAziBdAU029QnTwrtv8+NzQLw7/fThDzG4Tl2kXanGhRsUTaOfwAGTNvV5xQ3QFv7lq1CrFU7ecvgmh2HRCyngL1GtQUy/ts6cnfn4T4hlqZdVmZC75Fbnltfj+sdEYaO9TdtbuyuweE+S0jJHdo3y28gD2In+BehRXsUKX/OYvpqTbrR6mWovYv82xvYqc5Gg2vdyNON5eCTw8UGcv+mdWPPXzm66j+8NstaHKbBULLjruNxfsFbJmlXy7fzqaj5p6K6KD9UiOCoJaxfbVahOU7i2H48kC4GNE1640nB2/eqsAs8WmmK/eahPdvF0iA3G2qErMKPSUFegoBnh9qS6RgYr/ubuYHm/g4p8bSUONWjGbkVuj5a65vdml2JtdBr1Gjd87FHZNimRVsHnn9fBAvXhuBnqwZDYUyFwma/fBCdBpUFFnwd9/OI6aeqsofJmlxx7ITKKHINqeyamdEBdqVJinvSGtawSGdgnHuaIq1NbbMC0tUTF+QOcw/DznWrE4nvxGHR9qRFSwHueKqvDFzgvixT7FwTrF+V26lOXBXTyXy6rxg92t9c7vhog3+ni76JGvC5BM5HmmWuSU10KjVmHBbQORlhwhrjclJhhHckwYkuSdawtQPskGG9gFOjRAh5Iqs1MG12l75Vq5VclVjZre8exYuBKv3ljkukUHYVBiGJKjgty6dRyPd4BegwCzd+4t+U1BEAQ8979DYhrysTyTTPSwC7+749pYooMNKK4yizFjATqN2BzVqNOgU5gROeW1Ykq8p98srSs71oMSnS174bI+Y0pLD7vp8ZtiWIAOtfVW1FlsKK0yu7X0OLqQQwOkbEvurr2hX7x4kw+z/4/kIsJRxLoTKfJmsVV1Fui10rr59mvVrA4WD+b3tDy+PfL95kkJjg9McoGVEOo+SN0VjjFocmHBXZhnC6uQueRX1JitYj+6W1I7KWLhACb8/jq5P6rNVvH68OyNfZBTVuuUmalYZwB/QHIX06N0bwEsY62iziJa/niShty9VW0PdG4ohq2lIdFDdGhUKlWjnriDDVp88+hVHqeRF0uUXyASwoyiBeYHe42ayamdFfVp3MEtTBuPFcBiE9ApzCjGMQGSyOHwG154kJRqDbD02TscXCtX94zBkRyTaJr2BnkxQR58GmLUsrRkWQZXbb1VjIWQB026YnBSOL577CpFur4v6LVqfPfYaI/TOIoex5geV7FAHHmn9f/uuYS1h/LEcZdk8R5nPFh6mkJUsB7Il9yF0SF6xY0kOToIOeW12HOexSt5Ej39O4Uha+51Tj3LAGVsh9LS49wrq96iQU55LQor68Tg7W4OMT2hRp1oveHfoxyE0MQB8eLnWaO7Yc+FUoWocLT0uLNIajVq0dVbZbYoyh1w11ZUsF5RDwiAoh+VI45igBc1dQz+5sLMqFOLxQa9JUCnEa0xgJR1CUgWO96vTY6rTDCAufvl3H919wa3ocFA5hqlewuAonmpnCC95N6y2gTUWwXotSR6CKLdwy+YALPGOKYGTxna2avlcDdVjt2yMKKbMvsnXhbIGB2sF2MgQgxaxcX01tROTsuec0Mv3JXR1eki7olAvRYhRi0qai3iU6pUa0S6aJ4rqoJNYOZ6b6w1g+zNOVuKbtFBojsPUHZZB7yz9JTX1OP7zWcAMFfCxZIaRWNYbkFoCUsPIAXROrqvkqODsP1MsehGauj3dmx6yQmT3eQd60IBULR9sNoE5JTXYu+FUpitzJ3k+B9X2/tvcdERGqB1yrTLkD2AzB7bw2mbHPtaeYpNCTJoUWcxO2UNFVaycycmxOBkWfHOvcVEH0+xl7dHASRhFu/gnvUGlUqFEKNWdL+FyS09spIaKhXw9ITeOJZbgT7xIWLxxOaAu+lr6q2os1gVJSoAWUyPm154PAYRYMVh5b9pjdnqJFxbm0atfcmSJUhOTobRaER6ejp27drlcfqVK1eiT58+MBqNGDhwINauXasYz1MoHV8LFy4Up3nllVcwatQoBAYGIjw83OV6srOzMWnSJAQGBiI2NhZPP/00LBb/KIhEdGwcLT3yIMOescFiUcOGiHC4wDpWTZZni8jN3SqVStwGg1atSBHn6LVqnwQPh7u4QsWyALzWiHTu8fiSnnEhbW7eBli8SG+Z+yxQ72jpcX/z40/vF0trxPTsB69hdYe46CmrNosuG0eLR1Ph1hHemd5R1HRzsJB5iunxhPwmL8/6K65iokVeTJDf+H8+xqwQqYlhLi2XUbL/b6ismCgAjOsb1+AN0bGMgCeR4q7pqDyrzVH0eAxgt/8etfU21NZbxePreE6KosfHeB6OPKhfbk3h2VsAcP/obnh0TA+8M2OIS3HYFEIMWrHmlytrDxd78gw7LnqCDVrMTJdii4L0WjHQGQCq/aBAoc+iZ8WKFZgzZw7mz5+PvXv3IjU1FRMmTEBBQYHL6bdv344ZM2Zg1qxZ2LdvHzIzM5GZmYnDhw+L0+Tm5ipeS5cuhUqlwtSpU8VpzGYzpk2bhkceecTleqxWKyZNmgSz2Yzt27dj2bJl+PTTTzFv3jxfd5Egmh1FTE9YgKJoW+aQzl4LAcenyhHdlDExysJ+ypsdj+u5rk+sU8+fpsDXI3dvAcpAyNP2fkOOWXVtSZ94SWgaHcrye3Jv8WO5056C31XWfoRX2eaZWwlhRo/LagzcOlLpxpIjr3/kary38BtuiaxBJiDFxMgtPfy/9ZvdpcZbljgid2eF2ufjf/0J/eNdziPH2b3l/reVmo4qLT3cPRcTYlDUA2poeSEGKai4vKZeFD3y9igAC1LvFh2EzMHeWW+d1iMTYvKHpR6xwYgJMWBg5zA8Ob53o5btDWp7XB4ARX0mjmP9IEAqBntDvziMl7nHeRCz2J/LD4KZfRY9b7zxBh544AHcd9996NevH9577z0EBgZi6dKlLqd/++23MXHiRDz99NPo27cvXnrpJQwdOhSLFy8Wp4mPj1e8vv32W4wdOxbdu0v+x7/+9a944oknMHDgQFerwU8//YSjR4/i888/x+DBg3HjjTfipZdewpIlS2A2u64sSRCthdwUnBBmRNfIQKhUzEydOcT7i2OEbDlRQXqkOMSLyC09jumyvAbLtGHKoOumEhfCLT3cvcWzm6QbJW9IyrfBH+DF5gB2UTZ46d7iN0qeOTWgc5goYvNMtTBbbC0WzwM4W24c26c4WpYaLXrsN9yLsjglgIkeQRAU9Vq4GOfxOsO6ug6GjwxSxoBp1CpMH5aEjO5RGNM7xuU8clxlb7mDj6syu7H0uHJveXCXqVQqRZyTZOlRCqeUmGBsemoM7nTRZsIb5EJMft0IMmjx6zPX4ZtHRzm1TGlu+H/siL0Q4+HL5fhsxwUIgiArIilt26DEMOg0KvwuvQvSkiPE355bzvypQKFPosdsNmPPnj0YN26ctAC1GuPGjUNWVpbLebKyshTTA8CECRPcTp+fn481a9Zg1qxZvmwasrKyMHDgQMTFSSpzwoQJMJlMOHLkiMt56urqYDKZFC+CaAnkT2zxYUZEBRuwaFoq3pkxxCeXktzSM9xFNV+5Sd2xhs7LmQPx9SOjcF0f7wOVvSHR7qrjN1eX7q185zL5bY08mNmx4ag3lh7OwM5hiAk2wKBVwyYAueU1LZa5BcAp+NexfUqSXVCL4xvp3uIxPby3HBccZqsNplqLInvL0QI51I3oUbi37CL571MH4T8PjvTqRu5k6WkgpgdwtlZwl2RcqNFJ9DRUqkEez8XjbuRup+ZAYelx+K95qhTenNw8iMX8fbbjAipq63H30l14YdVhbD5RKFoY5ds298a++O0v4zA8ORIGrQbX2EsI8GuCvFVFW+PTr1dUVASr1aoQFgAQFxeHvLw8l/Pk5eX5NP2yZcsQEhKCKVOm+LJpbtfDx7liwYIFCAsLE19JSb4VCiMIbwl3SFkHgClDE8WLi7eEBejEG5qjawtgNzhugnd0b0UG6cUU5ebknoyueGZiH8y6imWQ8Is2jwOpt9rEG40/iZ4+CaHibxls1CqamHrM3nIheuRdrS+V1kg1epo5ngdwFjGOlh+Wts62JVCvabR7jQt1XoE5Jtgg9cqqrBOPb3iAXhHX0j0myEkESduujOnxFb1GKfI9WXq44Dwu63dXb7Vhtz2VP61rBEIM0jaoVc4p8Y6EiS4/af8dLT1NRRHTE9i8y/aWGSOSoFWrsOdCKZ5eeVC0am09xVreqFTK80CtViksP3/L7I+37xyMGwckAJDchleke6ulWbp0KWbOnAmjsXFBYL4wd+5clJeXi6+LFy+2+DqJjknniACEB+rQJz6kSTEeGrVKFE0ZKc6p9hq1SiwYGBfS8ucQwIoBPjImRbQscZcOT6m+UMzaMQTqNYoy+W1NWIAOr2QOxAs390OoUeeQvdVwvRYObxLLW3BcLKkW+6k5Fl5sDhwtPa7cV9zF1VjXFuB8ww0PlFLMiyrNipgeeVyLO9cW4ODeaqDIpCuc3VvuzyVeVZx3sweAg5fKUGW2IiJQh77xoQqrSqBe22BsHf9NskuqxfIPEYGuBV5jkae5t5XoiQ014saBTLCsOyIZDXgrGe6adDt/iBG3Du4sHi9/cm/5dPWNjo6GRqNBfr6yTkB+fj7i410HocXHx3s9/datW3HixAmsWLHCl80S1+OYRcbX627bDAYDDIbmNU0ShCsC9VpseXpsg0+S3vD2nUNwuazabUHFacOSsO5wHoYlN79VxxuG2vt2Hc01obbeqihK6Kk7elsgL/wod214rMgsexLvEhkoZrFwS8+mEwUoqKiDQasWf4vmxMnS40LYJEcHYtvpxmduAcoyCwATN0adBheKq5mlh4sehwahw9wEMQMOgcwumsE2hF6jFKOe3Fu8SOSRnHLYbALUahW2n2Y37YyUKKjVKoVVxZsq5Nxiy92XoUZts7ubQtzE9LQ2d2d0xfcHcgCwIO6KOov4IOOrGJPcW1dY9pZer0daWho2bNggDrPZbNiwYQMyMjJczpORkaGYHgDWr1/vcvqPP/4YaWlpSE1N9WWzxPUcOnRIkUW2fv16hIaGol+/fj4vjyCaG37TaCojukXitiHug5GfuKEXfnzimja7YCZGBCA6WI96q4AjOSbszS4DAEWKuD8id295CmQOMWpFt9hAWSVjbunZcIxdg0Z0i2yRgFOjTqPYPlcxO7yBrGOtHF9wtGiFB+rEmJyiKgdLj8yd5S6eB3CM6Wm6pceTUOlur8VUZbbinL1K9PYzXPSwHmlyS0+QN6LHfk7xGkzu3HhNIdRDTE9rMqxrBIZ0CYdGrcKrU5QJRL5ul5S9ZWu27WssPkvUOXPm4MMPP8SyZctw7NgxPPLII6iqqsJ9990HALj77rsxd+5ccfrHH38c69atw6JFi3D8+HG8+OKL2L17Nx577DHFck0mE1auXIn777/f5Xqzs7Oxf/9+ZGdnw2q1Yv/+/di/fz8qK5nvfPz48ejXrx/uuusuHDhwAD/++COef/55zJ49m6w5BNGKqFQqDLa3stiXXYoN9tot13qRndOWeBvIrFarRNEhb8yaFMkEBi8AOVrWfLS54bExIUatS2E1NS0RT97QC0/c0KvR6zDq1AqRERagF9sZFFXUyYrU6ZAYEYAQoxbdooOcMgrlRMkEWkM91Fwh3x6tWuWxro9Wo0Y/uzX08OVy1NZbsSebpdSPsruGQxWWnoYtT1yo8Rg1xxo9zYG77K3WRqVS4dP7RuDnOdfi5kEJCsEa5uN28X365WQhBN5YrI3w2b44ffp0FBYWYt68ecjLy8PgwYOxbt06MWg4OzsbarX0Rxw1ahS+/PJLPP/883juuefQs2dPrFq1CgMGDFAsd/ny5RAEATNmzHC53nnz5mHZsmXi9yFDhgAANm3ahDFjxkCj0WD16tV45JFHkJGRgaCgINxzzz3429/+5usuEgTRRIZ0CcfPx/Lx3YEcnCmsglatwjW9/Fv0yF2PDcVdRQcbUFFrUfSs4pYezuieLSd6ooINOF9c7TZmJ9igxR9lXccbg0qlQniATmy3EB6og87uniyqrFNYeoIMWmx6agz0WrXHuJjIJlp65B3XvXFHDewchr3ZZTh8uRwxwQaYLTbEhRrEAPNgRUyP9+4tsZVFC4geufWpMS7A5iQsQCeK074Jodh2mgUy+2rp+V16F3x34DLWHcnDkk2n8dh1TftvNoVG/aKPPfaYk6WGs3nzZqdh06ZNw7Rp0zwu88EHH8SDDz7odvynn36KTz/91OMyunbt6lTtmSCI1of3Azto7+Y+oltko7J1WhO1WoXO4QEoqqxT9BRzxfxb+uHAxXKMlFXElhecjAzSo29887UGcIRbepoSs+MN4YGS6AkL0ImxGdkl1aJFi8f+eJMaH2rUoldcMCprLY3adrkw9Uak9Ldb4g5dLofG/jA+KiVaFGYatQpBeg2qzFbvRI9DLEtzBzEDUkxPqFELbSukp3tL34QQSfT4GNOT1jUCf7t1AOZ+cwiv/3QSveJCMN6LYpQtAfXeIgii2RmUFA6VCuCW7Ov6xLbtBnnJiodGotpsbVCgjekdizG9lfsUFaQX+3mNsgfKthRcYEQ3ITvLG+TBzOEBOtEywovW6TVqRdZbQ6hUKqz+49Ww2oRG9WCSBzJ7ytzicPfjgYvl2GePLXMsghhi1KHKbPUYFM1xvNm3REwPd5M2tuFuSyFPnGhMrNGMEV1wPNeEZVkXcDjHRKKHIIj2Q7BBi95xIWKNlHF9m7cgYkvh6KLyBZVKhaSIQJwqqMTVLejaAqSU9JaoAyRH3l8pPFAnFizkdVv6dgr1uZdaUxpOyuf1RqT0jA2GQasWi+KN7R2DWxxqY4UYtcgzeWc5CnPIaGsJ0dM1KggrH85oUhB6S6AQPY20cD1/cz+M6ROLsb3b7iGIRA9BEC3CkC7hOJ5Xge4xQUhu4Zuzv/Dk+N7YcCwfk1Mb13fJW34/siuSIgNxVQsGSwPKJ/qwAD1SYoOg06igVasxa3Q3PHhtdw9zNz8K0eOFSNFq1OiTEIoDF8uQGBGAN6cPdrLA8RgabwKZHYOvWyKQGWDV1v2NlJhgsYN6Y+sH6TTqNhU8AIkegiBaiFtSO2H5bxdx98iuDU/cTpg4IB4TB7S82d6o03jVoLOpyG9uYQE6xIYYsempMQjSa1vshu8JeSCzN5YZgHUk/+TXc3g5c6BLCwWPofEuZd3BvdWG2VWtjV6rRt+EUBy8VO7U1+9KgkQPQRAtwqiUaJx55Sb46P0g/Ai5SAgXizA23gXYVHx1bwFMfN+S6r7dC7f0eOfeah1Lj7/y6m0DsetcCUZ2d64Gf6XgP6HhBEG0O9Rqlc8xH4T/IE8rb6uWCHIMikDm5in8yItmpnjRF06nUSsKQ7ZEyro/M6BzGP4wupvHFhT+Dll6CIIgCJfwmB69Ru21ZaUlUcb0NM/ta/bYHpg0KEEMDm+IsACd2Gm8o1l62gNk6SEIgiBcwq07oQE6v7DY6X2s0+MNarUK3WOCvd4/7uLSqlVtXjyQ8B0SPQRBEIRLeseHIFCvwdAu4W29KQBYMUHuWWku0eMrXAhGBOn9QggSvkEylSAIgnBJbIgRu/4yzi9cWxy9lhWA9CZlvSXgoqcjZW61J8jSQxAEQbgl2KD1q8BVvb01Q1sJMV6gMCKo7QO7Cd8h0UMQBEFcMei1TOy0tXsrKqhlW4AQLQOJHoIgCOKKQW8vUNhc2Vu+0ieepbj3TQhpk/UTTYNiegiCIIgrBp7BFdhG7q3JqZ2QmhiOLpFtV6SRaDxk6SEIgiCuGHhtnJbuMO8OlUqF5Oggpx5exJUBWXoIgiCIK4ZXMgfi0OUypCaGtfWmEFcgJHoIgiCIK4Z+nULRr1NoW28GcYVC7i2CIAiCIDoEJHoIgiAIgugQkOghCIIgCKJDQKKHIAiCIIgOAYkegiAIgiA6BCR6CIIgCILoEJDoIQiCIAiiQ0CihyAIgiCIDgGJHoIgCIIgOgQkegiCIAiC6BCQ6CEIgiAIokNAoocgCIIgiA4BiR6CIAiCIDoE1GVdhiAIAACTydTGW0IQBEEQhLfw+za/j7uDRI+MiooKAEBSUlIbbwlBEARBEL5SUVGBsLAwt+NVQkOyqANhs9mQk5ODkJAQqFSqZl22yWRCUlISLl68iNDQ0GZdNtE46Jj4H3RM/A86Jv4HHRNnBEFARUUFOnXqBLXafeQOWXpkqNVqJCYmtug6QkND6U/qZ9Ax8T/omPgfdEz8DzomSjxZeDgUyEwQBEEQRIeARA9BEARBEB0CEj2thMFgwPz582EwGNp6Uwg7dEz8Dzom/gcdE/+DjknjoUBmgiAIgiA6BGTpIQiCIAiiQ0CihyAIgiCIDgGJHoIgCIIgOgQkegiCIAiC6BCQ6GkFlixZguTkZBiNRqSnp2PXrl1tvUkdhhdffBEqlUrx6tOnjzi+trYWs2fPRlRUFIKDgzF16lTk5+e34Ra3P3755Rfccsst6NSpE1QqFVatWqUYLwgC5s2bh4SEBAQEBGDcuHE4deqUYpqSkhLMnDkToaGhCA8Px6xZs1BZWdmKe9G+aOiY3HvvvU7nzcSJExXT0DFpXhYsWIDhw4cjJCQEsbGxyMzMxIkTJxTTeHO9ys7OxqRJkxAYGIjY2Fg8/fTTsFgsrbkrfg2JnhZmxYoVmDNnDubPn4+9e/ciNTUVEyZMQEFBQVtvWoehf//+yM3NFV/btm0Txz3xxBP4/vvvsXLlSmzZsgU5OTmYMmVKG25t+6OqqgqpqalYsmSJy/GvvfYa/vnPf+K9997Dzp07ERQUhAkTJqC2tlacZubMmThy5AjWr1+P1atX45dffsGDDz7YWrvQ7mjomADAxIkTFefNf/7zH8V4OibNy5YtWzB79mzs2LED69evR319PcaPH4+qqipxmoauV1arFZMmTYLZbMb27duxbNkyfPrpp5g3b15b7JJ/IhAtyogRI4TZs2eL361Wq9CpUydhwYIFbbhVHYf58+cLqampLseVlZUJOp1OWLlypTjs2LFjAgAhKyurlbawYwFA+N///id+t9lsQnx8vLBw4UJxWFlZmWAwGIT//Oc/giAIwtGjRwUAwm+//SZO88MPPwgqlUq4fPlyq217e8XxmAiCINxzzz3Crbfe6nYeOiYtT0FBgQBA2LJliyAI3l2v1q5dK6jVaiEvL0+c5t133xVCQ0OFurq61t0BP4UsPS2I2WzGnj17MG7cOHGYWq3GuHHjkJWV1YZb1rE4deoUOnXqhO7du2PmzJnIzs4GAOzZswf19fWK49OnTx906dKFjk8rce7cOeTl5SmOQVhYGNLT08VjkJWVhfDwcAwbNkycZty4cVCr1di5c2erb3NHYfPmzYiNjUXv3r3xyCOPoLi4WBxHx6TlKS8vBwBERkYC8O56lZWVhYEDByIuLk6cZsKECTCZTDhy5Egrbr3/QqKnBSkqKoLValX8AQEgLi4OeXl5bbRVHYv09HR8+umnWLduHd59912cO3cOV199NSoqKpCXlwe9Xo/w8HDFPHR8Wg/+O3s6R/Ly8hAbG6sYr9VqERkZScephZg4cSL+/e9/Y8OGDfjHP/6BLVu24MYbb4TVagVAx6Slsdls+NOf/oSrrroKAwYMAACvrld5eXkuzyU+jqAu60Q758YbbxQ/Dxo0COnp6ejatSu++uorBAQEtOGWEYT/cuedd4qfBw4ciEGDBiElJQWbN2/G9ddf34Zb1jGYPXs2Dh8+rIg/JJoHsvS0INHR0dBoNE7R9fn5+YiPj2+jrerYhIeHo1evXjh9+jTi4+NhNptRVlammIaOT+vBf2dP50h8fLxT4L/FYkFJSQkdp1aie/fuiI6OxunTpwHQMWlJHnvsMaxevRqbNm1CYmKiONyb61V8fLzLc4mPI0j0tCh6vR5paWnYsGGDOMxms2HDhg3IyMhowy3ruFRWVuLMmTNISEhAWloadDqd4vicOHEC2dnZdHxaiW7duiE+Pl5xDEwmE3bu3Ckeg4yMDJSVlWHPnj3iNBs3boTNZkN6enqrb3NH5NKlSyguLkZCQgIAOiYtgSAIeOyxx/C///0PGzduRLdu3RTjvbleZWRk4NChQwpBun79eoSGhqJfv36tsyP+TltHUrd3li9fLhgMBuHTTz8Vjh49Kjz44INCeHi4IrqeaDmefPJJYfPmzcK5c+eEX3/9VRg3bpwQHR0tFBQUCIIgCA8//LDQpUsXYePGjcLu3buFjIwMISMjo423un1RUVEh7Nu3T9i3b58AQHjjjTeEffv2CRcuXBAEQRD+/ve/C+Hh4cK3334rHDx4ULj11luFbt26CTU1NeIyJk6cKAwZMkTYuXOnsG3bNqFnz57CjBkz2mqXrng8HZOKigrhqaeeErKysoRz584JP//8szB06FChZ8+eQm1trbgMOibNyyOPPCKEhYUJmzdvFnJzc8VXdXW1OE1D1yuLxSIMGDBAGD9+vLB//35h3bp1QkxMjDB37ty22CW/hERPK/DOO+8IXbp0EfR6vTBixAhhx44dbb1JHYbp06cLCQkJgl6vFzp37ixMnz5dOH36tDi+pqZGePTRR4WIiAghMDBQuO2224Tc3Nw23OL2x6ZNmwQATq977rlHEASWtv7CCy8IcXFxgsFgEK6//nrhxIkTimUUFxcLM2bMEIKDg4XQ0FDhvvvuEyoqKtpgb9oHno5JdXW1MH78eCEmJkbQ6XRC165dhQceeMDpQY2OSfPi6ngAED755BNxGm+uV+fPnxduvPFGISAgQIiOjhaefPJJob6+vpX3xn9RCYIgtLZ1iSAIgiAIorWhmB6CIAiCIDoEJHoIgiAIgugQkOghCIIgCKJDQKKHIAiCIIgOAYkegiAIgiA6BCR6CIIgCILoEJDoIQiCIAiiQ0CihyAIgiCIDgGJHoIgCIIgOgQkegiCIAiC6BCQ6CEIgiAIokNAoocgCIIgiA7B/wPGnjnS6GpMBwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(history.history['val_loss'][1:])\n",
    "plt.plot(history.history['loss'][1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lightgbm Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "cf = lgb.LGBMRegressor(colsample_bytree=0.8626313651274391, metric='None',\n",
    "                       min_child_samples=468, min_child_weight=0.01, n_estimators=5000,\n",
    "                       n_jobs=4, num_leaves=33, random_state=314, reg_alpha=7,\n",
    "                       reg_lambda=0.1, subsample=0.7272577195782406)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---cf.fit---\n",
      "LGBMRegressor(colsample_bytree=0.8626313651274391, metric='None',\n",
      "              min_child_samples=468, min_child_weight=0.01, n_estimators=5000,\n",
      "              n_jobs=4, num_leaves=33, random_state=314, reg_alpha=7,\n",
      "              reg_lambda=0.1, subsample=0.7272577195782406)\n",
      "---cf.score---\n",
      "-0.072123723819421\n",
      "---predict---\n",
      "[0.67393925 0.64559687 0.63074853 ... 0.32265549 0.34377814 0.35777767]\n"
     ]
    }
   ],
   "source": [
    "##############################################　　　自己加入的　　　##############################################\n",
    "\n",
    "train = dataset_train[feature_names] , dataset_train['return'] > 1\n",
    "test = dataset_test[feature_names] , dataset_test['return'] > 1 \n",
    "\n",
    "print('---cf.fit---')\n",
    "print(cf.fit(*train))\n",
    "print('---cf.score---')\n",
    "print(cf.score(*test))\n",
    "print('---predict---')\n",
    "print(cf.predict(test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###############################################　　　自己加入的　　　##############################################\n",
    "#\n",
    "#import lightgbm as lgb\n",
    "#\n",
    "#fit_params={\"early_stopping_rounds\":30, \n",
    "#            \"eval_metric\" : 'auc', \n",
    "#            \"eval_set\" : [test],\n",
    "#            'eval_names': ['valid'],\n",
    "#            'verbose': 100,\n",
    "#            'categorical_feature': 'auto'}\n",
    "#\n",
    "#from scipy.stats import randint as sp_randint\n",
    "#from scipy.stats import uniform as sp_uniform\n",
    "#\n",
    "#param_test ={'num_leaves': sp_randint(6, 50), \n",
    "#             'min_child_samples': sp_randint(100, 500), \n",
    "#             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "#             'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
    "#             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
    "#             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "#             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n",
    "#\n",
    "##This parameter defines the number of HP points to be tested\n",
    "#n_HP_points_to_test = 500\n",
    "#\n",
    "#from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "#\n",
    "##n_estimators is set to a \"large value\". The actual number of trees build will depend on early stopping and 5000 define only the absolute maximum\n",
    "#\n",
    "#clf = lgb.LGBMClassifier(max_depth=-1, random_state=314, silent=True, metric='None', n_jobs=4, n_estimators=5000)\n",
    "#gs = RandomizedSearchCV(\n",
    "#    estimator=clf, param_distributions=param_test, \n",
    "#    n_iter=n_HP_points_to_test,\n",
    "#    scoring='roc_auc',\n",
    "#    cv=3,\n",
    "#    refit=True,\n",
    "#    random_state=314,\n",
    "#    verbose=True)\n",
    "#\n",
    "#gs.fit(*train, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3128\\2647650527.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m###############################################　　　自己加入的　　　##############################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m##t1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gs' is not defined"
     ]
    }
   ],
   "source": [
    "###############################################　　　自己加入的　　　##############################################\n",
    "gs.best_estimator_\n",
    "##t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import pandas as pd\n",
    "#import lightgbm as lgb\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = \\\n",
    "#    train_test_split(data.iloc[:, 0:-1], # feature\n",
    "#                     data.iloc[:, -1], # label\n",
    "#                     test_size=0.2,#训练集、测试集割比例\n",
    "#                     stratify=data.iloc[:-1],# 这里保证割后y的比例布与原数据一致\n",
    "#                    )\n",
    "#\n",
    "#    gbm = lgb.LGBMClassifier(colsample_bytree=0.7,\n",
    "#                             max_depth=-6,\n",
    "#                             min_child_weight=0.0,)\n",
    "#                     \n",
    "#    gbm.fit(X_train, y_train)\n",
    "#    gbm_y_pre=gbm.predict(x_test)#分类的类别\n",
    "#    gbm_y_proba=gbm.predict_proba(X_test)# 分类的概率值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################　　　自己加入的　　　##############################################\n",
    "#\n",
    "#import matplotlib.pyplot as plt\n",
    "#\n",
    "#from sklearn.metrics import roc_auc_score, roc_curve\n",
    "#\n",
    "#gbm_auc = roc_auc_score(y_test,gbm_y_proba[:,1]) #计算auc\n",
    "#gbm_fpr,gbm_tpr,gbm_threasholds = roc_curve(y_test,gbm_y_proba[:,1])#计算ROC的值\n",
    "#plt.title(\"roc_curve of %s(AUC=%.4f)\" % ('gbm', gbm_auc))\n",
    "#plt.xlabel('Specificity') # specificity= 1 - np.array(gbm_fpr))\n",
    "#plt.ylabel('Sensitivity') # sensitivity = gbm_tpr\n",
    "#plt.plot(list(1 - np.array(gbm_fpr)), gbm_tpr)\n",
    "#plt.gca().invert_xaxis()# 将轴反转\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(colsample_bytree=0.7740467183023685, metric='None',\n",
       "              min_child_samples=395, min_child_weight=0.01, n_estimators=5000,\n",
       "              n_jobs=4, num_leaves=9, random_state=314, reg_alpha=5,\n",
       "              reg_lambda=10, subsample=0.4643892520208455)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import lightgbm as lgb\n",
    "cf = lgb.LGBMRegressor(colsample_bytree=0.7740467183023685, metric='None',\n",
    "               min_child_samples=395, min_child_weight=0.01, n_estimators=5000,\n",
    "               n_jobs=4, num_leaves=9, random_state=314, reg_alpha=5,\n",
    "               reg_lambda=10, subsample=0.4643892520208455)\n",
    "    \n",
    "cf.fit(dataset_train[feature_names].astype(float), dataset_train['rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.010211237977243215"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf.score(*test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "cf2 = RandomForestRegressor(n_estimators=100)\n",
    "cf2.fit(dataset_train[feature_names].astype(float), dataset_train['rank'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split Train Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.DataFrame(zip(cf.feature_importances_, feature_names), \n",
    "                           columns=['Value','Feature']).sort_values('Value', ascending=False)\n",
    "feature_imp\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = dataset.index.get_level_values('date') < '2021'\n",
    "dataset_train = dataset[select]\n",
    "dataset_test = dataset[~select]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_drop = dataset.dropna(subset=feature_names+['return'])\n",
    "\n",
    "vals = model.predict(dataset_drop[feature_names].astype(float))\n",
    "dataset_drop['result1'] = pd.Series(vals.swapaxes(0,1)[0], dataset_drop.index)\n",
    "\n",
    "vals = cf.predict(dataset_drop[feature_names].astype(float))\n",
    "dataset_drop['result2'] = pd.Series(vals, dataset_drop.index)\n",
    "\n",
    "vals = cf2.predict(dataset_drop[feature_names].astype(float))\n",
    "dataset_drop['result3'] = pd.Series(vals, dataset_drop.index)\n",
    "\n",
    "dataset_drop = dataset_drop.reset_index().set_index(\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "dates = sorted(list(set(dataset_drop.index)))\n",
    "\n",
    "rs = []\n",
    "for d in dates:\n",
    "    \n",
    "    dataset_time = dataset_drop.loc[d]\n",
    "    \n",
    "    dataset_time = drop_extreme_case(dataset_time , list1 , thresh=0.01)\n",
    "    \n",
    "    rank = dataset_time['result1'] + dataset_time['result2'] + dataset_time['result3'] \n",
    "    \n",
    "    condition = (rank >= rank.nlargest(20).iloc[-1]) \n",
    "    r = dataset_time['return'][condition].mean()\n",
    "\n",
    "    rs.append(r * (1-3/1000-1.425/1000*2*0.6))\n",
    "\n",
    "rs = pd.Series(rs, index=dates)['2021':].cumprod()\n",
    "\n",
    "s0050 = close['0050']['2021':]\n",
    "\n",
    "pd.DataFrame({'nn strategy return':rs.reindex(s0050.index, method='ffill'), '0050 return':s0050/s0050[0]}).plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#\n",
    "#return_history_1026 = pd.Series(rs, index=dates)['2021':].cumprod()\n",
    "##eq = (gain[hold == 1].mean(axis=1)).fillna(1).cumprod()\n",
    "#\n",
    "#pickle.dump(rs, open('return_history_1026.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyfolio as pf\n",
    "#import pandas as pd\n",
    "#\n",
    "#close.index = close.index.tz_localize(\"Asia/Taipei\")\n",
    "##pf.create_returns_tear_sheet(close['0050'].pct_change())\n",
    "#\n",
    "## 得到 上一個單元的 回測結果\n",
    "#ret = pickle.load(open(\"return_history_1026.pkl\", \"rb\"))\n",
    "#\n",
    "## 將回測報酬率取出來\n",
    "#ret = ret.pct_change().dropna()\n",
    "#ret.index = pd.to_datetime(ret.index).tz_localize('Asia/Taipei')\n",
    "#\n",
    "## 利用pyfolio 比較報酬率\n",
    "#pf.create_returns_tear_sheet(ret, benchmark_rets=close['0050'].pct_change())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 當月持股狀況"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.index.levels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the latest dataset\n",
    "last_date = \"2022-10-15\"#dataset.index.levels[1].max()\n",
    "is_last_date = dataset.index.get_level_values('date') == last_date\n",
    "last_dataset = dataset[is_last_date].copy()\n",
    "\n",
    "\n",
    "last_dataset = drop_extreme_case(last_dataset, list1 , thresh=0.01)\n",
    "\n",
    "\n",
    "# remove NaN testcases\n",
    "last_dataset = last_dataset.dropna(subset=feature_names)\n",
    "\n",
    "# predict\n",
    "\n",
    "vals = model.predict(last_dataset[feature_names].astype(float))\n",
    "last_dataset['result1'] = pd.Series(vals.swapaxes(0,1)[0], last_dataset.index)\n",
    "\n",
    "vals = cf.predict(last_dataset[feature_names].astype(float))\n",
    "last_dataset['result2'] = pd.Series(vals, last_dataset.index)\n",
    "\n",
    "vals = cf2.predict(last_dataset[feature_names].astype(float))\n",
    "last_dataset['result3'] = pd.Series(vals, last_dataset.index)\n",
    "\n",
    "# calculate score\n",
    "\n",
    "rank = last_dataset['result1'] + last_dataset['result2'] + last_dataset['result3']\n",
    "condition = (rank >= rank.nlargest(20).iloc[-1]) \n",
    "\n",
    "# plot rank distribution\n",
    "rank.hist(bins=20)\n",
    "\n",
    "\n",
    "# show the best 20 stocks\n",
    "slist1 = rank[condition].reset_index()['stock_id']\n",
    "\n",
    "#https://keras-cn.readthedocs.io/en/latest/models/model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 平均分配資產於股票之中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close = data.get(\"收盤價\")\n",
    "\n",
    "money = 60000\n",
    "stock_prices = close[rank[condition].reset_index()['stock_id']].iloc[-1]\n",
    "\n",
    "\n",
    "print(\"股票平分張數:\")\n",
    "money / len(stock_prices) / stock_prices / 1000\n"
   ]
  }
 ],
 "metadata": {
  "@deathbeds/ipydrawio": {
   "xml": "<mxfile host=\"17-0536659-02\" modified=\"2022-10-27T03:01:05.740Z\" agent=\"5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\" etag=\"8bODyUWCdQaexky56D9k\" version=\"20.2.8\" type=\"embed\"><diagram id=\"9nsO6hMlLNMTvIbReD-d\" name=\"第1頁\"><mxGraphModel dx=\"1458\" dy=\"721\" grid=\"1\" gridSize=\"10\" guides=\"1\" tooltips=\"1\" connect=\"1\" arrows=\"1\" fold=\"1\" page=\"1\" pageScale=\"1\" pageWidth=\"827\" pageHeight=\"1169\" math=\"0\" shadow=\"0\"><root><mxCell id=\"0\"/><mxCell id=\"1\" parent=\"0\"/><UserObject label=\"Tree Root\" treeRoot=\"1\" id=\"2\"><mxCell style=\"align=center;collapsible=0;container=1;recursiveResize=0;\" parent=\"1\" vertex=\"1\"><mxGeometry x=\"40\" y=\"40\" width=\"120\" height=\"60\" as=\"geometry\"/></mxCell></UserObject></root></mxGraphModel></diagram></mxfile>"
  },
  "kernelspec": {
   "display_name": "finlab",
   "language": "python",
   "name": "finlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
