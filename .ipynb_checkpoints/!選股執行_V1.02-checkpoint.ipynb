{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "# 移除不必要的警告\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 獲取歷史資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from finlab.data import Data\n",
    "#from finlab.ml import fundamental_features\n",
    "#fdf = fundamental_features()\n",
    "\n",
    "data = Data()\n",
    "\n",
    "close = data.get(\"收盤價\")\n",
    "open_ = data.get(\"開盤價\")\n",
    "high = data.get(\"最高價\")\n",
    "low = data.get(\"最低價\")\n",
    "vol = data.get(\"成交股數\")\n",
    "\n",
    "PB = data.get(\"股價淨值比\")\n",
    "pe = data.get(\"本益比\")\n",
    "\n",
    "#bi = data.get(\"營業利益\")\n",
    "\n",
    "\n",
    "#close = data.get_adj(\"收盤價\").round(2)\n",
    "\n",
    "#財務指標\n",
    "rev = data.get(\"當月營收\")\n",
    "l_rev = data.get(\"去年當月營收\")\n",
    "\n",
    "#t123 = data.get('土地')\n",
    "\n",
    "bargin_i=data.get(\"投信買賣超股數\")/data.get(\"成交股數\")\n",
    "bargin_f=data.get(\"外資自營商買賣超股數\")/data.get(\"成交股數\")\n",
    "bargin_s=data.get(\"自營商買賣超股數(自行買賣)\")/data.get(\"成交股數\")\n",
    "#\n",
    "\n",
    "vol=data.get('成交股數')/1000\n",
    "vol_ma5=vol.rolling(5).mean()\n",
    "\n",
    "rev.index = rev.index.shift(5, \"d\")         #每月頻率\n",
    "#周頻率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MFI  = data.talib(\"MFI\")\n",
    "##MFI.tail()\n",
    "#ub,mb,lb =data.talib(\"BBANDS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 營收相關"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################　　　自己加入的　　　##############################################\n",
    "import pandas as pd\n",
    "from finlab.__init__ import talib_all_stock\n",
    "from talib import abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias(n):\n",
    "    return close / close.rolling(n, min_periods=1).mean()\n",
    "\n",
    "def acc(n):\n",
    "    return close.shift(n) / (close.shift(2*n) + close) * 2\n",
    "\n",
    "def rsv(n):\n",
    "    l = close.rolling(n, min_periods=1).min() \n",
    "    h = close.rolling(n, min_periods=1).max()\n",
    "    \n",
    "    return (close - l) / (h - l)\n",
    "\n",
    "def mom(n):\n",
    "    return (rev / rev.shift(1)).shift(n)\n",
    "\n",
    "def yoy(n):\n",
    "    return (rev.shift(n) / rev.shift(12+n)) -1\n",
    "\n",
    "def bi(n):\n",
    "    return bargin_i/bargin_i.rolling(n).mean()\n",
    "\n",
    "def bf(n):\n",
    "    return bargin_f/bargin_f.rolling(n).mean()\n",
    "    \n",
    "def bs(n):\n",
    "    return bargin_s/bargin_s.rolling(n).mean()\n",
    "\n",
    "#-------------------------------------------\n",
    "\n",
    "features = {\n",
    "    'mom1': mom(1),\n",
    "    'mom2': mom(2),\n",
    "    'mom3': mom(3),\n",
    "    'mom4': mom(4),\n",
    "    'mom5': mom(5),\n",
    "    'mom6': mom(6),\n",
    "    'mom7': mom(7),\n",
    "    'mom8': mom(8),\n",
    "    'mom9': mom(9),\n",
    "\n",
    "    \n",
    "    'bias5': bias(5),\n",
    "    'bias10': bias(10),\n",
    "    'bias20': bias(20),\n",
    "    'bias60': bias(60),\n",
    "    'bias120': bias(120),\n",
    "    'bias240': bias(240),\n",
    "    \n",
    "    'acc5': acc(5),\n",
    "    'acc10': acc(10),\n",
    "    'acc20': acc(20),\n",
    "    'acc60': acc(60),\n",
    "    'acc120': acc(120),\n",
    "    'acc240': acc(240),\n",
    "    \n",
    "    'rsv5': rsv(5),\n",
    "    'rsv10': rsv(10),\n",
    "    'rsv20': rsv(20),\n",
    "    'rsv60': rsv(60),\n",
    "    'rsv120': rsv(120),\n",
    "    'rsv240': rsv(240),\n",
    "###############################################\n",
    "    'yoy': yoy(1),\n",
    "    'delta_yoy':yoy(1)-yoy(2),\n",
    "    \n",
    "    'PB':PB,\n",
    "    'PE':pe,\n",
    "    \n",
    "    #'v/ma':vol/vol_ma5,\n",
    "   #'bi5' : bi(5),\n",
    "   #'bi10' : bi(10),\n",
    "   #'bi20' : bi(20),\n",
    "   #'bi60' : bi(60),\n",
    "   # \n",
    "   #'bf5' : bf(5),\n",
    "   # #'bf10' : bf(10),\n",
    "   # #'bf20' : bf(20),\n",
    "   # #'bf60' : bf(60),\n",
    "   # \n",
    "   # 'bs5' : bs(5),\n",
    "   # 'bs10' : bs(10),\n",
    "   # 'bs20' : bs(20),\n",
    "   # 'bs60' : bs(60),\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>stock_id</th>\n",
       "      <th>1101</th>\n",
       "      <th>1102</th>\n",
       "      <th>1103</th>\n",
       "      <th>1104</th>\n",
       "      <th>1107</th>\n",
       "      <th>1108</th>\n",
       "      <th>1109</th>\n",
       "      <th>1110</th>\n",
       "      <th>1201</th>\n",
       "      <th>1203</th>\n",
       "      <th>...</th>\n",
       "      <th>9944</th>\n",
       "      <th>9945</th>\n",
       "      <th>9946</th>\n",
       "      <th>9949</th>\n",
       "      <th>9950</th>\n",
       "      <th>9951</th>\n",
       "      <th>9955</th>\n",
       "      <th>9958</th>\n",
       "      <th>9960</th>\n",
       "      <th>9962</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-02-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-03-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-04-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-05-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-06-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-15</th>\n",
       "      <td>0.084092</td>\n",
       "      <td>-0.106927</td>\n",
       "      <td>0.019662</td>\n",
       "      <td>0.130446</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.284005</td>\n",
       "      <td>0.684562</td>\n",
       "      <td>0.124592</td>\n",
       "      <td>0.007105</td>\n",
       "      <td>0.036142</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111964</td>\n",
       "      <td>0.329245</td>\n",
       "      <td>-0.148815</td>\n",
       "      <td>0.393150</td>\n",
       "      <td>1.249940</td>\n",
       "      <td>0.239243</td>\n",
       "      <td>-0.145710</td>\n",
       "      <td>-0.180520</td>\n",
       "      <td>0.116822</td>\n",
       "      <td>0.003516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-15</th>\n",
       "      <td>0.188504</td>\n",
       "      <td>-0.196042</td>\n",
       "      <td>0.084332</td>\n",
       "      <td>0.136037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.449827</td>\n",
       "      <td>-0.201942</td>\n",
       "      <td>0.304901</td>\n",
       "      <td>0.005675</td>\n",
       "      <td>0.212145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010391</td>\n",
       "      <td>0.178078</td>\n",
       "      <td>-0.673026</td>\n",
       "      <td>0.048772</td>\n",
       "      <td>-0.156177</td>\n",
       "      <td>0.245812</td>\n",
       "      <td>-0.294261</td>\n",
       "      <td>0.004491</td>\n",
       "      <td>0.010218</td>\n",
       "      <td>0.129923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-15</th>\n",
       "      <td>-0.008009</td>\n",
       "      <td>-0.075633</td>\n",
       "      <td>0.121301</td>\n",
       "      <td>0.066769</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.431791</td>\n",
       "      <td>-0.143446</td>\n",
       "      <td>0.234091</td>\n",
       "      <td>-0.024680</td>\n",
       "      <td>0.384607</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037130</td>\n",
       "      <td>0.413395</td>\n",
       "      <td>-0.057254</td>\n",
       "      <td>-0.179187</td>\n",
       "      <td>1.294386</td>\n",
       "      <td>0.322786</td>\n",
       "      <td>-0.276645</td>\n",
       "      <td>0.101934</td>\n",
       "      <td>0.063968</td>\n",
       "      <td>-0.084119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-15</th>\n",
       "      <td>0.162910</td>\n",
       "      <td>-0.113063</td>\n",
       "      <td>0.333061</td>\n",
       "      <td>0.272864</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.309856</td>\n",
       "      <td>-0.336393</td>\n",
       "      <td>-0.092717</td>\n",
       "      <td>-0.099534</td>\n",
       "      <td>0.200326</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.138579</td>\n",
       "      <td>0.619756</td>\n",
       "      <td>-0.773686</td>\n",
       "      <td>-0.041727</td>\n",
       "      <td>1.189093</td>\n",
       "      <td>0.225264</td>\n",
       "      <td>-0.463853</td>\n",
       "      <td>0.165291</td>\n",
       "      <td>-0.309252</td>\n",
       "      <td>0.568746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-15</th>\n",
       "      <td>-0.025629</td>\n",
       "      <td>-0.351210</td>\n",
       "      <td>0.209836</td>\n",
       "      <td>-0.148473</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.091673</td>\n",
       "      <td>-0.474314</td>\n",
       "      <td>-0.173711</td>\n",
       "      <td>-0.050133</td>\n",
       "      <td>-0.085944</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.347208</td>\n",
       "      <td>-0.330394</td>\n",
       "      <td>-0.703003</td>\n",
       "      <td>-0.215372</td>\n",
       "      <td>0.128140</td>\n",
       "      <td>-0.142444</td>\n",
       "      <td>-0.464537</td>\n",
       "      <td>-0.120989</td>\n",
       "      <td>-0.058771</td>\n",
       "      <td>0.330021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>218 rows × 2023 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "stock_id        1101      1102      1103      1104  1107      1108      1109  \\\n",
       "date                                                                           \n",
       "2005-02-15       NaN       NaN       NaN       NaN   NaN       NaN       NaN   \n",
       "2005-03-15       NaN       NaN       NaN       NaN   NaN       NaN       NaN   \n",
       "2005-04-15       NaN       NaN       NaN       NaN   NaN       NaN       NaN   \n",
       "2005-05-15       NaN       NaN       NaN       NaN   NaN       NaN       NaN   \n",
       "2005-06-15       NaN       NaN       NaN       NaN   NaN       NaN       NaN   \n",
       "...              ...       ...       ...       ...   ...       ...       ...   \n",
       "2022-11-15  0.084092 -0.106927  0.019662  0.130446   NaN  0.284005  0.684562   \n",
       "2022-12-15  0.188504 -0.196042  0.084332  0.136037   NaN  0.449827 -0.201942   \n",
       "2023-01-15 -0.008009 -0.075633  0.121301  0.066769   NaN  0.431791 -0.143446   \n",
       "2023-02-15  0.162910 -0.113063  0.333061  0.272864   NaN  0.309856 -0.336393   \n",
       "2023-03-15 -0.025629 -0.351210  0.209836 -0.148473   NaN -0.091673 -0.474314   \n",
       "\n",
       "stock_id        1110      1201      1203  ...      9944      9945      9946  \\\n",
       "date                                      ...                                 \n",
       "2005-02-15       NaN       NaN       NaN  ...       NaN       NaN       NaN   \n",
       "2005-03-15       NaN       NaN       NaN  ...       NaN       NaN       NaN   \n",
       "2005-04-15       NaN       NaN       NaN  ...       NaN       NaN       NaN   \n",
       "2005-05-15       NaN       NaN       NaN  ...       NaN       NaN       NaN   \n",
       "2005-06-15       NaN       NaN       NaN  ...       NaN       NaN       NaN   \n",
       "...              ...       ...       ...  ...       ...       ...       ...   \n",
       "2022-11-15  0.124592  0.007105  0.036142  ...  0.111964  0.329245 -0.148815   \n",
       "2022-12-15  0.304901  0.005675  0.212145  ...  0.010391  0.178078 -0.673026   \n",
       "2023-01-15  0.234091 -0.024680  0.384607  ... -0.037130  0.413395 -0.057254   \n",
       "2023-02-15 -0.092717 -0.099534  0.200326  ... -0.138579  0.619756 -0.773686   \n",
       "2023-03-15 -0.173711 -0.050133 -0.085944  ... -0.347208 -0.330394 -0.703003   \n",
       "\n",
       "stock_id        9949      9950      9951      9955      9958      9960  \\\n",
       "date                                                                     \n",
       "2005-02-15       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2005-03-15       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2005-04-15       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2005-05-15       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2005-06-15       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2022-11-15  0.393150  1.249940  0.239243 -0.145710 -0.180520  0.116822   \n",
       "2022-12-15  0.048772 -0.156177  0.245812 -0.294261  0.004491  0.010218   \n",
       "2023-01-15 -0.179187  1.294386  0.322786 -0.276645  0.101934  0.063968   \n",
       "2023-02-15 -0.041727  1.189093  0.225264 -0.463853  0.165291 -0.309252   \n",
       "2023-03-15 -0.215372  0.128140 -0.142444 -0.464537 -0.120989 -0.058771   \n",
       "\n",
       "stock_id        9962  \n",
       "date                  \n",
       "2005-02-15       NaN  \n",
       "2005-03-15       NaN  \n",
       "2005-04-15       NaN  \n",
       "2005-05-15       NaN  \n",
       "2005-06-15       NaN  \n",
       "...              ...  \n",
       "2022-11-15  0.003516  \n",
       "2022-12-15  0.129923  \n",
       "2023-01-15 -0.084119  \n",
       "2023-02-15  0.568746  \n",
       "2023-03-15  0.330021  \n",
       "\n",
       "[218 rows x 2023 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yoy(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yoy(n):\n",
    "    ## if rev.shift(n) < 0:## or (rev.shift(n) < 0):\n",
    "        return 0\n",
    "   ## else :\n",
    "   ##     return (rev.shift(n) / rev.shift(12+n)) -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bi(60).dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 財報指標"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "兩個feature結合[[連結網址]](https://hahow.in/courses/5b9d3a6dca498a001e917383/discussions/5d18b63eac23d80020ae4ce7)\n",
    "\n",
    "---\n",
    "```python\n",
    "from finlab import ml\n",
    "from finlab.data import Data\n",
    "\n",
    "data = Data()\n",
    "rsi = data.talib(\"RSI\")\n",
    "\n",
    "dataset = ml.fundamental_features()\n",
    "ml.add_feature(dataset, 'RSI', rsi)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python上課\\finlab_II\\finlab\\ml.py:49: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  T3900繼續營業部門稅前純益 = T3900繼續營業部門稅前純益[T3900繼續營業部門稅前純益.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:52: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  T3970經常稅後淨利 = T3970經常稅後淨利[T3970經常稅後淨利.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:55: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  T2000權益總計 =  T2000權益總計[T2000權益總計.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:58: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  T3295營業毛利 =  T3295營業毛利[T3295營業毛利.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:61: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  T3100營業收入淨額 = T3100營業收入淨額[T3100營業收入淨額.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:64: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  T3700營業外收入及支出 = T3700營業外收入及支出[T3700營業外收入及支出.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:72: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  T3910所得稅費用 = T3910所得稅費用[T3910所得稅費用.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:75: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  T3300營業費用 = T3300營業費用[T3300營業費用.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:78: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  T7211折舊 = T7211折舊[T7211折舊.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:81: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  T7212攤提 = T7212攤提[T7212攤提.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:87: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  T0170存貨 = T0170存貨[T0170存貨.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:90: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  股本 = 股本[股本.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:98: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  T0400不動產廠房設備合計 =T0400不動產廠房設備合計[T0400不動產廠房設備合計.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:101: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  T0960非流動資產 = T0960非流動資產[T0960非流動資產.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:104: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  T0180預付費用及預付款 = T0180預付費用及預付款[T0180預付費用及預付款.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:107: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  T1800非流動負債 = T1800非流動負債[T1800非流動負債.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:110: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  T0190其他流動資產 = T0190其他流動資產[T0190其他流動資產.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:113: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  T1100流動負債 = T1100流動負債[T1100流動負債.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:116: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  T0100流動資產 =T0100流動資產[T0100流動資產.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:119: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  T3501財物成本 = T3501財物成本[T3501財物成本.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:125: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  T3971本期綜合損益總額 = T3971本期綜合損益總額[T3971本期綜合損益總額.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:128: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  T3395營業利益 = T3395營業利益[T3395營業利益.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:137: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  R202用人費用率 = R202用人費用率[R202用人費用率.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:140: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  T3356研究發展費 = T3356研究發展費[T3356研究發展費.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:143: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  T7210營運現金流 = T7210營運現金流[T7210營運現金流.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:146: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  T3950歸屬母公司淨利 = T3950歸屬母公司淨利[T3950歸屬母公司淨利.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:149: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  T1000負債總額 =T1000負債總額[T1000負債總額.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:152: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  T3200營業成本 = T3200營業成本[T3200營業成本.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:156: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  T7324取得不動產廠房及設備 = T7324取得不動產廠房及設備[ T7324取得不動產廠房及設備.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:158: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  應收帳款關係人淨額 = 應收帳款關係人淨額[應收帳款關係人淨額.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:161: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  應收票據淨額 = 應收票據淨額[應收票據淨額.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:164: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  應收票據關係人淨額 = 應收票據關係人淨額[應收票據關係人淨額.columns & 收盤價.columns]\n",
      "D:\\python上課\\finlab_II\\finlab\\ml.py:167: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  應收帳款淨額 = 應收帳款淨額[應收帳款淨額.columns & 收盤價.columns]\n"
     ]
    }
   ],
   "source": [
    "from finlab.ml import fundamental_features\n",
    "dataset_fundamental = fundamental_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_fundamental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 技術指標"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# https://zhuanlan.zhihu.com/p/342075180 talib函数功能一览表\n",
    "\n",
    "def bias(n):\n",
    "    return close / close.rolling(n, min_periods=1).mean()\n",
    "\n",
    "def acc(n):\n",
    "    return close.shift(n) / (close.shift(2*n) + close) * 2\n",
    "\n",
    "def rsv(n):\n",
    "    l = close.rolling(n, min_periods=1).min()\n",
    "    h = close.rolling(n, min_periods=1).max()\n",
    "    \n",
    "    return (close - l) / (h - l)\n",
    "\n",
    "def mom(n):\n",
    "    return (rev / rev.shift(1)).shift(n)\n",
    "\n",
    "\n",
    "def bi_(n):\n",
    "    return (bargin_i / vol.shift(1)).shift(n)\n",
    "\n",
    "def bf(n):\n",
    "    return (bargin_f / vol.shift(1)).shift(n)\n",
    "    \n",
    "def bs(n):\n",
    "    return (bargin_s / vol.shift(1)).shift(n)\n",
    "\n",
    "def rsi(n):\n",
    "    #return talib_all_stock(ndays=10000, func=abstract.RSI, timeperiod=n)\n",
    "    return data.talib(\"RSI\",timeperiod=n)\n",
    "\n",
    "def MFI(n):\n",
    "    return data.talib(\"MFI\",timeperiod=n)\n",
    "\n",
    "def obv(n):\n",
    "    return data.talib(\"OBV\",timeperiod=n)\n",
    "\n",
    "\n",
    "\n",
    "features = {\n",
    "    \n",
    "    #'ATR14':data.talib(\"ATR\",timeperiod=14),\n",
    "    #'NATR14':data.talib('NATR',timeperiod=14),\n",
    "    #'TRANGE':data.talib('TRANGE'),\n",
    "    #'Adosc3':data.talib('ADOSC',timeperiod=3),\n",
    "    \n",
    "    #\"MFI5\":MFI(5),\n",
    "    #\"MFI10\":MFI(10),\n",
    "    \n",
    "    #'rsi6': rsi(6),  #DataFrame\n",
    "    #'rsi10': rsi(10),  #DataFrame\n",
    "    #'rsi14': rsi(14),  #DataFrame\n",
    "    #'rsi20': rsi(20),  #DataFrame\n",
    "    #'rsi50': rsi(50),  #DataFrame\n",
    "   \n",
    "    'mom1': mom(1),\n",
    "    'mom2': mom(2),\n",
    "    'mom3': mom(3),\n",
    "    'mom4': mom(4),\n",
    "    'mom5': mom(5),\n",
    "    'mom6': mom(6),\n",
    "    'mom7': mom(7),\n",
    "    'mom8': mom(8),\n",
    "    'mom9': mom(9),\n",
    "    \n",
    "    'yoy': yoy(1),\n",
    "    'delta_yoy':yoy(1)-yoy(2),\n",
    "    \n",
    "#    'ff':ff,\n",
    "    'PB':PB,\n",
    "    'PE':pe,   \n",
    "#  \n",
    "    'bias5': bias(5),\n",
    "    'bias10': bias(10),\n",
    "    'bias20': bias(20),\n",
    "    'bias60': bias(60),\n",
    "    'bias120': bias(120),\n",
    "    'bias240': bias(240),\n",
    "    \n",
    "    'acc5': acc(5),\n",
    "    'acc10': acc(10),\n",
    "    'acc20': acc(20),\n",
    "    'acc60': acc(60),\n",
    "    'acc120': acc(120),\n",
    "    'acc240': acc(240),\n",
    "    \n",
    "    #'rsv5': rsv(5),\n",
    "    #'rsv10': rsv(10),\n",
    "    #'rsv20': rsv(20),\n",
    "    #'rsv60': rsv(60),\n",
    "    #'rsv120': rsv(120),\n",
    "    #'rsv240': rsv(240),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "```\n",
    "https://www.twblogs.net/a/5d3f3173bd9eee517422735f\n",
    "W-WED\n",
    "https://docs.python.org/zh-tw/3/library/calendar.html\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加入其他features\n",
    "* http://finlabcourse.imotor.com/viewthread.php?tid=660&extra=page%3D1\n",
    "\n",
    "```python\n",
    "from finlab import ml\n",
    "from finlab.data import Data\n",
    "\n",
    "data = Data()\n",
    "rsi = data.talib(\"RSI\")\n",
    "\n",
    "dataset = ml.fundamental_features()\n",
    "ml.add_feature(dataset, 'RSI', rsi)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 組合dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認各指標清單"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t1 = data.talib(\"NATR\",timeperiod=14)\n",
    "#t1.to_csv('myfile.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 製作dataset\n",
    "\n",
    "##### 設定買賣頻率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2005-02-15', '2005-03-15', '2005-04-15', '2005-05-15',\n",
       "               '2005-06-15', '2005-07-15', '2005-08-15', '2005-09-15',\n",
       "               '2005-10-15', '2005-11-15',\n",
       "               ...\n",
       "               '2022-06-15', '2022-07-15', '2022-08-15', '2022-09-15',\n",
       "               '2022-10-15', '2022-11-15', '2022-12-15', '2023-01-15',\n",
       "               '2023-02-15', '2023-03-15'],\n",
       "              dtype='datetime64[ns]', name='date', length=218, freq=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rev.index = rev.index.tz_localize(\"Asia/Taipei\")\n",
    "every_month = rev.index\n",
    "every_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 將dataframe 組裝起來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features['bias20'].reindex(every_month, method='ffill')\n",
    "\n",
    "for name, f in features.items():\n",
    "    features[name] = f.reindex(every_month, method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, f in features.items():\n",
    "    features[name] = f.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(dataset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 裝自己要的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finlab import ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "股本 = data.get('股本合計').reindex(close.index, method='ffill')\n",
    "市值 = 股本 * close / 10 * 1000\n",
    "#t1['2330'].dropna()\n",
    "ml.add_feature(dataset, '市值', 市值)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mom1</th>\n",
       "      <th>mom2</th>\n",
       "      <th>mom3</th>\n",
       "      <th>mom4</th>\n",
       "      <th>mom5</th>\n",
       "      <th>mom6</th>\n",
       "      <th>mom7</th>\n",
       "      <th>mom8</th>\n",
       "      <th>mom9</th>\n",
       "      <th>bias5</th>\n",
       "      <th>...</th>\n",
       "      <th>rsv20</th>\n",
       "      <th>rsv60</th>\n",
       "      <th>rsv120</th>\n",
       "      <th>rsv240</th>\n",
       "      <th>yoy</th>\n",
       "      <th>delta_yoy</th>\n",
       "      <th>PB</th>\n",
       "      <th>PE</th>\n",
       "      <th>市值</th>\n",
       "      <th>vol_ma5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stock_id</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0015</th>\n",
       "      <th>2005-02-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-03-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-04-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-05-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-06-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">9962</th>\n",
       "      <th>2022-11-15</th>\n",
       "      <td>0.830797</td>\n",
       "      <td>1.280940</td>\n",
       "      <td>0.641536</td>\n",
       "      <td>1.694666</td>\n",
       "      <td>0.917472</td>\n",
       "      <td>0.678559</td>\n",
       "      <td>1.435387</td>\n",
       "      <td>1.209077</td>\n",
       "      <td>0.859901</td>\n",
       "      <td>1.024242</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.933962</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.003516</td>\n",
       "      <td>-0.722271</td>\n",
       "      <td>1.33</td>\n",
       "      <td>7.01</td>\n",
       "      <td>1.524723e+09</td>\n",
       "      <td>462.1834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-15</th>\n",
       "      <td>1.163303</td>\n",
       "      <td>0.830797</td>\n",
       "      <td>1.280940</td>\n",
       "      <td>0.641536</td>\n",
       "      <td>1.694666</td>\n",
       "      <td>0.917472</td>\n",
       "      <td>0.678559</td>\n",
       "      <td>1.435387</td>\n",
       "      <td>1.209077</td>\n",
       "      <td>1.024939</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.98750</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.435556</td>\n",
       "      <td>0.129923</td>\n",
       "      <td>0.126406</td>\n",
       "      <td>1.32</td>\n",
       "      <td>6.99</td>\n",
       "      <td>1.520212e+09</td>\n",
       "      <td>632.1574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-15</th>\n",
       "      <td>1.004024</td>\n",
       "      <td>1.163303</td>\n",
       "      <td>0.830797</td>\n",
       "      <td>1.280940</td>\n",
       "      <td>0.641536</td>\n",
       "      <td>1.694666</td>\n",
       "      <td>0.917472</td>\n",
       "      <td>0.678559</td>\n",
       "      <td>1.435387</td>\n",
       "      <td>1.004331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.94382</td>\n",
       "      <td>0.957627</td>\n",
       "      <td>0.586667</td>\n",
       "      <td>-0.084119</td>\n",
       "      <td>-0.214041</td>\n",
       "      <td>1.46</td>\n",
       "      <td>7.70</td>\n",
       "      <td>1.673587e+09</td>\n",
       "      <td>3007.9848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-15</th>\n",
       "      <td>1.249495</td>\n",
       "      <td>1.004024</td>\n",
       "      <td>1.163303</td>\n",
       "      <td>0.830797</td>\n",
       "      <td>1.280940</td>\n",
       "      <td>0.641536</td>\n",
       "      <td>1.694666</td>\n",
       "      <td>0.917472</td>\n",
       "      <td>0.678559</td>\n",
       "      <td>1.005587</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.697778</td>\n",
       "      <td>0.568746</td>\n",
       "      <td>0.652864</td>\n",
       "      <td>1.55</td>\n",
       "      <td>8.22</td>\n",
       "      <td>1.786362e+09</td>\n",
       "      <td>932.8926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-15</th>\n",
       "      <td>0.729045</td>\n",
       "      <td>1.249495</td>\n",
       "      <td>1.004024</td>\n",
       "      <td>1.163303</td>\n",
       "      <td>0.830797</td>\n",
       "      <td>1.280940</td>\n",
       "      <td>0.641536</td>\n",
       "      <td>1.694666</td>\n",
       "      <td>0.917472</td>\n",
       "      <td>1.002268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.96875</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.902222</td>\n",
       "      <td>0.330021</td>\n",
       "      <td>-0.238725</td>\n",
       "      <td>1.73</td>\n",
       "      <td>9.17</td>\n",
       "      <td>1.993869e+09</td>\n",
       "      <td>831.0838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>448644 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         mom1      mom2      mom3      mom4      mom5  \\\n",
       "stock_id date                                                           \n",
       "0015     2005-02-15       NaN       NaN       NaN       NaN       NaN   \n",
       "         2005-03-15       NaN       NaN       NaN       NaN       NaN   \n",
       "         2005-04-15       NaN       NaN       NaN       NaN       NaN   \n",
       "         2005-05-15       NaN       NaN       NaN       NaN       NaN   \n",
       "         2005-06-15       NaN       NaN       NaN       NaN       NaN   \n",
       "...                       ...       ...       ...       ...       ...   \n",
       "9962     2022-11-15  0.830797  1.280940  0.641536  1.694666  0.917472   \n",
       "         2022-12-15  1.163303  0.830797  1.280940  0.641536  1.694666   \n",
       "         2023-01-15  1.004024  1.163303  0.830797  1.280940  0.641536   \n",
       "         2023-02-15  1.249495  1.004024  1.163303  0.830797  1.280940   \n",
       "         2023-03-15  0.729045  1.249495  1.004024  1.163303  0.830797   \n",
       "\n",
       "                         mom6      mom7      mom8      mom9     bias5  ...  \\\n",
       "stock_id date                                                          ...   \n",
       "0015     2005-02-15       NaN       NaN       NaN       NaN       NaN  ...   \n",
       "         2005-03-15       NaN       NaN       NaN       NaN       NaN  ...   \n",
       "         2005-04-15       NaN       NaN       NaN       NaN       NaN  ...   \n",
       "         2005-05-15       NaN       NaN       NaN       NaN       NaN  ...   \n",
       "         2005-06-15       NaN       NaN       NaN       NaN       NaN  ...   \n",
       "...                       ...       ...       ...       ...       ...  ...   \n",
       "9962     2022-11-15  0.678559  1.435387  1.209077  0.859901  1.024242  ...   \n",
       "         2022-12-15  0.917472  0.678559  1.435387  1.209077  1.024939  ...   \n",
       "         2023-01-15  1.694666  0.917472  0.678559  1.435387  1.004331  ...   \n",
       "         2023-02-15  0.641536  1.694666  0.917472  0.678559  1.005587  ...   \n",
       "         2023-03-15  1.280940  0.641536  1.694666  0.917472  1.002268  ...   \n",
       "\n",
       "                        rsv20    rsv60    rsv120    rsv240       yoy  \\\n",
       "stock_id date                                                          \n",
       "0015     2005-02-15       NaN      NaN       NaN       NaN       NaN   \n",
       "         2005-03-15       NaN      NaN       NaN       NaN       NaN   \n",
       "         2005-04-15       NaN      NaN       NaN       NaN       NaN   \n",
       "         2005-05-15       NaN      NaN       NaN       NaN       NaN   \n",
       "         2005-06-15       NaN      NaN       NaN       NaN       NaN   \n",
       "...                       ...      ...       ...       ...       ...   \n",
       "9962     2022-11-15  1.000000  1.00000  0.933962  0.440000  0.003516   \n",
       "         2022-12-15  1.000000  0.98750  0.989899  0.435556  0.129923   \n",
       "         2023-01-15  0.895833  0.94382  0.957627  0.586667 -0.084119   \n",
       "         2023-02-15  1.000000  1.00000  1.000000  0.697778  0.568746   \n",
       "         2023-03-15  0.920000  0.96875  0.978723  0.902222  0.330021   \n",
       "\n",
       "                     delta_yoy    PB    PE            市值    vol_ma5  \n",
       "stock_id date                                                        \n",
       "0015     2005-02-15        NaN   NaN   NaN           NaN        NaN  \n",
       "         2005-03-15        NaN   NaN   NaN           NaN        NaN  \n",
       "         2005-04-15        NaN   NaN   NaN           NaN        NaN  \n",
       "         2005-05-15        NaN   NaN   NaN           NaN        NaN  \n",
       "         2005-06-15        NaN   NaN   NaN           NaN        NaN  \n",
       "...                        ...   ...   ...           ...        ...  \n",
       "9962     2022-11-15  -0.722271  1.33  7.01  1.524723e+09   462.1834  \n",
       "         2022-12-15   0.126406  1.32  6.99  1.520212e+09   632.1574  \n",
       "         2023-01-15  -0.214041  1.46  7.70  1.673587e+09  3007.9848  \n",
       "         2023-02-15   0.652864  1.55  8.22  1.786362e+09   932.8926  \n",
       "         2023-03-15  -0.238725  1.73  9.17  1.993869e+09   831.0838  \n",
       "\n",
       "[448644 rows x 33 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml.add_feature(dataset, 'vol_ma5', vol_ma5)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################　　　自己加入的　　　##############################################\n",
    "dataset.index = dataset.index.set_names(['stock_id','date'], level=[0,1])\n",
    "\n",
    "\n",
    "#dataset.index.levels[1].name = 'date'\n",
    "#dataset.index.levels[0].name = 'stock_id'\n",
    "\n",
    "#因為你pandas更新到新版了\n",
    "## profit.index.levels[0].name = 'year'\n",
    "## profit.index.levels[1].name = 'month'\n",
    "#這兩行的語法被棄用，請改成\n",
    "#profit.index=profit.index.set_names('year', level=0)\n",
    "#profit.index=profit.index.set_names('month', level=1)\n",
    "#or profit.index=profit.index.set_names(['year','month'], level=[0,1])\n",
    "#直接一行\n",
    "#就可以了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#list(dataset_fundamental.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataset_fundamental.reindex(dataset.index).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data[組合](https://hahow.in/courses/5b9d3a6dca498a001e917383/discussions/5d18b63eac23d80020ae4ce7)\n",
    "```python\n",
    "new_df = pd.concat([dataset_fundamental['R406_經常利益成長率'],dataset],axis=1).dropna(how='any')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.concat([dataset_fundamental,dataset],axis=1).dropna(how='any')\n",
    "dataset1 = new_df.fillna(method='ffill')#[(new_df.index.get_level_values('stock_id')=='2330')]\n",
    "#dataset = dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#feature_names=list(dataset1.columns)\n",
    "#feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 新增 label(績效/排名)\n",
    " - 定義一下要比績效還是要比排名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\python上課\\finlab_II\\finlab\\data.py:103: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead\n",
      "  all_index = (df.index | item.index).sort_values()\n",
      "D:\\python上課\\finlab_II\\finlab\\data.py:103: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead\n",
      "  all_index = (df.index | item.index).sort_values()\n",
      "D:\\python上課\\finlab_II\\finlab\\data.py:103: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead\n",
      "  all_index = (df.index | item.index).sort_values()\n",
      "D:\\python上課\\finlab_II\\finlab\\data.py:103: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead\n",
      "  all_index = (df.index | item.index).sort_values()\n",
      "D:\\python上課\\finlab_II\\finlab\\data.py:103: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead\n",
      "  all_index = (df.index | item.index).sort_values()\n",
      "D:\\python上課\\finlab_II\\finlab\\data.py:103: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead\n",
      "  all_index = (df.index | item.index).sort_values()\n",
      "D:\\python上課\\finlab_II\\finlab\\data.py:103: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead\n",
      "  all_index = (df.index | item.index).sort_values()\n",
      "D:\\python上課\\finlab_II\\finlab\\data.py:103: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead\n",
      "  all_index = (df.index | item.index).sort_values()\n"
     ]
    }
   ],
   "source": [
    "from finlab import ml\n",
    "\n",
    "ml.add_profit_prediction(dataset)\n",
    "ml.add_rank_prediction(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profit(return) rank\n",
    "predi_target = 'rank'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 刪除太大太小的歷史資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(448644, 35)\n",
      "(388315, 35)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "\n",
    "def drop_extreme_case(dataset, feature_names, thresh=0.01):\n",
    "    \n",
    "    extreme_cases = pd.Series(False, index=dataset.index)\n",
    "    for f in feature_names:\n",
    "        tf = dataset[f]\n",
    "        extreme_cases = extreme_cases | (tf < tf.quantile(thresh)) | (tf > tf.quantile(1-thresh))\n",
    "    dataset = dataset[~extreme_cases]\n",
    "    return dataset\n",
    "\n",
    "dataset_drop_extreme_case = drop_extreme_case(dataset , feature_names , thresh=0.01)\n",
    "\n",
    "print(dataset_drop_extreme_case.shape)\n",
    "\n",
    "##(436774, 25)\n",
    "##(388157, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dropna = dataset_drop_extreme_case.dropna(how='any')\n",
    "dataset_dropna = dataset_dropna.reset_index().set_index(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_drop_extreme_case.index.get_level_values(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################\n",
    "##############################################　　　自己加入的　　　##############################################\n",
    "##################################################################################################################\n",
    "\n",
    "dataset_dropna.index = pd.to_datetime(dataset_dropna.index)\n",
    "dataset_dropna = dataset_dropna.sort_index()\n",
    "\n",
    "#修復＜class ‘numpy.ndarray‘＞　https://blog.csdn.net/lxbin/article/details/114005757"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset_dropna.loc[:'2022-03']\n",
    "dataset_test = dataset_dropna.loc['2022-04':]\n",
    "\n",
    "#date_arr = dataset.index.get_level_values('date') < '2020'\n",
    "#dataset_train = dataset[date_arr]\n",
    "#dataset_test = dataset[~date_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset_train[feature_names] , dataset_train['return'] > 1.02\n",
    "test = dataset_test[feature_names] , dataset_test['return'] > 1.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 機器學習\n",
    " - 目前只有三個，技術指標也要再增加一下feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_names = feature_names1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_train.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKMP_DUPLICATE_LIB_OK\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mSequential()\n\u001b[0;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39madd(layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m100\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m---> 12\u001b[0m                       input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mfeature_names\u001b[49m),),\n\u001b[0;32m     13\u001b[0m                       kernel_initializer\u001b[38;5;241m=\u001b[39minitializers\u001b[38;5;241m.\u001b[39mhe_normal(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)))\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39madd(layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m100\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     15\u001b[0m                       kernel_initializer\u001b[38;5;241m=\u001b[39minitializers\u001b[38;5;241m.\u001b[39mhe_normal(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)))\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39madd(layers\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.7\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_names' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Dense(100, activation='relu',\n",
    "                      input_shape=(len(feature_names),),\n",
    "                      kernel_initializer=initializers.he_normal(seed=0)))\n",
    "model.add(layers.Dense(100, activation='relu',\n",
    "                      kernel_initializer=initializers.he_normal(seed=0)))\n",
    "model.add(layers.Dropout(0.7))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=\"adam\",)\n",
    "\n",
    "print('start fitting')\n",
    "history = model.fit(dataset_train[feature_names], dataset_train[predi_target],\n",
    "                    batch_size=1000,         #1000  #每一个batch的大小\n",
    "                    epochs=225, #225          #迭代次数\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    #validation_data =        #(测试集的输入特征，测试集的标签），\n",
    "                    #validation_split =       # 从测试集中划分多少比例给训练集，\n",
    "                    #validation_freq = 20        #测试的epoch间隔数                     \n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(history.history['val_loss'][1:])\n",
    "plt.plot(history.history['loss'][1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lightgbm Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---cf.fit---\n",
      "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "              importance_type='split', learning_rate=0.1, max_depth=-1,\n",
      "              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
      "              n_estimators=5000, n_jobs=-1, num_leaves=31, objective=None,\n",
      "              random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
      "              subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
      "---cf.score---\n",
      "-0.15228202458828988\n",
      "---predict---\n",
      "[0.18609816 0.25282365 0.34559793 ... 0.35794939 0.40217216 0.11720002]\n"
     ]
    }
   ],
   "source": [
    "##############################################　　　自己加入的　　　##############################################\n",
    "import lightgbm as lgb\n",
    "\n",
    "cf = lgb.LGBMRegressor(n_estimators=5000)\n",
    "\n",
    "print('---cf.fit---')\n",
    "print(cf.fit(*train))\n",
    "print('---cf.score---')\n",
    "print(cf.score(*test))\n",
    "print('---predict---')\n",
    "print(cf.predict(test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 參數優化_1110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import lightgbm\n",
    "\n",
    "fit_params={\"early_stopping_rounds\":30, \n",
    "            \"eval_metric\" : 'auc', \n",
    "            \"eval_set\" : [test],\n",
    "            'eval_names': ['valid'],\n",
    "            'verbose': 100,\n",
    "            'categorical_feature': 'auto'}\n",
    "\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "param_test ={'num_leaves': sp_randint(6, 50), \n",
    "             'min_child_samples': sp_randint(100, 500), \n",
    "             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "             'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
    "             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
    "             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n",
    "\n",
    "#This parameter defines the number of HP points to be tested\n",
    "n_HP_points_to_test = 100\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "#n_estimators is set to a \"large value\". The actual number of trees build will depend on early stopping and 5000 define only the absolute maximum\n",
    "clf = lgb.LGBMRegressor(max_depth=-1, random_state=314, silent=True, metric='None', n_jobs=4, n_estimators=5000) #LGBMClassifier\n",
    "gs = RandomizedSearchCV(\n",
    "    estimator=clf, param_distributions=param_test, \n",
    "    n_iter=n_HP_points_to_test,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    refit=True,\n",
    "    random_state=314,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\orang\\.conda\\envs\\finlab\\lib\\site-packages\\sklearn\\model_selection\\_split.py:442: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=np.int)\n",
      "C:\\Users\\orang\\.conda\\envs\\finlab\\lib\\site-packages\\sklearn\\model_selection\\_split.py:102: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\orang\\.conda\\envs\\finlab\\lib\\site-packages\\sklearn\\model_selection\\_split.py:102: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "C:\\Users\\orang\\.conda\\envs\\finlab\\lib\\site-packages\\sklearn\\model_selection\\_split.py:102: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_mask = np.zeros(_num_samples(X), dtype=np.bool)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid's auc: 0.578606\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid's auc: 0.545011\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid's auc: 0.606459\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.58065\n",
      "Early stopping, best iteration is:\n",
      "[87]\tvalid's auc: 0.583329\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.54163\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid's auc: 0.60767\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.563931\n",
      "Early stopping, best iteration is:\n",
      "[94]\tvalid's auc: 0.565304\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid's auc: 0.549161\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid's auc: 0.629005\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid's auc: 0.582309\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[28]\tvalid's auc: 0.546849\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid's auc: 0.625726\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.571381\n",
      "Early stopping, best iteration is:\n",
      "[122]\tvalid's auc: 0.573393\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.539619\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid's auc: 0.592235\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.573138\n",
      "Early stopping, best iteration is:\n",
      "[158]\tvalid's auc: 0.577872\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid's auc: 0.555047\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.635401\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[46]\tvalid's auc: 0.581854\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid's auc: 0.542755\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.612115\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.576607\n",
      "Early stopping, best iteration is:\n",
      "[90]\tvalid's auc: 0.578844\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.534839\n",
      "Early stopping, best iteration is:\n",
      "[112]\tvalid's auc: 0.538643\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.595804\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[64]\tvalid's auc: 0.577497\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid's auc: 0.544246\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.628481\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.577905\n",
      "Early stopping, best iteration is:\n",
      "[126]\tvalid's auc: 0.580049\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.553781\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.610044\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.583558\n",
      "Early stopping, best iteration is:\n",
      "[107]\tvalid's auc: 0.586125\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid's auc: 0.530729\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.599686\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.58004\n",
      "Early stopping, best iteration is:\n",
      "[157]\tvalid's auc: 0.581549\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid's auc: 0.548616\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.615948\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.578538\n",
      "Early stopping, best iteration is:\n",
      "[112]\tvalid's auc: 0.581535\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.541412\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.592919\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.568406\n",
      "Early stopping, best iteration is:\n",
      "[121]\tvalid's auc: 0.571303\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.553235\n",
      "Early stopping, best iteration is:\n",
      "[163]\tvalid's auc: 0.55732\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid's auc: 0.619724\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.577709\n",
      "[200]\tvalid's auc: 0.583642\n",
      "Early stopping, best iteration is:\n",
      "[170]\tvalid's auc: 0.584175\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid's auc: 0.539258\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.622426\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[63]\tvalid's auc: 0.582816\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.535356\n",
      "Early stopping, best iteration is:\n",
      "[81]\tvalid's auc: 0.541411\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid's auc: 0.600794\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.577347\n",
      "Early stopping, best iteration is:\n",
      "[76]\tvalid's auc: 0.580152\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.541935\n",
      "Early stopping, best iteration is:\n",
      "[78]\tvalid's auc: 0.546418\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.622281\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.577154\n",
      "Early stopping, best iteration is:\n",
      "[105]\tvalid's auc: 0.578274\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[49]\tvalid's auc: 0.544216\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid's auc: 0.61627\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.576574\n",
      "[200]\tvalid's auc: 0.58429\n",
      "Early stopping, best iteration is:\n",
      "[260]\tvalid's auc: 0.587269\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.540696\n",
      "Early stopping, best iteration is:\n",
      "[76]\tvalid's auc: 0.543435\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[29]\tvalid's auc: 0.594073\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.580155\n",
      "Early stopping, best iteration is:\n",
      "[103]\tvalid's auc: 0.581063\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's auc: 0.540203\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.608613\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[59]\tvalid's auc: 0.571611\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[56]\tvalid's auc: 0.546992\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.611623\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.576657\n",
      "Early stopping, best iteration is:\n",
      "[109]\tvalid's auc: 0.578872\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid's auc: 0.54722\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.632791\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.5689\n",
      "Early stopping, best iteration is:\n",
      "[73]\tvalid's auc: 0.570642\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.550322\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.597582\n",
      "Early stopping, best iteration is:\n",
      "[71]\tvalid's auc: 0.598633\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.574298\n",
      "Early stopping, best iteration is:\n",
      "[107]\tvalid's auc: 0.576183\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid's auc: 0.544565\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.624768\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.573928\n",
      "Early stopping, best iteration is:\n",
      "[93]\tvalid's auc: 0.576805\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.542718\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.613846\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[44]\tvalid's auc: 0.562034\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.547543\n",
      "Early stopping, best iteration is:\n",
      "[160]\tvalid's auc: 0.553058\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.628218\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[53]\tvalid's auc: 0.578915\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[44]\tvalid's auc: 0.546826\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.617678\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.576526\n",
      "Early stopping, best iteration is:\n",
      "[81]\tvalid's auc: 0.579977\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid's auc: 0.547989\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.615074\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.575146\n",
      "Early stopping, best iteration is:\n",
      "[134]\tvalid's auc: 0.577396\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.541121\n",
      "Early stopping, best iteration is:\n",
      "[88]\tvalid's auc: 0.544335\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.624907\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.573741\n",
      "Early stopping, best iteration is:\n",
      "[165]\tvalid's auc: 0.579691\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[55]\tvalid's auc: 0.541669\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid's auc: 0.59583\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.569574\n",
      "Early stopping, best iteration is:\n",
      "[108]\tvalid's auc: 0.570912\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.543281\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.593821\n",
      "Early stopping, best iteration is:\n",
      "[80]\tvalid's auc: 0.595935\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[39]\tvalid's auc: 0.575712\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid's auc: 0.543598\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.617507\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.578089\n",
      "Early stopping, best iteration is:\n",
      "[79]\tvalid's auc: 0.579082\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[58]\tvalid's auc: 0.539397\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid's auc: 0.608449\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.565569\n",
      "Early stopping, best iteration is:\n",
      "[107]\tvalid's auc: 0.568238\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.551128\n",
      "Early stopping, best iteration is:\n",
      "[98]\tvalid's auc: 0.553515\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.627583\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.577861\n",
      "[200]\tvalid's auc: 0.585815\n",
      "[300]\tvalid's auc: 0.587978\n",
      "Early stopping, best iteration is:\n",
      "[282]\tvalid's auc: 0.588002\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.545294\n",
      "Early stopping, best iteration is:\n",
      "[121]\tvalid's auc: 0.548493\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[49]\tvalid's auc: 0.597894\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.586303\n",
      "Early stopping, best iteration is:\n",
      "[84]\tvalid's auc: 0.587048\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[49]\tvalid's auc: 0.538091\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's auc: 0.604357\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid's auc: 0.576006\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.553564\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.605667\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.561773\n",
      "Early stopping, best iteration is:\n",
      "[84]\tvalid's auc: 0.56501\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid's auc: 0.548255\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.629797\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.583253\n",
      "Early stopping, best iteration is:\n",
      "[124]\tvalid's auc: 0.584566\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.550085\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's auc: 0.613717\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.579748\n",
      "Early stopping, best iteration is:\n",
      "[128]\tvalid's auc: 0.582206\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[62]\tvalid's auc: 0.536699\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid's auc: 0.60355\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.586512\n",
      "Early stopping, best iteration is:\n",
      "[114]\tvalid's auc: 0.589586\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid's auc: 0.537224\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid's auc: 0.611432\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.562824\n",
      "[200]\tvalid's auc: 0.570986\n",
      "Early stopping, best iteration is:\n",
      "[234]\tvalid's auc: 0.574653\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.548953\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.629813\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid's auc: 0.568981\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.550664\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[29]\tvalid's auc: 0.594071\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.577699\n",
      "Early stopping, best iteration is:\n",
      "[90]\tvalid's auc: 0.579537\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalid's auc: 0.544917\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid's auc: 0.609393\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[57]\tvalid's auc: 0.581343\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.551714\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.624911\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.578545\n",
      "Early stopping, best iteration is:\n",
      "[84]\tvalid's auc: 0.581976\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[63]\tvalid's auc: 0.544693\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.618832\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.582854\n",
      "Early stopping, best iteration is:\n",
      "[89]\tvalid's auc: 0.584237\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid's auc: 0.538453\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid's auc: 0.611627\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.577709\n",
      "[200]\tvalid's auc: 0.582293\n",
      "Early stopping, best iteration is:\n",
      "[170]\tvalid's auc: 0.584241\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[61]\tvalid's auc: 0.547169\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.605437\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[64]\tvalid's auc: 0.577645\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid's auc: 0.543123\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.611296\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid's auc: 0.564049\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid's auc: 0.548684\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.625578\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.5812\n",
      "Early stopping, best iteration is:\n",
      "[78]\tvalid's auc: 0.582489\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.546075\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.611987\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalid's auc: 0.572938\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid's auc: 0.535814\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.603127\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.579454\n",
      "Early stopping, best iteration is:\n",
      "[161]\tvalid's auc: 0.584199\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[55]\tvalid's auc: 0.54369\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[10]\tvalid's auc: 0.611972\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.585383\n",
      "Early stopping, best iteration is:\n",
      "[76]\tvalid's auc: 0.587076\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[59]\tvalid's auc: 0.542356\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid's auc: 0.606939\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.578879\n",
      "Early stopping, best iteration is:\n",
      "[113]\tvalid's auc: 0.580116\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.553\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.620072\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.563198\n",
      "[200]\tvalid's auc: 0.566999\n",
      "Early stopping, best iteration is:\n",
      "[229]\tvalid's auc: 0.571411\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.548677\n",
      "Early stopping, best iteration is:\n",
      "[116]\tvalid's auc: 0.553437\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid's auc: 0.628998\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.575358\n",
      "Early stopping, best iteration is:\n",
      "[125]\tvalid's auc: 0.57658\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid's auc: 0.542987\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid's auc: 0.600763\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.574187\n",
      "Early stopping, best iteration is:\n",
      "[91]\tvalid's auc: 0.575811\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.555632\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.627978\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.581152\n",
      "Early stopping, best iteration is:\n",
      "[73]\tvalid's auc: 0.585191\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.564204\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid's auc: 0.604482\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.581319\n",
      "Early stopping, best iteration is:\n",
      "[124]\tvalid's auc: 0.58294\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.537187\n",
      "Early stopping, best iteration is:\n",
      "[129]\tvalid's auc: 0.540574\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.596899\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.577246\n",
      "Early stopping, best iteration is:\n",
      "[77]\tvalid's auc: 0.579091\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[61]\tvalid's auc: 0.539985\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid's auc: 0.602233\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.576342\n",
      "[200]\tvalid's auc: 0.579455\n",
      "Early stopping, best iteration is:\n",
      "[200]\tvalid's auc: 0.579455\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[44]\tvalid's auc: 0.536592\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.600379\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.577751\n",
      "Early stopping, best iteration is:\n",
      "[82]\tvalid's auc: 0.580816\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.539339\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid's auc: 0.61042\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.57705\n",
      "Early stopping, best iteration is:\n",
      "[70]\tvalid's auc: 0.579719\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.554652\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.625411\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.572772\n",
      "Early stopping, best iteration is:\n",
      "[118]\tvalid's auc: 0.575881\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.547258\n",
      "Early stopping, best iteration is:\n",
      "[108]\tvalid's auc: 0.549794\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.641677\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[52]\tvalid's auc: 0.579293\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[68]\tvalid's auc: 0.542074\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid's auc: 0.608808\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[60]\tvalid's auc: 0.577676\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.54312\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.611255\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.582485\n",
      "Early stopping, best iteration is:\n",
      "[88]\tvalid's auc: 0.585136\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.546882\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.638173\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[50]\tvalid's auc: 0.579142\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.54753\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.613449\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.569809\n",
      "Early stopping, best iteration is:\n",
      "[73]\tvalid's auc: 0.572326\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.54534\n",
      "Early stopping, best iteration is:\n",
      "[127]\tvalid's auc: 0.549606\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.648499\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.567647\n",
      "Early stopping, best iteration is:\n",
      "[159]\tvalid's auc: 0.573908\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.543446\n",
      "Early stopping, best iteration is:\n",
      "[158]\tvalid's auc: 0.549402\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.653138\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.576295\n",
      "[200]\tvalid's auc: 0.583568\n",
      "Early stopping, best iteration is:\n",
      "[252]\tvalid's auc: 0.58538\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.541448\n",
      "Early stopping, best iteration is:\n",
      "[102]\tvalid's auc: 0.542226\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's auc: 0.600195\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[62]\tvalid's auc: 0.56695\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.545008\n",
      "Early stopping, best iteration is:\n",
      "[79]\tvalid's auc: 0.550103\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.628218\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid's auc: 0.581\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.55415\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.632333\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid's auc: 0.573589\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.54266\n",
      "Early stopping, best iteration is:\n",
      "[109]\tvalid's auc: 0.544557\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.606591\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.574073\n",
      "Early stopping, best iteration is:\n",
      "[76]\tvalid's auc: 0.577552\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[44]\tvalid's auc: 0.549452\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.625451\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.581757\n",
      "Early stopping, best iteration is:\n",
      "[79]\tvalid's auc: 0.583658\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.542913\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.628862\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.577213\n",
      "Early stopping, best iteration is:\n",
      "[109]\tvalid's auc: 0.58011\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.538159\n",
      "Early stopping, best iteration is:\n",
      "[78]\tvalid's auc: 0.540383\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.596794\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.577826\n",
      "[200]\tvalid's auc: 0.581335\n",
      "Early stopping, best iteration is:\n",
      "[205]\tvalid's auc: 0.582084\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[29]\tvalid's auc: 0.537123\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[24]\tvalid's auc: 0.601389\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[42]\tvalid's auc: 0.580218\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.541666\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid's auc: 0.612048\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.580697\n",
      "[200]\tvalid's auc: 0.588225\n",
      "Early stopping, best iteration is:\n",
      "[189]\tvalid's auc: 0.589602\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid's auc: 0.547083\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.610825\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.582505\n",
      "Early stopping, best iteration is:\n",
      "[90]\tvalid's auc: 0.585411\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[49]\tvalid's auc: 0.542211\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid's auc: 0.612287\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[48]\tvalid's auc: 0.583341\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's auc: 0.538953\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[10]\tvalid's auc: 0.609643\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[57]\tvalid's auc: 0.583785\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[49]\tvalid's auc: 0.538203\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.606527\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.578557\n",
      "[200]\tvalid's auc: 0.582696\n",
      "Early stopping, best iteration is:\n",
      "[193]\tvalid's auc: 0.583825\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.539758\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid's auc: 0.596197\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[55]\tvalid's auc: 0.576941\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid's auc: 0.54715\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.623778\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid's auc: 0.582766\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[54]\tvalid's auc: 0.54157\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.604952\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.572002\n",
      "Early stopping, best iteration is:\n",
      "[72]\tvalid's auc: 0.576242\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.548583\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.618625\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[44]\tvalid's auc: 0.583354\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid's auc: 0.53618\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.619239\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.581879\n",
      "Early stopping, best iteration is:\n",
      "[124]\tvalid's auc: 0.584973\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.546938\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.629901\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.588606\n",
      "Early stopping, best iteration is:\n",
      "[96]\tvalid's auc: 0.589739\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[55]\tvalid's auc: 0.542164\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalid's auc: 0.613126\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.57461\n",
      "Early stopping, best iteration is:\n",
      "[111]\tvalid's auc: 0.580165\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[44]\tvalid's auc: 0.545543\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.625095\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.572717\n",
      "[200]\tvalid's auc: 0.579191\n",
      "Early stopping, best iteration is:\n",
      "[223]\tvalid's auc: 0.580023\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.568695\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.61695\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.572436\n",
      "Early stopping, best iteration is:\n",
      "[77]\tvalid's auc: 0.575748\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.56428\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.611819\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[64]\tvalid's auc: 0.578945\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.553479\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.637453\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.587414\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.544736\n",
      "Early stopping, best iteration is:\n",
      "[158]\tvalid's auc: 0.554198\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.632075\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[58]\tvalid's auc: 0.581223\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.549418\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.628787\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid's auc: 0.576604\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.555066\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[9]\tvalid's auc: 0.619898\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid's auc: 0.57116\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[25]\tvalid's auc: 0.542385\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\tvalid's auc: 0.615402\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[57]\tvalid's auc: 0.579049\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.552458\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.629399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:  3.1min finished\n",
      "C:\\Users\\orang\\.conda\\envs\\finlab\\lib\\site-packages\\sklearn\\model_selection\\_search.py:794: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.573141\n",
      "Early stopping, best iteration is:\n",
      "[129]\tvalid's auc: 0.575799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "                   estimator=LGBMRegressor(boosting_type='gbdt',\n",
       "                                           class_weight=None,\n",
       "                                           colsample_bytree=1.0,\n",
       "                                           importance_type='split',\n",
       "                                           learning_rate=0.1, max_depth=-1,\n",
       "                                           metric='None', min_child_samples=20,\n",
       "                                           min_child_weight=0.001,\n",
       "                                           min_split_gain=0.0,\n",
       "                                           n_estimators=5000, n_jobs=4,\n",
       "                                           num_leaves=31, objective=None,\n",
       "                                           random_state=314, reg_a...\n",
       "                                        'num_leaves': <scipy.stats._distn_infrastructure.rv_frozen object at 0x00000235EC8CF188>,\n",
       "                                        'reg_alpha': [0, 0.1, 1, 2, 5, 7, 10,\n",
       "                                                      50, 100],\n",
       "                                        'reg_lambda': [0, 0.1, 1, 5, 10, 20, 50,\n",
       "                                                       100],\n",
       "                                        'subsample': <scipy.stats._distn_infrastructure.rv_frozen object at 0x00000235F9FC2F08>},\n",
       "                   pre_dispatch='2*n_jobs', random_state=314, refit=True,\n",
       "                   return_train_score=False, scoring='roc_auc', verbose=True)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(*train, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5707550324527193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None,\n",
       "              colsample_bytree=0.5748561875650441, importance_type='split',\n",
       "              learning_rate=0.1, max_depth=-1, metric='None',\n",
       "              min_child_samples=148, min_child_weight=100.0, min_split_gain=0.0,\n",
       "              n_estimators=5000, n_jobs=4, num_leaves=33, objective=None,\n",
       "              random_state=314, reg_alpha=50, reg_lambda=5, silent=True,\n",
       "              subsample=0.5431696497636938, subsample_for_bin=200000,\n",
       "              subsample_freq=0)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(gs.best_score_)\n",
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#gs.best_estimator_\n",
    "#LGBMClassifier(colsample_bytree=0.6433117836032942, metric='None',\n",
    "#               min_child_samples=224, min_child_weight=1e-05, n_estimators=5000,\n",
    "#               n_jobs=4, num_leaves=20, random_state=314, reg_alpha=10,\n",
    "#               reg_lambda=10, subsample=0.8945613420997809)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##cf = lgb.LGBMClassifier(boosting_type='gbdt', class_weight=None,\n",
    "##               colsample_bytree=0.4467801334566121, importance_type='split',\n",
    "##               learning_rate=0.1, max_depth=-1, metric='None',\n",
    "##               min_child_samples=107, min_child_weight=1, min_split_gain=0.0,\n",
    "##               n_estimators=5000, n_jobs=4, num_leaves=43, objective=None,\n",
    "##               random_state=314, reg_alpha=10, reg_lambda=100, silent=True,\n",
    "##               subsample=0.40294551048668437, subsample_for_bin=200000,\n",
    "##               subsample_freq=0)\n",
    "##\n",
    "##\n",
    "##cf.fit(dataset_train[feature_names],dataset_train['return'] > 1.05, **fit_params)\n",
    "##cf.score(dataset_test[feature_names],dataset_test['return'] > 1.05)\n",
    "##下面那行調整中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.579564\n",
      "Early stopping, best iteration is:\n",
      "[116]\tvalid's auc: 0.582997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.014674144058148262"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##原本的上面那行\n",
    "cf = lgb.LGBMRegressor(boosting_type='gbdt', class_weight=None,\n",
    "                       colsample_bytree=0.541999392404749, importance_type='split',\n",
    "                       learning_rate=0.1, max_depth=-1, metric='None',\n",
    "                       min_child_samples=430, min_child_weight=10.0, min_split_gain=0.0,\n",
    "                       n_estimators=5000, n_jobs=4, num_leaves=44, objective=None,\n",
    "                       random_state=314, reg_alpha=1, reg_lambda=100, silent=True,\n",
    "                       subsample=0.7653033041096617, subsample_for_bin=200000,\n",
    "                       subsample_freq=0)\n",
    "\n",
    "cf.fit(dataset_train[feature_names],dataset_train['return'] > 1.0, **fit_params)\n",
    "cf.score(dataset_test[feature_names],dataset_test['return'] > 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([214, 183, 164, 169, 210, 176, 204, 178, 201, 175, 106, 199, 194,\n",
       "       175, 143, 169, 165, 171, 189, 198, 130, 103,  99, 130, 119, 105,\n",
       "       179, 120, 152, 130, 138])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Value', ylabel='Feature'>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_imp = pd.DataFrame(zip(cf.feature_importances_, feature_names), \n",
    "                           columns=['Value','Feature']).sort_values('Value', ascending=False)\n",
    "feature_imp\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Training until validation scores don't improve for 30 rounds.\n",
    "[100]\tvalid's auc: 0.551389\n",
    "Early stopping, best iteration is:\n",
    "[89]\tvalid's auc: 0.553699\n",
    "0.003857956228475623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(cf.fit(*train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import lightgbm as lgb\n",
    "#gbm = lgb.LGBMClassifier(n_estimators=100, random_state=5, learning_rate=0.01)\n",
    "#gbm.fit(dataset_train[feature_names], dataset_train['return'] > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tqdm\n",
    "#\n",
    "#n = 3\n",
    "#\n",
    "#X = []\n",
    "#y = []\n",
    "#indexes = []\n",
    "#dataset_scaled_x = dataset_scaled[feature_names]\n",
    "#\n",
    "#for i in tqdm.tqdm_notebook(range(0, len(dataset_scaled)-n)):\n",
    "#    X.append(dataset_scaled_x.iloc[i:i+n].values)\n",
    "#    y.append(dataset_scaled['return'].iloc[i+n-1])\n",
    "#    indexes.append(dataset_scaled.index[i+n-1])\n",
    "##dataset_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#X = np.array(X)\n",
    "#y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import lightgbm as lgb\n",
    "#cf = lgb.LGBMRegressor(colsample_bytree=0.7740467183023685, metric='None',\n",
    "#               min_child_samples=395, min_child_weight=0.01, n_estimators=5000,\n",
    "#               n_jobs=4, num_leaves=9, random_state=314, reg_alpha=5,\n",
    "#               reg_lambda=10, subsample=0.4643892520208455)\n",
    "#    \n",
    "#cf.fit(dataset_train[feature_names].astype(float), dataset_train['rank'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "cf2 = RandomForestRegressor(n_estimators=100)\n",
    "cf2.fit(dataset_train[feature_names].astype(float), dataset_train[predi_target])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 參數優化_1110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy.stats import randint as sp_randint \n",
    "#from sklearn.model_selection import RandomizedSearchCV \n",
    "## build a classifier \n",
    "#cf2 = RandomForestRegressor(n_estimators=100) \n",
    "## specify parameters and distributions to sample from \n",
    "#param_dist = {\"max_depth\": [3, None], \n",
    "#              \"max_features\": sp_randint(1, 11), \n",
    "#              \"min_samples_split\": sp_randint(2, 11), \n",
    "#              \"min_samples_leaf\": sp_randint(1, 11), \n",
    "#              \"bootstrap\": [True, False], \n",
    "#              \"criterion\": [\"mse\", \"mae\"]} \n",
    "## run randomized search \n",
    "#n_iter_search = 20 \n",
    "#rs = RandomizedSearchCV(cf2, param_distributions=param_dist, \n",
    "#                                   n_iter=n_iter_search) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rs.fit(dataset_train[features], dataset_train['return'] >1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split Train Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.DataFrame(zip(cf2.feature_importances_, feature_names), \n",
    "                           columns=['Value','Feature']).sort_values('Value', ascending=False)\n",
    "feature_imp\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = dataset.index.get_level_values('date') < '2022-03'\n",
    "dataset_train = dataset[select]\n",
    "dataset_test = dataset[~select]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_drop = dataset.dropna(subset=feature_names+['return'])\n",
    "\n",
    "vals = model.predict(dataset_drop[feature_names].astype(float))\n",
    "dataset_drop['result1'] = pd.Series(vals.swapaxes(0,1)[0], dataset_drop.index)\n",
    "\n",
    "vals = cf.predict(dataset_drop[feature_names].astype(float))\n",
    "dataset_drop['result2'] = pd.Series(vals, dataset_drop.index)\n",
    "\n",
    "vals = cf2.predict(dataset_drop[feature_names].astype(float))\n",
    "dataset_drop['result3'] = pd.Series(vals, dataset_drop.index)\n",
    "\n",
    "dataset_drop = dataset_drop.reset_index().set_index(\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 把量加進來做篩選\n",
    " * https://hahow.in/courses/5b9d3a6dca498a001e917383/shapeussions/60c96f5b018697e8a6131cbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把量加進來\n",
    "vol=data.get('成交股數')/1000\n",
    "vol_ma5=vol.rolling(5).mean()\n",
    "\n",
    "vol_filter=vol_ma5>1000\n",
    "vol_filter=vol_filter[vol_filter].fillna(0)\n",
    "#vol_filter\n",
    "t1 = vol_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#condition2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_filter = t1.reindex(every_month, method='ffill')#.loc['2010-02-15']\n",
    "#vol_filter.loc['2010-02-15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#t1 = vol_ma5.iloc[-1].dropna()\n",
    "#t1.to_csv('./tmp/132.csv')\n",
    "#t1.hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#市值\n",
    "#t1 = (predi_target >= predi_target.nlargest(20).iloc[-1]) * condition_Filter_v2.astype(float)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "vol = 100\n",
    "test_period = '2022-04'\n",
    "\n",
    "dates = sorted(list(set(dataset_drop.index)))\n",
    "\n",
    "rs = []\n",
    "rs_v1 = []\n",
    "rs_v2 = []\n",
    "rs_v3 = []\n",
    "\n",
    "for d in tqdm(dates):\n",
    "    #print(d)\n",
    "    dataset_time = dataset_drop.loc[d]\n",
    "    \n",
    "    dataset_time = drop_extreme_case(dataset_time , feature_names , thresh=0.01)\n",
    "    \n",
    "    #print(dataset_time)\n",
    "    \n",
    "    predi_target = dataset_time['result1'] + dataset_time['result2'] + dataset_time['result3'] \n",
    "    \n",
    "    #predi_target = predi_target * (dataset_time['vol_ma5'] >vol).astype(float)\n",
    "            ###\n",
    "\n",
    "    \n",
    "    condition1 = dataset_time['vol_ma5'] > vol\n",
    "    condition_Filter_v1 = condition1\n",
    "    predi_target_v1 = predi_target * condition_Filter_v1.astype(float)\n",
    "    \n",
    "    condition2 = (1e10 < dataset_time['市值']) < 1e11\n",
    "    \n",
    "    condition_Filter_v2 =  condition1 + condition2\n",
    "    predi_target_v2 = predi_target * condition_Filter_v2.astype(float)\n",
    "    \n",
    "    condition_P = (predi_target >= predi_target.nlargest(20).iloc[-1])\n",
    "    condition_v1 = (predi_target_v1 >= predi_target_v1.nlargest(20).iloc[-1])\n",
    "    condition_v2 = (predi_target_v2 >= predi_target_v2.nlargest(20).iloc[-1])\n",
    "    \n",
    "    predi_target = predi_target * (dataset_time['vol_ma5'] >vol).astype(float)\n",
    "    condition_v3 = (predi_target >= predi_target.nlargest(20).iloc[-1]) & condition_Filter_v2 #.astype(float)\n",
    "    \n",
    "    #print(vol_filter.loc[d])\n",
    "    #print(condition)\n",
    "    \n",
    "    r = dataset_time['return'][condition_P].mean()\n",
    "    rs.append(r * (1-3/1000-1.425/1000*2*0.6))\n",
    "    \n",
    "    r_v1 = dataset_time['return'][condition_v1].mean()\n",
    "    rs_v1.append(r_v1 * (1-3/1000-1.425/1000*2*0.6))    \n",
    "    \n",
    "    r_v2 = dataset_time['return'][condition_v2].mean()\n",
    "    rs_v2.append(r_v2 * (1-3/1000-1.425/1000*2*0.6))    \n",
    "\n",
    "    r_v3 = dataset_time['return'][condition_v3].mean()\n",
    "    rs_v3.append(r_v3 * (1-3/1000-1.425/1000*2*0.6))    \n",
    "        \n",
    "rs = pd.Series(rs, index=dates)[test_period:].cumprod()\n",
    "rs_v1 = pd.Series(rs_v1, index=dates)[test_period:].cumprod() #*\n",
    "rs_v2 = pd.Series(rs_v2, index=dates)[test_period:].cumprod() #*\n",
    "rs_v3 = pd.Series(rs_v3, index=dates)[test_period:].cumprod() #*\n",
    "\n",
    "s0050 = close['0050'][test_period:]\n",
    "\n",
    "pd.DataFrame({'nn strategy return':rs.reindex(s0050.index, method='ffill'),\n",
    "              'nn strategy return_V1(vol>100)':rs_v1.reindex(s0050.index, method='ffill'),\n",
    "              'nn strategy return_V2(vol+market cap)':rs_v2.reindex(s0050.index, method='ffill'),\n",
    "              #'nn strategy return_V3(vol+market cap)':rs_v3.reindex(s0050.index, method='ffill'),\n",
    "              '0050 return':s0050/s0050[0]}).plot()\n",
    "\n",
    "\n",
    "\n",
    "import winsound\n",
    "frequency = 2000\n",
    "duration = 100\n",
    "winsound.Beep(frequency, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#\n",
    "#return_history_1026 = pd.Series(rs, index=dates)['2021':].cumprod()\n",
    "##eq = (gain[hold == 1].mean(axis=1)).fillna(1).cumprod()\n",
    "#\n",
    "#pickle.dump(rs, open('return_history_1026.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyfolio as pf\n",
    "#import pandas as pd\n",
    "#\n",
    "#close.index = close.index.tz_localize(\"Asia/Taipei\")\n",
    "##pf.create_returns_tear_sheet(close['0050'].pct_change())\n",
    "#\n",
    "## 得到 上一個單元的 回測結果\n",
    "#ret = pickle.load(open(\"return_history_1026.pkl\", \"rb\"))\n",
    "#\n",
    "## 將回測報酬率取出來\n",
    "#ret = ret.pct_change().dropna()\n",
    "#ret.index = pd.to_datetime(ret.index).tz_localize('Asia/Taipei')\n",
    "#\n",
    "## 利用pyfolio 比較報酬率\n",
    "#pf.create_returns_tear_sheet(ret, benchmark_rets=close['0050'].pct_change())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 當月持股狀況"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the latest dataset\n",
    "last_date = dataset.index.levels[1].max()#\"2022-10-15\"\n",
    "is_last_date = dataset.index.get_level_values('date') == last_date\n",
    "last_dataset = dataset[is_last_date].copy()\n",
    "\n",
    "\n",
    "last_dataset = drop_extreme_case(last_dataset, feature_names , thresh=0.01)\n",
    "t1 = last_dataset\n",
    "\n",
    "# remove NaN testcases\n",
    "last_dataset = last_dataset.dropna(subset=feature_names)\n",
    "\n",
    "# predict\n",
    "\n",
    "vals = model.predict(last_dataset[feature_names].astype(float))\n",
    "last_dataset['result1'] = pd.Series(vals.swapaxes(0,1)[0], last_dataset.index)\n",
    "\n",
    "vals = cf.predict(last_dataset[feature_names].astype(float))\n",
    "last_dataset['result2'] = pd.Series(vals, last_dataset.index)\n",
    "\n",
    "vals = cf2.predict(last_dataset[feature_names].astype(float))\n",
    "last_dataset['result3'] = pd.Series(vals, last_dataset.index)\n",
    "\n",
    "\n",
    "# calculate score\n",
    "\n",
    "predi_target = last_dataset['result1'] + last_dataset['result2'] + last_dataset['result3']\n",
    "\n",
    "#\n",
    "##predi_target = predi_target * vol_filter.iloc[-1] #******加上量的濾網\n",
    "\n",
    "\n",
    "predi_target = predi_target * (last_dataset['vol_ma5'] >vol).astype(float)  #排除平均成交量小於MA1000張\n",
    "#predi_target = predi_target * (last_dataset['市值'] < 1e10).astype(float)   #排除市值小於100億以下的股票\n",
    "\n",
    "condition0 = (predi_target >= predi_target.nlargest(20).iloc[-1])\n",
    "condition1 = last_dataset['vol_ma5'] >vol\n",
    "condition2 = last_dataset['市值'] < 1e10\n",
    "\n",
    "condition = condition0 & condition1 #& condition2\n",
    "\n",
    "#vol_filter\n",
    "\n",
    "# plot rank distribution\n",
    "predi_target[predi_target!=0].hist(bins=20)\n",
    "\n",
    "\n",
    "# show the best 20 stocks\n",
    "slist1 = predi_target[condition].reset_index()['stock_id']\n",
    "\n",
    "#https://keras-cn.readthedocs.io/en/latest/models/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date = dataset.index.levels[1].max()#\"2022-10-15\"\n",
    "is_last_date = dataset.index.get_level_values('date') == last_date\n",
    "last_dataset = dataset[is_last_date].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank.sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 平均分配資產於股票之中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close = data.get(\"收盤價\")\n",
    "\n",
    "money = 150000\n",
    "stock_prices = close[slist1].iloc[-1]\n",
    "\n",
    "\n",
    "print(\"股票平分張數:\")\n",
    "money / len(stock_prices) / stock_prices / 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_dataset['市值'][slist1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_dataset['vol_ma5'][slist1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################################################################################\n",
    "## 移動窗格最佳化\n",
    "[有空來試試看連結](https://hahow.in/courses/5b9d3a6dca498a001e917383/discussions/61b4c90147843d0006cf2593https://hahow.in/courses/5b9d3a6dca498a001e917383/discussions/61b4c90147843d0006cf2593)\n",
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def select(df):\n",
    "\n",
    "    rank = df['pre']\n",
    "\n",
    "    condition1 = (rank >= rank.nlargest(1).iloc[-1])\n",
    "\n",
    "    return df['return'][condition1].mean() * (1-3/1000-1.425/1000*2*0.6)\n",
    "\n",
    "end = 5\n",
    "\n",
    "cf = lgb.LGBMRegressor(n_estimators=500)\n",
    "\n",
    "\n",
    "\n",
    "train_time = ['2015','2016','2017','2018','2019']\n",
    "\n",
    "s_time = ['2007','2008','2009','2010','2011']\n",
    "\n",
    "test_time = ['2016','2017','2018','2019','2020']\n",
    "\n",
    "#dataset_copy = dataset_dropna.copy()\n",
    "\n",
    "store_mse = []\n",
    "\n",
    "\n",
    "for time in range(end):\n",
    "\n",
    "    print('%d 次執行中'%(time))\n",
    "\n",
    "    dataset_dropna2_train = dataset_copy.loc[s_time[time]:train_time[time]] #2007~ 2015   2008~2016   2009~2017  2010~2018  ....\n",
    "\n",
    "    dataset_dropna2_test = dataset_copy.loc[test_time[time]:test_time[time]]#            2016                2017               2018              2019  .....\n",
    "\n",
    "    \n",
    " cf.fit(dataset_dropna2_train[feature_names].astype(float), dataset_dropna2_train['rank'])\n",
    "\n",
    "    \n",
    " predict = cf.predict(dataset_dropna2_test[feature_names])\n",
    "\n",
    "    dataset_dropna2_test['pre'] = predict\n",
    "\n",
    "dates = dataset_dropna2_test.index.get_level_values('date')\n",
    "\n",
    "b = dataset_dropna2_test.groupby(dates).apply(select).cumprod()\n",
    "\n",
    "s0050 = close['0050'][test_time[time]:test_time[time]]\n",
    "\n",
    "s0056 = close['0056'][test_time[time]:test_time[time]]\n",
    "\n",
    "pd.DataFrame({'Best 1 stocks return(include handling fee)':b.reindex(s0050.index, method='ffill'), \n",
    "\n",
    "              '0050':s0050/s0050[0],'0056':s0056/s0056[0]}).plot()\n",
    "\n",
    "plt.ylabel('return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mystrategy(data):\n",
    "\n",
    "    close = data.get(\"收盤價\", 120)\n",
    "\n",
    "    vol = data.get('成交股數', 120)\n",
    " \n",
    "\n",
    "    condition1 = close.max() / close.min()\n",
    "\n",
    "    rev = data.get(\"當月營收\", 14)\n",
    "\n",
    "    condition2 = (rev.iloc[-1] / rev.iloc[-13])\n",
    "\n",
    "    condition3 = (rev.iloc[-2] / rev.iloc[-14])\n",
    "\n",
    "    稅後淨利 = data.get('本期淨利（淨損）', 5)\n",
    "\n",
    "    # 股東權益，有兩個名稱，有些公司叫做權益總計，有些叫做權益總額，所以得把它們抓出來\n",
    "\n",
    "    權益總計 = data.get('權益總計', 5)\n",
    "\n",
    "    權益總額 = data.get('權益總額', 5)\n",
    "\n",
    "    # 並且把它們合併起來\n",
    "\n",
    "    權益總計.fillna(權益總額, inplace=True)\n",
    "\n",
    "    roe = 稅後淨利 / 權益總計\n",
    "\n",
    "    condition4 = roe.iloc[-1] / roe.iloc[-5]\n",
    "\n",
    "    select_stock = (condition1.rank() + # 該數值越小越好\n",
    "\n",
    "    condition2.rank(ascending=False) + # 該值越大越好 \n",
    "\n",
    "    condition3.rank(ascending=False) + # 該值越大越好\n",
    "\n",
    "    condition4.rank(ascending=False) # 該值越大越好 \n",
    "\n",
    "    ).rank() <= 10\n",
    "    \n",
    "    df=select_stock[select_stock]\n",
    "    vol=vol[df.index].iloc[-1]\n",
    "    cond_vol=vol>(1000*1000)\n",
    "\n",
    "    return cond_vol[cond_vol]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = mystrategy(data)\n",
    "stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    權益總計 = data.get('權益總計', 5)\n",
    "\n",
    "    權益總額 = data.get('權益總額', 5)\n",
    "\n",
    "    # 並且把它們合併起來\n",
    "\n",
    "    權益總計.fillna(權益總額, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "權益總計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close = data.get(\"收盤價\", 120)\n",
    "vol = data.get('成交股數', 120)\n",
    "condition1 = close.max() / close.min()\n",
    "rev = data.get(\"當月營收\", 14)\n",
    "condition2 = (rev.iloc[-1] / rev.iloc[-13])\n",
    "condition3 = (rev.iloc[-2] / rev.iloc[-14])\n",
    "稅後淨利 = data.get('本期淨利（淨損）', 5)\n",
    "# 股東權益，有兩個名稱，有些公司叫做權益總計，有些叫做權益總額，所以得把它們抓出來\n",
    "權益總計 = data.get('權益總計', 5)\n",
    "權益總額 = data.get('權益總額', 5)\n",
    "# 並且把它們合併起來\n",
    "權益總計.fillna(權益總額, inplace=True)\n",
    "roe = 稅後淨利 / 權益總計\n",
    "condition4 = roe.iloc[-1] / roe.iloc[-5]\n",
    "select_stock = (condition1.rank() + # 該數值越小越好\n",
    "condition2.rank(ascending=False) + # 該值越大越好 \n",
    "condition3.rank(ascending=False) + # 該值越大越好\n",
    "condition4.rank(ascending=False) # 該值越大越好 \n",
    ").rank() <= 10\n",
    "df=select_stock[select_stock]\n",
    "vol=vol[df.index].iloc[-1]\n",
    "cond_vol=vol>(1000*1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@deathbeds/ipydrawio": {
   "xml": "<mxfile host=\"17-0536659-02\" modified=\"2022-10-27T03:01:05.740Z\" agent=\"5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\" etag=\"8bODyUWCdQaexky56D9k\" version=\"20.2.8\" type=\"embed\"><diagram id=\"9nsO6hMlLNMTvIbReD-d\" name=\"第1頁\"><mxGraphModel dx=\"1458\" dy=\"721\" grid=\"1\" gridSize=\"10\" guides=\"1\" tooltips=\"1\" connect=\"1\" arrows=\"1\" fold=\"1\" page=\"1\" pageScale=\"1\" pageWidth=\"827\" pageHeight=\"1169\" math=\"0\" shadow=\"0\"><root><mxCell id=\"0\"/><mxCell id=\"1\" parent=\"0\"/><UserObject label=\"Tree Root\" treeRoot=\"1\" id=\"2\"><mxCell style=\"align=center;collapsible=0;container=1;recursiveResize=0;\" parent=\"1\" vertex=\"1\"><mxGeometry x=\"40\" y=\"40\" width=\"120\" height=\"60\" as=\"geometry\"/></mxCell></UserObject></root></mxGraphModel></diagram></mxfile>"
  },
  "kernelspec": {
   "display_name": "finlab_N",
   "language": "python",
   "name": "finlab_n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
