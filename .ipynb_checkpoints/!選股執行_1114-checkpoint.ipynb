{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 移除不必要的警告\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 獲取歷史資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from finlab.data import Data\n",
    "from finlab.ml import fundamental_features\n",
    "fdf = fundamental_features()\n",
    "\n",
    "data = Data()\n",
    "\n",
    "close = data.get(\"收盤價\")\n",
    "open_ = data.get(\"開盤價\")\n",
    "high = data.get(\"最高價\")\n",
    "low = data.get(\"最低價\")\n",
    "vol = data.get(\"成交股數\")\n",
    "\n",
    "PB = data.get(\"股價淨值比\")\n",
    "pe = data.get(\"本益比\")\n",
    "\n",
    "#close = data.get_adj(\"收盤價\").round(2)\n",
    "\n",
    "#財務指標\n",
    "rev = data.get(\"當月營收\")\n",
    "l_rev = data.get(\"去年當月營收\")\n",
    "\n",
    "#t123 = data.get('土地')\n",
    "\n",
    "#bargin_i=data.get(\"投信買賣超股數\")\n",
    "#bargin_f=data.get(\"外資自營商買賣超股數\")\n",
    "#bargin_s=data.get(\"自營商買賣超股數(自行買賣)\")\n",
    "#\n",
    "\n",
    "rev.index = rev.index.shift(5, \"d\")         #每月頻率\n",
    "#周頻率"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "```\n",
    "https://www.twblogs.net/a/5d3f3173bd9eee517422735f\n",
    "W-WED\n",
    "https://docs.python.org/zh-tw/3/library/calendar.html\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MFI  = data.talib(\"MFI\")\n",
    "##MFI.tail()\n",
    "#ub,mb,lb =data.talib(\"BBANDS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 營收相關"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################　　　自己加入的　　　##############################################\n",
    "import pandas as pd\n",
    "from finlab.__init__ import talib_all_stock\n",
    "from talib import abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias(n):\n",
    "    return close / close.rolling(n, min_periods=1).mean()\n",
    "\n",
    "def acc(n):\n",
    "    return close.shift(n) / (close.shift(2*n) + close) * 2\n",
    "\n",
    "def rsv(n):\n",
    "    l = close.rolling(n, min_periods=1).min()\n",
    "    h = close.rolling(n, min_periods=1).max()\n",
    "    \n",
    "    return (close - l) / (h - l)\n",
    "\n",
    "def mom(n):\n",
    "    return (rev / rev.shift(1)).shift(n)\n",
    "\n",
    "def yoy(n):\n",
    "    return (rev.shift(n) / rev.shift(12+n)) -1\n",
    "\n",
    "\n",
    "features = {\n",
    "    'mom1': mom(1),\n",
    "    'mom2': mom(2),\n",
    "    'mom3': mom(3),\n",
    "    'mom4': mom(4),\n",
    "    'mom5': mom(5),\n",
    "    'mom6': mom(6),\n",
    "    'mom7': mom(7),\n",
    "    'mom8': mom(8),\n",
    "    'mom9': mom(9),\n",
    "    \n",
    "    'bias5': bias(5),\n",
    "    'bias10': bias(10),\n",
    "    'bias20': bias(20),\n",
    "    'bias60': bias(60),\n",
    "    'bias120': bias(120),\n",
    "    'bias240': bias(240),\n",
    "    \n",
    "    'acc5': acc(5),\n",
    "    'acc10': acc(10),\n",
    "    'acc20': acc(20),\n",
    "    'acc60': acc(60),\n",
    "    'acc120': acc(120),\n",
    "    'acc240': acc(240),\n",
    "    \n",
    "    'rsv5': rsv(5),\n",
    "    'rsv10': rsv(10),\n",
    "    'rsv20': rsv(20),\n",
    "    'rsv60': rsv(60),\n",
    "    'rsv120': rsv(120),\n",
    "    'rsv240': rsv(240),\n",
    "###############################################\n",
    "    'yoy': yoy(1),\n",
    "    'delta_yoy':(yoy(1)/yoy(2))-1,\n",
    "    \n",
    "    'PB':PB,\n",
    "    'PE':pe,       \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 財報指標"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "兩個feature結合\n",
    "[https://hahow.in/courses/5b9d3a6dca498a001e917383/discussions/5d18b63eac23d80020ae4ce7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finlab.ml import fundamental_features\n",
    "dataset_fundamental = fundamental_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>T3395營業利益</th>\n",
       "      <th>T7210營運現金流</th>\n",
       "      <th>T3950歸屬母公司淨利</th>\n",
       "      <th>T7211折舊</th>\n",
       "      <th>T0100流動資產</th>\n",
       "      <th>T1100流動負債</th>\n",
       "      <th>T7324取得不動產廠房及設備</th>\n",
       "      <th>T3970經常稅後淨利</th>\n",
       "      <th>R101_ROA稅後息前</th>\n",
       "      <th>R11V_ROA綜合損益</th>\n",
       "      <th>...</th>\n",
       "      <th>R409_淨值成長率</th>\n",
       "      <th>R501_流動比率</th>\n",
       "      <th>R502_速動比率</th>\n",
       "      <th>R503_利息支出率</th>\n",
       "      <th>R678_營運資金</th>\n",
       "      <th>R607_總資產週轉次數</th>\n",
       "      <th>R610_存貨週轉率</th>\n",
       "      <th>R612_固定資產週轉次數</th>\n",
       "      <th>R613_淨值週轉率次</th>\n",
       "      <th>R69B_自由現金流量</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stock_id</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1101</th>\n",
       "      <th>2013-05-15</th>\n",
       "      <td>2026729.0</td>\n",
       "      <td>2274053.0</td>\n",
       "      <td>1911110.0</td>\n",
       "      <td>1571884.0</td>\n",
       "      <td>77246355.0</td>\n",
       "      <td>65913019.0</td>\n",
       "      <td>-211456.0</td>\n",
       "      <td>1999624.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>117.194382</td>\n",
       "      <td>83.137456</td>\n",
       "      <td>-8.186859</td>\n",
       "      <td>11333336.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-08-14</th>\n",
       "      <td>4588992.0</td>\n",
       "      <td>6029953.0</td>\n",
       "      <td>949640.0</td>\n",
       "      <td>1543041.0</td>\n",
       "      <td>77139390.0</td>\n",
       "      <td>68971246.0</td>\n",
       "      <td>-1033541.0</td>\n",
       "      <td>3802105.0</td>\n",
       "      <td>-1.233843</td>\n",
       "      <td>0.944284</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111.842825</td>\n",
       "      <td>79.609613</td>\n",
       "      <td>-13.298556</td>\n",
       "      <td>8168144.0</td>\n",
       "      <td>0.104927</td>\n",
       "      <td>2.279272</td>\n",
       "      <td>0.258545</td>\n",
       "      <td>0.201496</td>\n",
       "      <td>9467257.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-11-14</th>\n",
       "      <td>5910634.0</td>\n",
       "      <td>5168764.0</td>\n",
       "      <td>5025600.0</td>\n",
       "      <td>1605424.0</td>\n",
       "      <td>77212043.0</td>\n",
       "      <td>73733391.0</td>\n",
       "      <td>-470935.0</td>\n",
       "      <td>5344662.0</td>\n",
       "      <td>-1.130208</td>\n",
       "      <td>2.481737</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104.717879</td>\n",
       "      <td>83.502774</td>\n",
       "      <td>-17.083477</td>\n",
       "      <td>3478652.0</td>\n",
       "      <td>0.110355</td>\n",
       "      <td>2.335831</td>\n",
       "      <td>0.272965</td>\n",
       "      <td>0.211719</td>\n",
       "      <td>12033719.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-03-31</th>\n",
       "      <td>5249620.0</td>\n",
       "      <td>8500835.0</td>\n",
       "      <td>6450131.0</td>\n",
       "      <td>2118671.0</td>\n",
       "      <td>87788055.0</td>\n",
       "      <td>70398494.0</td>\n",
       "      <td>-1089134.0</td>\n",
       "      <td>3972292.0</td>\n",
       "      <td>-0.339410</td>\n",
       "      <td>3.102975</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>124.701609</td>\n",
       "      <td>102.946688</td>\n",
       "      <td>-32.624816</td>\n",
       "      <td>17389561.0</td>\n",
       "      <td>0.119738</td>\n",
       "      <td>2.763869</td>\n",
       "      <td>0.302962</td>\n",
       "      <td>0.222264</td>\n",
       "      <td>-6823797.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-05-15</th>\n",
       "      <td>3684224.0</td>\n",
       "      <td>6895832.0</td>\n",
       "      <td>4015175.0</td>\n",
       "      <td>1594505.0</td>\n",
       "      <td>92121458.0</td>\n",
       "      <td>74388787.0</td>\n",
       "      <td>-940633.0</td>\n",
       "      <td>2791235.0</td>\n",
       "      <td>-1.732411</td>\n",
       "      <td>1.630076</td>\n",
       "      <td>...</td>\n",
       "      <td>8.866880</td>\n",
       "      <td>123.837828</td>\n",
       "      <td>101.776758</td>\n",
       "      <td>-9.792979</td>\n",
       "      <td>17732671.0</td>\n",
       "      <td>0.093998</td>\n",
       "      <td>2.191306</td>\n",
       "      <td>0.245688</td>\n",
       "      <td>0.171327</td>\n",
       "      <td>4901868.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">9962</th>\n",
       "      <th>2021-05-15</th>\n",
       "      <td>8016.0</td>\n",
       "      <td>149610.0</td>\n",
       "      <td>6847.0</td>\n",
       "      <td>5027.0</td>\n",
       "      <td>699386.0</td>\n",
       "      <td>76254.0</td>\n",
       "      <td>-3081.0</td>\n",
       "      <td>6607.0</td>\n",
       "      <td>0.618888</td>\n",
       "      <td>0.636208</td>\n",
       "      <td>...</td>\n",
       "      <td>1.131801</td>\n",
       "      <td>917.179427</td>\n",
       "      <td>271.866394</td>\n",
       "      <td>1.005915</td>\n",
       "      <td>623132.0</td>\n",
       "      <td>0.380922</td>\n",
       "      <td>0.726929</td>\n",
       "      <td>1.533102</td>\n",
       "      <td>0.435345</td>\n",
       "      <td>37994.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-14</th>\n",
       "      <td>40404.0</td>\n",
       "      <td>-155514.0</td>\n",
       "      <td>41534.0</td>\n",
       "      <td>4813.0</td>\n",
       "      <td>976044.0</td>\n",
       "      <td>274123.0</td>\n",
       "      <td>-2374.0</td>\n",
       "      <td>37397.0</td>\n",
       "      <td>3.243048</td>\n",
       "      <td>3.596651</td>\n",
       "      <td>...</td>\n",
       "      <td>7.003658</td>\n",
       "      <td>356.060601</td>\n",
       "      <td>110.054975</td>\n",
       "      <td>0.178902</td>\n",
       "      <td>701921.0</td>\n",
       "      <td>0.594034</td>\n",
       "      <td>1.150951</td>\n",
       "      <td>2.593983</td>\n",
       "      <td>0.710227</td>\n",
       "      <td>-34385.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-14</th>\n",
       "      <td>48861.0</td>\n",
       "      <td>-435827.0</td>\n",
       "      <td>42439.0</td>\n",
       "      <td>4855.0</td>\n",
       "      <td>1420059.0</td>\n",
       "      <td>671731.0</td>\n",
       "      <td>-4677.0</td>\n",
       "      <td>40088.0</td>\n",
       "      <td>2.691891</td>\n",
       "      <td>2.839898</td>\n",
       "      <td>...</td>\n",
       "      <td>11.206472</td>\n",
       "      <td>211.402928</td>\n",
       "      <td>27.425413</td>\n",
       "      <td>0.432543</td>\n",
       "      <td>748328.0</td>\n",
       "      <td>0.428734</td>\n",
       "      <td>0.649981</td>\n",
       "      <td>2.435492</td>\n",
       "      <td>0.635698</td>\n",
       "      <td>2993.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-31</th>\n",
       "      <td>66532.0</td>\n",
       "      <td>74308.0</td>\n",
       "      <td>57469.0</td>\n",
       "      <td>4541.0</td>\n",
       "      <td>1413495.0</td>\n",
       "      <td>581642.0</td>\n",
       "      <td>-8826.0</td>\n",
       "      <td>56074.0</td>\n",
       "      <td>3.352989</td>\n",
       "      <td>3.383973</td>\n",
       "      <td>...</td>\n",
       "      <td>15.804715</td>\n",
       "      <td>243.018042</td>\n",
       "      <td>39.386083</td>\n",
       "      <td>1.907177</td>\n",
       "      <td>831853.0</td>\n",
       "      <td>0.503416</td>\n",
       "      <td>0.661852</td>\n",
       "      <td>3.250884</td>\n",
       "      <td>0.808212</td>\n",
       "      <td>-14303.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-15</th>\n",
       "      <td>44919.0</td>\n",
       "      <td>304147.0</td>\n",
       "      <td>39436.0</td>\n",
       "      <td>4397.0</td>\n",
       "      <td>1217003.0</td>\n",
       "      <td>352094.0</td>\n",
       "      <td>-1292.0</td>\n",
       "      <td>37016.0</td>\n",
       "      <td>2.371942</td>\n",
       "      <td>2.484802</td>\n",
       "      <td>...</td>\n",
       "      <td>19.138403</td>\n",
       "      <td>345.647185</td>\n",
       "      <td>85.204235</td>\n",
       "      <td>2.087938</td>\n",
       "      <td>864909.0</td>\n",
       "      <td>0.525429</td>\n",
       "      <td>0.758426</td>\n",
       "      <td>3.189613</td>\n",
       "      <td>0.753799</td>\n",
       "      <td>9429.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65527 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     T3395營業利益  T7210營運現金流  T3950歸屬母公司淨利    T7211折舊  \\\n",
       "stock_id date                                                         \n",
       "1101     2013-05-15  2026729.0   2274053.0     1911110.0  1571884.0   \n",
       "         2013-08-14  4588992.0   6029953.0      949640.0  1543041.0   \n",
       "         2013-11-14  5910634.0   5168764.0     5025600.0  1605424.0   \n",
       "         2014-03-31  5249620.0   8500835.0     6450131.0  2118671.0   \n",
       "         2014-05-15  3684224.0   6895832.0     4015175.0  1594505.0   \n",
       "...                        ...         ...           ...        ...   \n",
       "9962     2021-05-15     8016.0    149610.0        6847.0     5027.0   \n",
       "         2021-08-14    40404.0   -155514.0       41534.0     4813.0   \n",
       "         2021-11-14    48861.0   -435827.0       42439.0     4855.0   \n",
       "         2022-03-31    66532.0     74308.0       57469.0     4541.0   \n",
       "         2022-05-15    44919.0    304147.0       39436.0     4397.0   \n",
       "\n",
       "                      T0100流動資產   T1100流動負債  T7324取得不動產廠房及設備  T3970經常稅後淨利  \\\n",
       "stock_id date                                                               \n",
       "1101     2013-05-15  77246355.0  65913019.0        -211456.0    1999624.0   \n",
       "         2013-08-14  77139390.0  68971246.0       -1033541.0    3802105.0   \n",
       "         2013-11-14  77212043.0  73733391.0        -470935.0    5344662.0   \n",
       "         2014-03-31  87788055.0  70398494.0       -1089134.0    3972292.0   \n",
       "         2014-05-15  92121458.0  74388787.0        -940633.0    2791235.0   \n",
       "...                         ...         ...              ...          ...   \n",
       "9962     2021-05-15    699386.0     76254.0          -3081.0       6607.0   \n",
       "         2021-08-14    976044.0    274123.0          -2374.0      37397.0   \n",
       "         2021-11-14   1420059.0    671731.0          -4677.0      40088.0   \n",
       "         2022-03-31   1413495.0    581642.0          -8826.0      56074.0   \n",
       "         2022-05-15   1217003.0    352094.0          -1292.0      37016.0   \n",
       "\n",
       "                     R101_ROA稅後息前  R11V_ROA綜合損益  ...  R409_淨值成長率   R501_流動比率  \\\n",
       "stock_id date                                    ...                           \n",
       "1101     2013-05-15           NaN           NaN  ...         NaN  117.194382   \n",
       "         2013-08-14     -1.233843      0.944284  ...         NaN  111.842825   \n",
       "         2013-11-14     -1.130208      2.481737  ...         NaN  104.717879   \n",
       "         2014-03-31     -0.339410      3.102975  ...         NaN  124.701609   \n",
       "         2014-05-15     -1.732411      1.630076  ...    8.866880  123.837828   \n",
       "...                           ...           ...  ...         ...         ...   \n",
       "9962     2021-05-15      0.618888      0.636208  ...    1.131801  917.179427   \n",
       "         2021-08-14      3.243048      3.596651  ...    7.003658  356.060601   \n",
       "         2021-11-14      2.691891      2.839898  ...   11.206472  211.402928   \n",
       "         2022-03-31      3.352989      3.383973  ...   15.804715  243.018042   \n",
       "         2022-05-15      2.371942      2.484802  ...   19.138403  345.647185   \n",
       "\n",
       "                      R502_速動比率  R503_利息支出率   R678_營運資金  R607_總資產週轉次數  \\\n",
       "stock_id date                                                           \n",
       "1101     2013-05-15   83.137456   -8.186859  11333336.0           NaN   \n",
       "         2013-08-14   79.609613  -13.298556   8168144.0      0.104927   \n",
       "         2013-11-14   83.502774  -17.083477   3478652.0      0.110355   \n",
       "         2014-03-31  102.946688  -32.624816  17389561.0      0.119738   \n",
       "         2014-05-15  101.776758   -9.792979  17732671.0      0.093998   \n",
       "...                         ...         ...         ...           ...   \n",
       "9962     2021-05-15  271.866394    1.005915    623132.0      0.380922   \n",
       "         2021-08-14  110.054975    0.178902    701921.0      0.594034   \n",
       "         2021-11-14   27.425413    0.432543    748328.0      0.428734   \n",
       "         2022-03-31   39.386083    1.907177    831853.0      0.503416   \n",
       "         2022-05-15   85.204235    2.087938    864909.0      0.525429   \n",
       "\n",
       "                     R610_存貨週轉率  R612_固定資產週轉次數  R613_淨值週轉率次  R69B_自由現金流量  \n",
       "stock_id date                                                             \n",
       "1101     2013-05-15         NaN            NaN          NaN          NaN  \n",
       "         2013-08-14    2.279272       0.258545     0.201496    9467257.0  \n",
       "         2013-11-14    2.335831       0.272965     0.211719   12033719.0  \n",
       "         2014-03-31    2.763869       0.302962     0.222264   -6823797.0  \n",
       "         2014-05-15    2.191306       0.245688     0.171327    4901868.0  \n",
       "...                         ...            ...          ...          ...  \n",
       "9962     2021-05-15    0.726929       1.533102     0.435345      37994.0  \n",
       "         2021-08-14    1.150951       2.593983     0.710227     -34385.0  \n",
       "         2021-11-14    0.649981       2.435492     0.635698       2993.0  \n",
       "         2022-03-31    0.661852       3.250884     0.808212     -14303.0  \n",
       "         2022-05-15    0.758426       3.189613     0.753799       9429.0  \n",
       "\n",
       "[65527 rows x 48 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_fundamental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 技術指標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://zhuanlan.zhihu.com/p/342075180 talib函数功能一览表\n",
    "\n",
    "def bias(n):\n",
    "    return close / close.rolling(n, min_periods=1).mean()\n",
    "\n",
    "def acc(n):\n",
    "    return close.shift(n) / (close.shift(2*n) + close) * 2\n",
    "\n",
    "def rsv(n):\n",
    "    l = close.rolling(n, min_periods=1).min()\n",
    "    h = close.rolling(n, min_periods=1).max()\n",
    "    \n",
    "    return (close - l) / (h - l)\n",
    "\n",
    "def mom(n):\n",
    "    return (rev / rev.shift(1)).shift(n)\n",
    "\n",
    "\n",
    "def bi_(n):\n",
    "    return (bargin_i / vol.shift(1)).shift(n)\n",
    "\n",
    "def bf(n):\n",
    "    return (bargin_f / vol.shift(1)).shift(n)\n",
    "    \n",
    "def bs(n):\n",
    "    return (bargin_s / vol.shift(1)).shift(n)\n",
    "\n",
    "def rsi(n):\n",
    "    #return talib_all_stock(ndays=10000, func=abstract.RSI, timeperiod=n)\n",
    "    return data.talib(\"RSI\",timeperiod=n)\n",
    "\n",
    "def MFI(n):\n",
    "    return data.talib(\"MFI\",timeperiod=n)\n",
    "\n",
    "def obv(n):\n",
    "    return data.talib(\"OBV\",timeperiod=n)\n",
    "\n",
    "\n",
    "\n",
    "features = {\n",
    "    \n",
    "    #'ATR14':data.talib(\"ATR\",timeperiod=14),\n",
    "    #'NATR14':data.talib('NATR',timeperiod=14),\n",
    "    #'TRANGE':data.talib('TRANGE'),\n",
    "    #'Adosc3':data.talib('ADOSC',timeperiod=3),\n",
    "    \n",
    "    #\"MFI5\":MFI(5),\n",
    "    #\"MFI10\":MFI(10),\n",
    "    \n",
    "    #'rsi6': rsi(6),  #DataFrame\n",
    "    #'rsi10': rsi(10),  #DataFrame\n",
    "    #'rsi14': rsi(14),  #DataFrame\n",
    "    #'rsi20': rsi(20),  #DataFrame\n",
    "    #'rsi50': rsi(50),  #DataFrame\n",
    "   \n",
    "    'mom1': mom(1),\n",
    "    'mom2': mom(2),\n",
    "    'mom3': mom(3),\n",
    "    'mom4': mom(4),\n",
    "    'mom5': mom(5),\n",
    "    'mom6': mom(6),\n",
    "    'mom7': mom(7),\n",
    "    'mom8': mom(8),\n",
    "    'mom9': mom(9),\n",
    "    \n",
    "    'yoy': yoy(1),\n",
    "    'delta_yoy':yoy(1)-yoy(2),\n",
    "    \n",
    "#    'ff':ff,\n",
    "    'PB':PB,\n",
    "    'PE':pe,   \n",
    "#  \n",
    "    'bias5': bias(5),\n",
    "    'bias10': bias(10),\n",
    "    'bias20': bias(20),\n",
    "    'bias60': bias(60),\n",
    "    'bias120': bias(120),\n",
    "    'bias240': bias(240),\n",
    "    \n",
    "    'acc5': acc(5),\n",
    "    'acc10': acc(10),\n",
    "    'acc20': acc(20),\n",
    "    'acc60': acc(60),\n",
    "    'acc120': acc(120),\n",
    "    'acc240': acc(240),\n",
    "    \n",
    "    #'rsv5': rsv(5),\n",
    "    #'rsv10': rsv(10),\n",
    "    #'rsv20': rsv(20),\n",
    "    #'rsv60': rsv(60),\n",
    "    #'rsv120': rsv(120),\n",
    "    #'rsv240': rsv(240),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 組合dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認各指標清單"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PB',\n",
       " 'PE',\n",
       " 'acc10',\n",
       " 'acc120',\n",
       " 'acc20',\n",
       " 'acc240',\n",
       " 'acc5',\n",
       " 'acc60',\n",
       " 'bias10',\n",
       " 'bias120',\n",
       " 'bias20',\n",
       " 'bias240',\n",
       " 'bias5',\n",
       " 'bias60',\n",
       " 'delta_yoy',\n",
       " 'mom1',\n",
       " 'mom2',\n",
       " 'mom3',\n",
       " 'mom4',\n",
       " 'mom5',\n",
       " 'mom6',\n",
       " 'mom7',\n",
       " 'mom8',\n",
       " 'mom9',\n",
       " 'yoy']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1=sorted(features)\n",
    "list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t1 = data.talib(\"NATR\",timeperiod=14)\n",
    "#t1.to_csv('myfile.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 製作dataset\n",
    "\n",
    "##### 設定買賣頻率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2005-02-15', '2005-03-15', '2005-04-15', '2005-05-15',\n",
       "               '2005-06-15', '2005-07-15', '2005-08-15', '2005-09-15',\n",
       "               '2005-10-15', '2005-11-15',\n",
       "               ...\n",
       "               '2022-02-15', '2022-03-15', '2022-04-15', '2022-05-15',\n",
       "               '2022-06-15', '2022-07-15', '2022-08-15', '2022-09-15',\n",
       "               '2022-10-15', '2022-11-15'],\n",
       "              dtype='datetime64[ns]', name='date', length=214, freq=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rev.index = rev.index.tz_localize(\"Asia/Taipei\")\n",
    "every_month = rev.index\n",
    "every_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 將dataframe 組裝起來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features['bias20'].reindex(every_month, method='ffill')\n",
    "\n",
    "for name, f in features.items():\n",
    "    features[name] = f.reindex(every_month, method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name, f in features.items():\n",
    "    features[name] = f.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################　　　自己加入的　　　##############################################\n",
    "dataset.index = dataset.index.set_names(['stock_id','date'], level=[0,1])\n",
    "\n",
    "\n",
    "#dataset.index.levels[1].name = 'date'\n",
    "#dataset.index.levels[0].name = 'stock_id'\n",
    "\n",
    "#因為你pandas更新到新版了\n",
    "## profit.index.levels[0].name = 'year'\n",
    "## profit.index.levels[1].name = 'month'\n",
    "#這兩行的語法被棄用，請改成\n",
    "#profit.index=profit.index.set_names('year', level=0)\n",
    "#profit.index=profit.index.set_names('month', level=1)\n",
    "#or profit.index=profit.index.set_names(['year','month'], level=[0,1])\n",
    "#直接一行\n",
    "#就可以了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 新增 label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finlab import ml\n",
    "\n",
    "ml.add_profit_prediction(dataset)\n",
    "ml.add_rank_prediction(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 刪除太大太小的歷史資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(436774, 27)\n",
      "(377853, 27)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "\n",
    "def drop_extreme_case(dataset, feature_names, thresh=0.01):\n",
    "    \n",
    "    extreme_cases = pd.Series(False, index=dataset.index)\n",
    "    for f in feature_names:\n",
    "        tf = dataset[f]\n",
    "        extreme_cases = extreme_cases | (tf < tf.quantile(thresh)) | (tf > tf.quantile(1-thresh))\n",
    "    dataset = dataset[~extreme_cases]\n",
    "    return dataset\n",
    "\n",
    "dataset_drop_extreme_case = drop_extreme_case(dataset , list1 , thresh=0.01)\n",
    "\n",
    "print(dataset_drop_extreme_case.shape)\n",
    "\n",
    "##(436560, 27)\n",
    "##(377538, 27)\n",
    "\n",
    "##(505602, 75)\n",
    "##(446580, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dropna = dataset_drop_extreme_case.dropna(how='any')\n",
    "dataset_dropna = dataset_dropna.reset_index().set_index(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2005-02-15', '2005-03-15', '2005-04-15', '2005-05-15',\n",
       "               '2005-06-15', '2005-07-15', '2005-08-15', '2005-09-15',\n",
       "               '2005-10-15', '2005-11-15',\n",
       "               ...\n",
       "               '2021-11-15', '2021-12-15', '2022-02-15', '2022-03-15',\n",
       "               '2022-04-15', '2022-06-15', '2022-08-15', '2022-09-15',\n",
       "               '2022-10-15', '2022-11-15'],\n",
       "              dtype='datetime64[ns]', name='date', length=377853, freq=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_drop_extreme_case.index.get_level_values(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################　　　自己加入的　　　##############################################\n",
    "\n",
    "dataset_dropna.index = pd.to_datetime(dataset_dropna.index)\n",
    "dataset_dropna = dataset_dropna.sort_index()\n",
    "\n",
    "#修復＜class ‘numpy.ndarray‘＞　https://blog.csdn.net/lxbin/article/details/114005757"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset_dropna[:'2020']\n",
    "dataset_test = dataset_dropna['2021':]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 機器學習\n",
    " - 目前只有三個，技術指標也要在增加一下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 100)               2600      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 12,801\n",
      "Trainable params: 12,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "start fitting\n",
      "Epoch 1/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.280 - ETA: 0s - loss: 0.260 - ETA: 0s - loss: 0.227 - ETA: 0s - loss: 0.180 - ETA: 0s - loss: 0.152 - ETA: 0s - loss: 0.137 - 1s 5ms/step - loss: 0.1289 - val_loss: 0.0714\n",
      "Epoch 2/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0736 - val_loss: 0.0714\n",
      "Epoch 3/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0736 - val_loss: 0.0714\n",
      "Epoch 4/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0736 - val_loss: 0.0713\n",
      "Epoch 5/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 6/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 7/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 8/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 9/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0714\n",
      "Epoch 10/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0736 - val_loss: 0.0714\n",
      "Epoch 11/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 12/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0714\n",
      "Epoch 13/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 14/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 15/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 16/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0735 - val_loss: 0.0712\n",
      "Epoch 17/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 18/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0712\n",
      "Epoch 19/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 20/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0712\n",
      "Epoch 21/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 22/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 23/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0712\n",
      "Epoch 24/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0712\n",
      "Epoch 25/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 26/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0734 - val_loss: 0.0713\n",
      "Epoch 27/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 28/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0713\n",
      "Epoch 29/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0712\n",
      "Epoch 30/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 31/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 32/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 33/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 34/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 35/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 36/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0735 - val_loss: 0.0712\n",
      "Epoch 37/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 38/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 39/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0734 - val_loss: 0.0711\n",
      "Epoch 40/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 41/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 42/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 43/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0734 - val_loss: 0.0711\n",
      "Epoch 44/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 45/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 46/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 47/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0734 - val_loss: 0.0711\n",
      "Epoch 48/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 49/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 50/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 51/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 52/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0711\n",
      "Epoch 53/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0711\n",
      "Epoch 54/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 55/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 56/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 57/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 58/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0734 - val_loss: 0.0711\n",
      "Epoch 59/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 60/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 61/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0734 - val_loss: 0.0711\n",
      "Epoch 62/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 63/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0734 - val_loss: 0.0711\n",
      "Epoch 64/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 65/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 66/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 67/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0734 - val_loss: 0.0711\n",
      "Epoch 68/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 69/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 70/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 71/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0712\n",
      "Epoch 72/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 73/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 74/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.082 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0712\n",
      "Epoch 75/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0734 - val_loss: 0.0712\n",
      "Epoch 76/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0734 - val_loss: 0.0711\n",
      "Epoch 77/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 78/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 79/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 80/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 81/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0712\n",
      "Epoch 82/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 83/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 84/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 85/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0712\n",
      "Epoch 86/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 87/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 88/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 89/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 90/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 91/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 92/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 93/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 94/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 95/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 96/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 97/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0734 - val_loss: 0.0711\n",
      "Epoch 98/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 99/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 100/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 101/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0712\n",
      "Epoch 102/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 103/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0712\n",
      "Epoch 104/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 105/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 106/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 107/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 108/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 109/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 110/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 111/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 112/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 113/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0712\n",
      "Epoch 114/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 115/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 116/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 117/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0712\n",
      "Epoch 118/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0732 - val_loss: 0.0710\n",
      "Epoch 119/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 120/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 121/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 122/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 123/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 124/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 125/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 126/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 127/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0732 - val_loss: 0.0710\n",
      "Epoch 128/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 129/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 130/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 131/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0732 - val_loss: 0.0710\n",
      "Epoch 132/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0710\n",
      "Epoch 133/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0710\n",
      "Epoch 134/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 135/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0733 - val_loss: 0.0711\n",
      "Epoch 136/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 137/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 138/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 139/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 140/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 141/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0710\n",
      "Epoch 142/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 143/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0709\n",
      "Epoch 144/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 145/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 146/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0732 - val_loss: 0.0708\n",
      "Epoch 147/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0732 - val_loss: 0.0710\n",
      "Epoch 148/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 149/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 150/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0732 - val_loss: 0.0710\n",
      "Epoch 151/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 152/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 153/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 154/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0732 - val_loss: 0.0710\n",
      "Epoch 155/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0732 - val_loss: 0.0710\n",
      "Epoch 156/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0732 - val_loss: 0.0710\n",
      "Epoch 157/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 158/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0732 - val_loss: 0.0709\n",
      "Epoch 159/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0709\n",
      "Epoch 160/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0711\n",
      "Epoch 161/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 162/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 163/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 5ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 164/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0732 - val_loss: 0.0710\n",
      "Epoch 165/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 166/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 167/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 168/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 169/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 170/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0731 - val_loss: 0.0709\n",
      "Epoch 171/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 172/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0709\n",
      "Epoch 173/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0732 - val_loss: 0.0710\n",
      "Epoch 174/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0709\n",
      "Epoch 175/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0709\n",
      "Epoch 176/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 177/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0709\n",
      "Epoch 178/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0732 - val_loss: 0.0709\n",
      "Epoch 179/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0709\n",
      "Epoch 180/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0709\n",
      "Epoch 181/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0708\n",
      "Epoch 182/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 183/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 184/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 185/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0731 - val_loss: 0.0709\n",
      "Epoch 186/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 187/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0731 - val_loss: 0.0709\n",
      "Epoch 188/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0712\n",
      "Epoch 189/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 190/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 191/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 192/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 193/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0709\n",
      "Epoch 194/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0709\n",
      "Epoch 195/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 196/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 197/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 198/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 199/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0709\n",
      "Epoch 200/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 201/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0709\n",
      "Epoch 202/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 203/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 204/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0731 - val_loss: 0.0708\n",
      "Epoch 205/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0709\n",
      "Epoch 206/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 207/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0731 - val_loss: 0.0710\n",
      "Epoch 208/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0731 - val_loss: 0.0711\n",
      "Epoch 209/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0731 - val_loss: 0.0708\n",
      "Epoch 210/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0709\n",
      "Epoch 211/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0731 - val_loss: 0.0709\n",
      "Epoch 212/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0709\n",
      "Epoch 213/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 214/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0712\n",
      "Epoch 215/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0708\n",
      "Epoch 216/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0709\n",
      "Epoch 217/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0710\n",
      "Epoch 218/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 219/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0729 - val_loss: 0.0710\n",
      "Epoch 220/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0709\n",
      "Epoch 221/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 222/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0709\n",
      "Epoch 223/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 224/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 225/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 226/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0712\n",
      "Epoch 227/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0729 - val_loss: 0.0709\n",
      "Epoch 228/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0729 - val_loss: 0.0708\n",
      "Epoch 229/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0709\n",
      "Epoch 230/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 231/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 232/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0710\n",
      "Epoch 233/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 234/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 235/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0729 - val_loss: 0.0708\n",
      "Epoch 236/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0709\n",
      "Epoch 237/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0731 - val_loss: 0.0709\n",
      "Epoch 238/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0710\n",
      "Epoch 239/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 240/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0709\n",
      "Epoch 241/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0729 - val_loss: 0.0709\n",
      "Epoch 242/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0709\n",
      "Epoch 243/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0729 - val_loss: 0.0710\n",
      "Epoch 244/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0708\n",
      "Epoch 245/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 246/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0709\n",
      "Epoch 247/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0708\n",
      "Epoch 248/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0729 - val_loss: 0.0709\n",
      "Epoch 249/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0729 - val_loss: 0.0709\n",
      "Epoch 250/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0710\n",
      "Epoch 251/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 252/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0711\n",
      "Epoch 253/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0710\n",
      "Epoch 254/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0730 - val_loss: 0.0709\n",
      "Epoch 255/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0709\n",
      "Epoch 256/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0730 - val_loss: 0.0710\n",
      "Epoch 257/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 258/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0729 - val_loss: 0.0710\n",
      "Epoch 259/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0729 - val_loss: 0.0710\n",
      "Epoch 260/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0710\n",
      "Epoch 261/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 262/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0729 - val_loss: 0.0711\n",
      "Epoch 263/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0729 - val_loss: 0.0709\n",
      "Epoch 264/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0709\n",
      "Epoch 265/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0729 - val_loss: 0.0709\n",
      "Epoch 266/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0729 - val_loss: 0.0710\n",
      "Epoch 267/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0729 - val_loss: 0.0710\n",
      "Epoch 268/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0729 - val_loss: 0.0708\n",
      "Epoch 269/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0708\n",
      "Epoch 270/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0729 - val_loss: 0.0708\n",
      "Epoch 271/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0729 - val_loss: 0.0710\n",
      "Epoch 272/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - 0s 3ms/step - loss: 0.0729 - val_loss: 0.0708\n",
      "Epoch 273/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0710\n",
      "Epoch 274/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0710\n",
      "Epoch 275/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0710\n",
      "Epoch 276/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0710\n",
      "Epoch 277/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0708\n",
      "Epoch 278/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0729 - val_loss: 0.0709\n",
      "Epoch 279/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0708\n",
      "Epoch 280/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0709\n",
      "Epoch 281/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 282/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0709\n",
      "Epoch 283/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0729 - val_loss: 0.0709\n",
      "Epoch 284/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0709\n",
      "Epoch 285/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0709\n",
      "Epoch 286/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 287/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 288/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0709\n",
      "Epoch 289/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0709\n",
      "Epoch 290/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0729 - val_loss: 0.0710\n",
      "Epoch 291/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 292/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0709\n",
      "Epoch 293/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0709\n",
      "Epoch 294/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0708\n",
      "Epoch 295/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0710\n",
      "Epoch 296/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0710\n",
      "Epoch 297/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0709\n",
      "Epoch 298/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0710\n",
      "Epoch 299/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 300/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0710\n",
      "Epoch 301/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0709\n",
      "Epoch 302/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0710\n",
      "Epoch 303/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0710\n",
      "Epoch 304/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0712\n",
      "Epoch 305/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 306/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0710\n",
      "Epoch 307/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0709\n",
      "Epoch 308/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0710\n",
      "Epoch 309/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 310/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0728 - val_loss: 0.0710\n",
      "Epoch 311/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0708\n",
      "Epoch 312/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0729 - val_loss: 0.0709\n",
      "Epoch 313/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0710\n",
      "Epoch 314/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0709\n",
      "Epoch 315/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0710\n",
      "Epoch 316/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0709\n",
      "Epoch 317/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0710\n",
      "Epoch 318/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 319/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0710\n",
      "Epoch 320/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0709\n",
      "Epoch 321/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0710\n",
      "Epoch 322/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0710\n",
      "Epoch 323/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0710\n",
      "Epoch 324/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0710\n",
      "Epoch 325/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 326/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0709\n",
      "Epoch 327/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0710\n",
      "Epoch 328/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0710\n",
      "Epoch 329/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0709\n",
      "Epoch 330/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0709\n",
      "Epoch 331/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 332/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0710\n",
      "Epoch 333/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 334/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0710\n",
      "Epoch 335/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0711\n",
      "Epoch 336/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0712\n",
      "Epoch 337/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 338/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0709\n",
      "Epoch 339/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 340/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0709\n",
      "Epoch 341/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 342/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0709\n",
      "Epoch 343/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0728 - val_loss: 0.0710\n",
      "Epoch 344/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0709\n",
      "Epoch 345/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0709\n",
      "Epoch 346/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0710\n",
      "Epoch 347/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 348/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0710\n",
      "Epoch 349/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0710\n",
      "Epoch 350/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0709\n",
      "Epoch 351/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0710\n",
      "Epoch 352/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0709\n",
      "Epoch 353/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 354/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0710\n",
      "Epoch 355/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0710\n",
      "Epoch 356/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0710\n",
      "Epoch 357/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0710\n",
      "Epoch 358/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 359/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 360/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 361/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0709\n",
      "Epoch 362/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0712\n",
      "Epoch 363/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0710\n",
      "Epoch 364/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 365/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 366/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0710\n",
      "Epoch 367/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 368/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 369/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0709\n",
      "Epoch 370/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0709\n",
      "Epoch 371/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0710\n",
      "Epoch 372/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0710\n",
      "Epoch 373/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 374/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 375/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0710\n",
      "Epoch 376/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0727 - val_loss: 0.0711\n",
      "Epoch 377/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 378/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0710\n",
      "Epoch 379/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0710\n",
      "Epoch 380/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 381/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 382/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0710\n",
      "Epoch 383/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 384/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 385/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 386/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0725 - val_loss: 0.0710\n",
      "Epoch 387/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 388/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 389/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0709\n",
      "Epoch 390/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 391/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0710\n",
      "Epoch 392/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 393/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 394/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 395/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 396/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 397/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 398/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0710\n",
      "Epoch 399/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 400/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 401/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0710\n",
      "Epoch 402/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0725 - val_loss: 0.0710\n",
      "Epoch 403/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 404/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 405/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 406/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0710\n",
      "Epoch 407/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 408/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 409/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 410/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0726 - val_loss: 0.0712\n",
      "Epoch 411/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0725 - val_loss: 0.0710\n",
      "Epoch 412/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 413/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 414/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0710\n",
      "Epoch 415/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0727 - val_loss: 0.0712\n",
      "Epoch 416/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 417/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 418/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0710\n",
      "Epoch 419/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 420/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 421/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0709\n",
      "Epoch 422/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 423/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0710\n",
      "Epoch 424/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0725 - val_loss: 0.0710\n",
      "Epoch 425/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 426/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0710\n",
      "Epoch 427/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 428/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0710\n",
      "Epoch 429/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 430/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 431/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 432/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 433/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0724 - val_loss: 0.0710\n",
      "Epoch 434/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 435/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 436/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 437/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 438/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 439/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0726 - val_loss: 0.0711\n",
      "Epoch 440/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 441/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0725 - val_loss: 0.0710\n",
      "Epoch 442/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0710\n",
      "Epoch 443/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 444/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 445/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 446/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 447/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 448/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 449/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 450/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 451/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 452/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 453/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0711\n",
      "Epoch 454/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0724 - val_loss: 0.0710\n",
      "Epoch 455/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 456/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 457/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 458/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 459/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 460/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 461/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 462/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 463/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 464/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0710\n",
      "Epoch 465/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 466/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0710\n",
      "Epoch 467/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0724 - val_loss: 0.0710\n",
      "Epoch 468/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 469/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 470/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0725 - val_loss: 0.0712\n",
      "Epoch 471/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 472/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 473/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 474/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 475/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 476/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 477/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 478/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 479/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 480/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 481/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 482/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 483/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0724 - val_loss: 0.0713\n",
      "Epoch 484/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0723 - val_loss: 0.0711\n",
      "Epoch 485/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0723 - val_loss: 0.0711\n",
      "Epoch 486/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 487/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0713\n",
      "Epoch 488/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 489/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 490/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 491/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0723 - val_loss: 0.0711\n",
      "Epoch 492/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0710\n",
      "Epoch 493/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0711\n",
      "Epoch 494/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 495/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0713\n",
      "Epoch 496/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0723 - val_loss: 0.0711\n",
      "Epoch 497/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 498/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0712\n",
      "Epoch 499/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 500/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 501/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 502/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0723 - val_loss: 0.0711\n",
      "Epoch 503/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0711\n",
      "Epoch 504/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 505/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0711\n",
      "Epoch 506/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0712\n",
      "Epoch 507/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0723 - val_loss: 0.0713\n",
      "Epoch 508/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0723 - val_loss: 0.0711\n",
      "Epoch 509/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0724 - val_loss: 0.0712\n",
      "Epoch 510/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 511/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0711\n",
      "Epoch 512/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0713\n",
      "Epoch 513/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0723 - val_loss: 0.0712\n",
      "Epoch 514/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0723 - val_loss: 0.0711\n",
      "Epoch 515/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0711\n",
      "Epoch 516/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0724 - val_loss: 0.0713\n",
      "Epoch 517/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0710\n",
      "Epoch 518/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0711\n",
      "Epoch 519/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0723 - val_loss: 0.0713\n",
      "Epoch 520/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0723 - val_loss: 0.0713\n",
      "Epoch 521/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 522/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0711\n",
      "Epoch 523/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0723 - val_loss: 0.0711\n",
      "Epoch 524/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 525/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0711\n",
      "Epoch 526/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 527/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 528/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0711\n",
      "Epoch 529/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 530/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 531/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0712\n",
      "Epoch 532/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0711\n",
      "Epoch 533/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 534/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 535/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0711\n",
      "Epoch 536/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0723 - val_loss: 0.0711\n",
      "Epoch 537/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0723 - val_loss: 0.0712\n",
      "Epoch 538/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 539/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0723 - val_loss: 0.0712\n",
      "Epoch 540/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0713\n",
      "Epoch 541/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0723 - val_loss: 0.0713\n",
      "Epoch 542/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 543/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0722 - val_loss: 0.0711\n",
      "Epoch 544/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0711\n",
      "Epoch 545/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 546/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 547/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0723 - val_loss: 0.0712\n",
      "Epoch 548/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0723 - val_loss: 0.0712\n",
      "Epoch 549/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 550/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0723 - val_loss: 0.0713\n",
      "Epoch 551/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 552/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 553/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0710\n",
      "Epoch 554/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0711\n",
      "Epoch 555/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0722 - val_loss: 0.0711\n",
      "Epoch 556/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 557/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 558/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 559/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0712\n",
      "Epoch 560/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 561/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 562/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 563/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0722 - val_loss: 0.0714\n",
      "Epoch 564/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 565/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0723 - val_loss: 0.0712\n",
      "Epoch 566/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0722 - val_loss: 0.0713\n",
      "Epoch 567/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0721 - val_loss: 0.0711\n",
      "Epoch 568/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0722 - val_loss: 0.0711\n",
      "Epoch 569/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0711\n",
      "Epoch 570/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0722 - val_loss: 0.0710\n",
      "Epoch 571/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0722 - val_loss: 0.0711\n",
      "Epoch 572/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 573/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0714\n",
      "Epoch 574/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 575/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0712\n",
      "Epoch 576/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0712\n",
      "Epoch 577/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0712\n",
      "Epoch 578/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0711\n",
      "Epoch 579/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0721 - val_loss: 0.0712\n",
      "Epoch 580/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0721 - val_loss: 0.0713\n",
      "Epoch 581/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0721 - val_loss: 0.0711\n",
      "Epoch 582/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0721 - val_loss: 0.0711\n",
      "Epoch 583/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0711\n",
      "Epoch 584/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0711\n",
      "Epoch 585/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0721 - val_loss: 0.0710\n",
      "Epoch 586/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0721 - val_loss: 0.0711\n",
      "Epoch 587/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0721 - val_loss: 0.0713\n",
      "Epoch 588/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 589/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0721 - val_loss: 0.0715\n",
      "Epoch 590/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0722 - val_loss: 0.0711\n",
      "Epoch 591/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0712\n",
      "Epoch 592/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 593/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 594/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0721 - val_loss: 0.0712\n",
      "Epoch 595/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 596/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0721 - val_loss: 0.0711\n",
      "Epoch 597/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0711\n",
      "Epoch 598/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0721 - val_loss: 0.0713\n",
      "Epoch 599/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0721 - val_loss: 0.0712\n",
      "Epoch 600/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0714\n",
      "Epoch 601/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0711\n",
      "Epoch 602/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0721 - val_loss: 0.0713\n",
      "Epoch 603/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0711\n",
      "Epoch 604/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0720 - val_loss: 0.0713\n",
      "Epoch 605/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0720 - val_loss: 0.0712\n",
      "Epoch 606/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0715\n",
      "Epoch 607/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0713\n",
      "Epoch 608/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0713\n",
      "Epoch 609/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0712\n",
      "Epoch 610/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0711\n",
      "Epoch 611/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0713\n",
      "Epoch 612/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0712\n",
      "Epoch 613/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0721 - val_loss: 0.0714\n",
      "Epoch 614/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0713\n",
      "Epoch 615/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0713\n",
      "Epoch 616/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0722 - val_loss: 0.0712\n",
      "Epoch 617/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 618/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0715\n",
      "Epoch 619/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0715\n",
      "Epoch 620/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0712\n",
      "Epoch 621/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0721 - val_loss: 0.0713\n",
      "Epoch 622/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0713\n",
      "Epoch 623/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0713\n",
      "Epoch 624/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0712\n",
      "Epoch 625/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0711\n",
      "Epoch 626/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0711\n",
      "Epoch 627/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0713\n",
      "Epoch 628/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0721 - val_loss: 0.0711\n",
      "Epoch 629/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0720 - val_loss: 0.0713\n",
      "Epoch 630/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0713\n",
      "Epoch 631/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0721 - val_loss: 0.0713\n",
      "Epoch 632/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0713\n",
      "Epoch 633/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0721 - val_loss: 0.0712\n",
      "Epoch 634/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0720 - val_loss: 0.0712\n",
      "Epoch 635/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0719 - val_loss: 0.0713\n",
      "Epoch 636/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 637/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 638/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0712\n",
      "Epoch 639/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0712\n",
      "Epoch 640/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0712\n",
      "Epoch 641/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 642/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 643/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 644/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0721 - val_loss: 0.0712\n",
      "Epoch 645/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0720 - val_loss: 0.0713\n",
      "Epoch 646/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0715\n",
      "Epoch 647/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0712\n",
      "Epoch 648/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0721 - val_loss: 0.0713\n",
      "Epoch 649/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0713\n",
      "Epoch 650/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0715\n",
      "Epoch 651/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0713\n",
      "Epoch 652/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0712\n",
      "Epoch 653/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0721 - val_loss: 0.0712\n",
      "Epoch 654/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0713\n",
      "Epoch 655/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0713\n",
      "Epoch 656/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 657/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 658/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0713\n",
      "Epoch 659/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0720 - val_loss: 0.0712\n",
      "Epoch 660/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 661/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 3ms/step - loss: 0.0720 - val_loss: 0.0713\n",
      "Epoch 662/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 663/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 664/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 665/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0713\n",
      "Epoch 666/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 667/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0713\n",
      "Epoch 668/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0717\n",
      "Epoch 669/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0716\n",
      "Epoch 670/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 1s 8ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 671/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 672/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0715\n",
      "Epoch 673/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 674/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 675/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0715\n",
      "Epoch 676/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0715\n",
      "Epoch 677/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 678/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 679/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 680/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 681/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 682/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 683/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 684/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0713\n",
      "Epoch 685/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 686/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0715\n",
      "Epoch 687/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 688/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 689/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 690/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0713\n",
      "Epoch 691/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 692/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 693/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 694/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 695/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 696/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 697/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0713\n",
      "Epoch 698/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 699/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 700/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 701/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 702/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 703/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0712\n",
      "Epoch 704/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0712\n",
      "Epoch 705/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0718 - val_loss: 0.0714\n",
      "Epoch 706/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0712\n",
      "Epoch 707/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 708/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 709/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0720 - val_loss: 0.0715\n",
      "Epoch 710/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 711/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 712/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0718 - val_loss: 0.0714\n",
      "Epoch 713/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 714/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 715/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0720 - val_loss: 0.0716\n",
      "Epoch 716/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0713\n",
      "Epoch 717/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0717\n",
      "Epoch 718/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 719/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 720/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 721/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0719 - val_loss: 0.0713\n",
      "Epoch 722/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0713\n",
      "Epoch 723/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0713\n",
      "Epoch 724/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0714\n",
      "Epoch 725/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0714\n",
      "Epoch 726/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 727/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 728/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0714\n",
      "Epoch 729/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 730/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0712\n",
      "Epoch 731/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 732/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 733/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0713\n",
      "Epoch 734/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 735/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 736/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0714\n",
      "Epoch 737/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0716\n",
      "Epoch 738/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0713\n",
      "Epoch 739/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0718 - val_loss: 0.0714\n",
      "Epoch 740/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 741/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0718 - val_loss: 0.0713\n",
      "Epoch 742/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 743/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0718\n",
      "Epoch 744/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 745/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 746/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 747/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 748/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0713\n",
      "Epoch 749/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0714\n",
      "Epoch 750/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 751/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0717\n",
      "Epoch 752/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 753/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 754/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 755/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0714\n",
      "Epoch 756/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0714\n",
      "Epoch 757/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 758/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 759/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0714\n",
      "Epoch 760/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 761/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 762/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0714\n",
      "Epoch 763/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0714\n",
      "Epoch 764/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0714\n",
      "Epoch 765/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0718 - val_loss: 0.0714\n",
      "Epoch 766/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 767/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0718 - val_loss: 0.0714\n",
      "Epoch 768/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 769/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0714\n",
      "Epoch 770/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 771/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0719 - val_loss: 0.0715\n",
      "Epoch 772/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0718 - val_loss: 0.0717\n",
      "Epoch 773/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0718 - val_loss: 0.0717\n",
      "Epoch 774/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 775/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0718 - val_loss: 0.0714\n",
      "Epoch 776/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0718 - val_loss: 0.0718\n",
      "Epoch 777/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 778/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 779/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0718 - val_loss: 0.0714\n",
      "Epoch 780/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 781/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 782/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0714\n",
      "Epoch 783/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 784/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 785/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 786/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0718\n",
      "Epoch 787/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 788/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0714\n",
      "Epoch 789/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 790/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0719\n",
      "Epoch 791/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0714\n",
      "Epoch 792/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 793/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 794/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0714\n",
      "Epoch 795/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0718 - val_loss: 0.0714\n",
      "Epoch 796/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 797/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 798/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0714\n",
      "Epoch 799/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 800/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0714\n",
      "Epoch 801/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 802/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0717\n",
      "Epoch 803/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0718 - val_loss: 0.0714\n",
      "Epoch 804/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 805/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 806/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 807/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0713\n",
      "Epoch 808/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 809/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 810/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0714\n",
      "Epoch 811/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 812/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0714\n",
      "Epoch 813/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 814/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 815/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 816/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 817/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 818/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 819/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 820/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0714\n",
      "Epoch 821/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 822/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0715\n",
      "Epoch 823/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0717\n",
      "Epoch 824/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0717\n",
      "Epoch 825/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 826/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 827/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 828/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 829/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 830/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 831/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0718 - val_loss: 0.0717\n",
      "Epoch 832/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 833/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 834/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 835/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0718 - val_loss: 0.0714\n",
      "Epoch 836/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 837/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0716\n",
      "Epoch 838/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0718\n",
      "Epoch 839/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 840/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 841/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 842/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0718 - val_loss: 0.0717\n",
      "Epoch 843/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0716\n",
      "Epoch 844/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0714\n",
      "Epoch 845/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0718\n",
      "Epoch 846/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 847/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 848/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0714\n",
      "Epoch 849/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0716\n",
      "Epoch 850/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 851/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 852/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 853/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0714\n",
      "Epoch 854/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 855/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 856/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 857/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 858/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 859/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 860/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 861/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 862/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 863/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0716\n",
      "Epoch 864/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0715\n",
      "Epoch 865/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0714\n",
      "Epoch 866/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 867/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0716\n",
      "Epoch 868/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 869/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 870/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0716\n",
      "Epoch 871/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 872/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0718 - val_loss: 0.0716\n",
      "Epoch 873/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 874/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 875/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0714\n",
      "Epoch 876/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 877/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 878/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0716\n",
      "Epoch 879/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0716\n",
      "Epoch 880/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0717\n",
      "Epoch 881/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 882/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0716\n",
      "Epoch 883/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0716\n",
      "Epoch 884/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0714\n",
      "Epoch 885/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0714\n",
      "Epoch 886/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 887/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0717\n",
      "Epoch 888/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0718\n",
      "Epoch 889/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0714\n",
      "Epoch 890/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0714\n",
      "Epoch 891/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0716\n",
      "Epoch 892/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0716\n",
      "Epoch 893/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 894/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0716\n",
      "Epoch 895/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0716\n",
      "Epoch 896/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 897/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 898/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0718\n",
      "Epoch 899/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 900/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 901/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 902/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 903/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0713\n",
      "Epoch 904/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 905/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 906/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0714\n",
      "Epoch 907/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 908/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0716\n",
      "Epoch 909/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0714\n",
      "Epoch 910/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0717\n",
      "Epoch 911/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0717\n",
      "Epoch 912/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0716\n",
      "Epoch 913/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0717\n",
      "Epoch 914/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0717\n",
      "Epoch 915/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 916/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 917/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0715\n",
      "Epoch 918/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 919/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0716\n",
      "Epoch 920/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 921/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 922/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 923/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 924/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 925/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0714\n",
      "Epoch 926/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0716\n",
      "Epoch 927/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 928/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 929/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0716\n",
      "Epoch 930/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 931/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 932/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 933/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0716\n",
      "Epoch 934/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0715\n",
      "Epoch 935/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 936/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0716\n",
      "Epoch 937/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0717\n",
      "Epoch 938/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0718\n",
      "Epoch 939/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0717 - val_loss: 0.0717\n",
      "Epoch 940/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0715\n",
      "Epoch 941/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0718\n",
      "Epoch 942/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0717\n",
      "Epoch 943/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0717\n",
      "Epoch 944/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0715\n",
      "Epoch 945/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0717\n",
      "Epoch 946/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 947/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 948/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 949/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 950/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0715\n",
      "Epoch 951/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0714\n",
      "Epoch 952/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0714\n",
      "Epoch 953/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0717\n",
      "Epoch 954/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0715\n",
      "Epoch 955/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0714\n",
      "Epoch 956/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0717\n",
      "Epoch 957/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0715\n",
      "Epoch 958/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0717\n",
      "Epoch 959/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0715\n",
      "Epoch 960/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 961/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 962/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0717 - val_loss: 0.0716\n",
      "Epoch 963/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0717\n",
      "Epoch 964/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0717\n",
      "Epoch 965/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 966/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0715\n",
      "Epoch 967/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0715\n",
      "Epoch 968/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 969/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0717\n",
      "Epoch 970/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0716\n",
      "Epoch 971/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0719\n",
      "Epoch 972/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 973/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 974/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0715\n",
      "Epoch 975/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0717\n",
      "Epoch 976/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0717\n",
      "Epoch 977/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0718\n",
      "Epoch 978/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0715\n",
      "Epoch 979/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0715\n",
      "Epoch 980/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 981/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0715\n",
      "Epoch 982/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0718\n",
      "Epoch 983/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 984/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0713\n",
      "Epoch 985/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0717\n",
      "Epoch 986/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 987/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 988/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0715\n",
      "Epoch 989/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 990/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 991/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0714\n",
      "Epoch 992/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 993/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0714\n",
      "Epoch 994/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0715\n",
      "Epoch 995/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 996/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0715\n",
      "Epoch 997/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 998/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 999/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 1000/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 1001/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 1002/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1003/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0717\n",
      "Epoch 1004/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 1005/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0717\n",
      "Epoch 1006/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0717\n",
      "Epoch 1007/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0718\n",
      "Epoch 1008/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1009/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0715\n",
      "Epoch 1010/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0718\n",
      "Epoch 1011/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0714\n",
      "Epoch 1012/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0716\n",
      "Epoch 1013/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 1014/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1015/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0717\n",
      "Epoch 1016/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0715\n",
      "Epoch 1017/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0717\n",
      "Epoch 1018/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0714\n",
      "Epoch 1019/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0717\n",
      "Epoch 1020/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0718\n",
      "Epoch 1021/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0715\n",
      "Epoch 1022/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 1023/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1024/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0715\n",
      "Epoch 1025/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 1026/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 1027/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1028/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0718\n",
      "Epoch 1029/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0714\n",
      "Epoch 1030/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0720\n",
      "Epoch 1031/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0718\n",
      "Epoch 1032/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0717\n",
      "Epoch 1033/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 1034/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0715\n",
      "Epoch 1035/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1036/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 1037/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0717\n",
      "Epoch 1038/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 1039/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1040/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0717\n",
      "Epoch 1041/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0718\n",
      "Epoch 1042/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1043/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1044/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1045/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0715\n",
      "Epoch 1046/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1047/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 1048/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1049/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 1050/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0716\n",
      "Epoch 1051/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0716 - val_loss: 0.0717\n",
      "Epoch 1052/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1053/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 1054/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0715\n",
      "Epoch 1055/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1056/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0715\n",
      "Epoch 1057/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0715\n",
      "Epoch 1058/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 1059/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0715\n",
      "Epoch 1060/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0715\n",
      "Epoch 1061/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0715\n",
      "Epoch 1062/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1063/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0714\n",
      "Epoch 1064/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0718\n",
      "Epoch 1065/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 1066/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 1067/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.078 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0715\n",
      "Epoch 1068/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0717\n",
      "Epoch 1069/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0716\n",
      "Epoch 1070/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0715\n",
      "Epoch 1071/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1072/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1073/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1074/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 1075/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0715\n",
      "Epoch 1076/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1077/5000\n",
      "103/103 [==============================] - ETA: 1s - loss: 0.066 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1078/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1079/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0719\n",
      "Epoch 1080/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0715\n",
      "Epoch 1081/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1082/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0715\n",
      "Epoch 1083/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0717\n",
      "Epoch 1084/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0718\n",
      "Epoch 1085/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1086/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1087/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1088/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1089/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0714\n",
      "Epoch 1090/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1091/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1092/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0715\n",
      "Epoch 1093/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1094/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0718\n",
      "Epoch 1095/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0715\n",
      "Epoch 1096/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1097/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0715\n",
      "Epoch 1098/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1099/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 1100/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1101/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1102/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1103/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0716\n",
      "Epoch 1104/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1105/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1106/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1107/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0718\n",
      "Epoch 1108/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0718\n",
      "Epoch 1109/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0718\n",
      "Epoch 1110/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0715 - val_loss: 0.0718\n",
      "Epoch 1111/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1112/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1113/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0715\n",
      "Epoch 1114/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1115/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0720\n",
      "Epoch 1116/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0718\n",
      "Epoch 1117/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1118/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1119/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1120/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0718\n",
      "Epoch 1121/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1122/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1123/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0715\n",
      "Epoch 1124/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0715\n",
      "Epoch 1125/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0719\n",
      "Epoch 1126/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0715\n",
      "Epoch 1127/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1128/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0718\n",
      "Epoch 1129/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1130/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1131/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1132/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0715\n",
      "Epoch 1133/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0715 - val_loss: 0.0715\n",
      "Epoch 1134/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1135/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1136/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1137/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1138/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1139/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1140/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1141/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0718\n",
      "Epoch 1142/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1143/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1144/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0716 - val_loss: 0.0718\n",
      "Epoch 1145/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0715\n",
      "Epoch 1146/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0718\n",
      "Epoch 1147/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1148/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1149/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1150/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1151/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0719\n",
      "Epoch 1152/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1153/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1154/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0715\n",
      "Epoch 1155/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1156/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.079 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1157/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1158/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1159/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1160/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1161/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0715\n",
      "Epoch 1162/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0718\n",
      "Epoch 1163/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0718\n",
      "Epoch 1164/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1165/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0718\n",
      "Epoch 1166/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0719\n",
      "Epoch 1167/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0715\n",
      "Epoch 1168/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1169/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1170/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0718\n",
      "Epoch 1171/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1172/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0719\n",
      "Epoch 1173/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1174/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0718\n",
      "Epoch 1175/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1176/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1177/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1178/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1179/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1180/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1181/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0715\n",
      "Epoch 1182/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1183/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1184/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0718\n",
      "Epoch 1185/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1186/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1187/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1188/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1189/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1190/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1191/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0715\n",
      "Epoch 1192/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1193/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1194/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1195/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0715\n",
      "Epoch 1196/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1197/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1198/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0718\n",
      "Epoch 1199/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1200/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0718\n",
      "Epoch 1201/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1202/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1203/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0719\n",
      "Epoch 1204/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1205/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0718\n",
      "Epoch 1206/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1207/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1208/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1209/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0719\n",
      "Epoch 1210/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1211/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0718\n",
      "Epoch 1212/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1213/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1214/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0718\n",
      "Epoch 1215/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0715\n",
      "Epoch 1216/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1217/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1218/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0720\n",
      "Epoch 1219/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1220/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1221/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0718\n",
      "Epoch 1222/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0716\n",
      "Epoch 1223/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1224/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1225/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1226/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1227/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0719\n",
      "Epoch 1228/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1229/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0719\n",
      "Epoch 1230/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0719\n",
      "Epoch 1231/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1232/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0720\n",
      "Epoch 1233/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0719\n",
      "Epoch 1234/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0714 - val_loss: 0.0716\n",
      "Epoch 1235/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1236/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1237/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1238/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1239/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0718\n",
      "Epoch 1240/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0720\n",
      "Epoch 1241/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0718\n",
      "Epoch 1242/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0716\n",
      "Epoch 1243/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0718\n",
      "Epoch 1244/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1245/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1246/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1247/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0720\n",
      "Epoch 1248/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1249/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0719\n",
      "Epoch 1250/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1251/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1252/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0716\n",
      "Epoch 1253/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1254/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1255/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1256/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1257/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0718\n",
      "Epoch 1258/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0719\n",
      "Epoch 1259/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1260/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1261/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1262/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0718\n",
      "Epoch 1263/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1264/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0718\n",
      "Epoch 1265/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1266/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0719\n",
      "Epoch 1267/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1268/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0716\n",
      "Epoch 1269/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0719\n",
      "Epoch 1270/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1271/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0716\n",
      "Epoch 1272/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0711 - val_loss: 0.0717\n",
      "Epoch 1273/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1274/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0718\n",
      "Epoch 1275/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1276/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0717\n",
      "Epoch 1277/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0719\n",
      "Epoch 1278/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0715\n",
      "Epoch 1279/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0718\n",
      "Epoch 1280/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0715\n",
      "Epoch 1281/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1282/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1283/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1284/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1285/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1286/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1287/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0714 - val_loss: 0.0720\n",
      "Epoch 1288/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0718\n",
      "Epoch 1289/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1290/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1291/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0716\n",
      "Epoch 1292/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0715\n",
      "Epoch 1293/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0719\n",
      "Epoch 1294/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0720\n",
      "Epoch 1295/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1296/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1297/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0718\n",
      "Epoch 1298/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0719\n",
      "Epoch 1299/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1300/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1301/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0716\n",
      "Epoch 1302/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1303/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0719\n",
      "Epoch 1304/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1305/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1306/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1307/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1308/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1309/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0718\n",
      "Epoch 1310/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0718\n",
      "Epoch 1311/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1312/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1313/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0718\n",
      "Epoch 1314/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0719\n",
      "Epoch 1315/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1316/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0718\n",
      "Epoch 1317/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0718\n",
      "Epoch 1318/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1319/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0716\n",
      "Epoch 1320/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1321/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1322/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0719\n",
      "Epoch 1323/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1324/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0718\n",
      "Epoch 1325/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1326/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1327/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1328/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0718\n",
      "Epoch 1329/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0718\n",
      "Epoch 1330/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0716\n",
      "Epoch 1331/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1332/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1333/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0719\n",
      "Epoch 1334/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1335/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1336/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0720\n",
      "Epoch 1337/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1338/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1339/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0719\n",
      "Epoch 1340/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0711 - val_loss: 0.0718\n",
      "Epoch 1341/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1342/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1343/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0718\n",
      "Epoch 1344/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0719\n",
      "Epoch 1345/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1346/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1347/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0716\n",
      "Epoch 1348/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0718\n",
      "Epoch 1349/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1350/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1351/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0719\n",
      "Epoch 1352/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1353/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1354/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1355/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1356/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1357/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1358/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0719\n",
      "Epoch 1359/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1360/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0716\n",
      "Epoch 1361/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1362/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0718\n",
      "Epoch 1363/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1364/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1365/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1366/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1367/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0718\n",
      "Epoch 1368/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1369/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1370/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0711 - val_loss: 0.0717\n",
      "Epoch 1371/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1372/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0719\n",
      "Epoch 1373/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1374/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1375/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1376/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0717\n",
      "Epoch 1377/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0720\n",
      "Epoch 1378/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0718\n",
      "Epoch 1379/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1380/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0722\n",
      "Epoch 1381/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0716\n",
      "Epoch 1382/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1383/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1384/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.076 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1385/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1386/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0720\n",
      "Epoch 1387/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1388/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0713 - val_loss: 0.0718\n",
      "Epoch 1389/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0716\n",
      "Epoch 1390/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 5ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1391/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0719\n",
      "Epoch 1392/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1393/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0718\n",
      "Epoch 1394/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0716\n",
      "Epoch 1395/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0719\n",
      "Epoch 1396/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0711 - val_loss: 0.0719\n",
      "Epoch 1397/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1398/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0719\n",
      "Epoch 1399/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0716\n",
      "Epoch 1400/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.066 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0716\n",
      "Epoch 1401/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1402/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1403/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1404/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1405/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1406/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1407/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1408/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1409/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0717\n",
      "Epoch 1410/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0720\n",
      "Epoch 1411/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1412/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0711 - val_loss: 0.0719\n",
      "Epoch 1413/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0719\n",
      "Epoch 1414/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1415/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0711 - val_loss: 0.0720\n",
      "Epoch 1416/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0719\n",
      "Epoch 1417/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.077 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0718\n",
      "Epoch 1418/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1419/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1420/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0719\n",
      "Epoch 1421/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1422/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1423/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0719\n",
      "Epoch 1424/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0719\n",
      "Epoch 1425/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0717\n",
      "Epoch 1426/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0716\n",
      "Epoch 1427/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1428/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0720\n",
      "Epoch 1429/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 1430/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0719\n",
      "Epoch 1431/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1432/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.064 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0720\n",
      "Epoch 1433/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0711 - val_loss: 0.0717\n",
      "Epoch 1434/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1435/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0720\n",
      "Epoch 1436/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0720\n",
      "Epoch 1437/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0719\n",
      "Epoch 1438/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0720\n",
      "Epoch 1439/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0711 - val_loss: 0.0719\n",
      "Epoch 1440/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0717\n",
      "Epoch 1441/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0711 - val_loss: 0.0719\n",
      "Epoch 1442/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.065 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0713 - val_loss: 0.0718\n",
      "Epoch 1443/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0719\n",
      "Epoch 1444/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0719\n",
      "Epoch 1445/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1446/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1447/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0716\n",
      "Epoch 1448/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0719\n",
      "Epoch 1449/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1450/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0719\n",
      "Epoch 1451/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0719\n",
      "Epoch 1452/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0717\n",
      "Epoch 1453/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0719\n",
      "Epoch 1454/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0718\n",
      "Epoch 1455/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0720\n",
      "Epoch 1456/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.075 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1457/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0719\n",
      "Epoch 1458/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0718\n",
      "Epoch 1459/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0721\n",
      "Epoch 1460/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1461/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0719\n",
      "Epoch 1462/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0711 - val_loss: 0.0719\n",
      "Epoch 1463/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0719\n",
      "Epoch 1464/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0721\n",
      "Epoch 1465/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0718\n",
      "Epoch 1466/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0718\n",
      "Epoch 1467/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0716\n",
      "Epoch 1468/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0721\n",
      "Epoch 1469/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0721\n",
      "Epoch 1470/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0720\n",
      "Epoch 1471/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0711 - val_loss: 0.0719\n",
      "Epoch 1472/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1473/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 3ms/step - loss: 0.0710 - val_loss: 0.0721\n",
      "Epoch 1474/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0711 - val_loss: 0.0720\n",
      "Epoch 1475/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0710 - val_loss: 0.0718\n",
      "Epoch 1476/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1477/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0718\n",
      "Epoch 1478/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0710 - val_loss: 0.0719\n",
      "Epoch 1479/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0719\n",
      "Epoch 1480/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.067 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0720\n",
      "Epoch 1481/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0717\n",
      "Epoch 1482/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0720\n",
      "Epoch 1483/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.069 - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0712 - val_loss: 0.0720\n",
      "Epoch 1484/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0711 - val_loss: 0.0717\n",
      "Epoch 1485/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 4ms/step - loss: 0.0711 - val_loss: 0.0717\n",
      "Epoch 1486/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.072 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0711 - val_loss: 0.0718\n",
      "Epoch 1487/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.073 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0717\n",
      "Epoch 1488/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0711 - val_loss: 0.0718\n",
      "Epoch 1489/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0720\n",
      "Epoch 1490/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0712 - val_loss: 0.0721\n",
      "Epoch 1491/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.070 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0710 - val_loss: 0.0717\n",
      "Epoch 1492/5000\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.074 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071 - 0s 3ms/step - loss: 0.0711 - val_loss: 0.0719\n",
      "Epoch 1493/5000\n",
      " 39/103 [==========>...................] - ETA: 0s - loss: 0.068 - ETA: 0s - loss: 0.071 - ETA: 0s - loss: 0.071"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Dense(100, activation='relu',\n",
    "                      input_shape=(len(feature_names),),\n",
    "                      kernel_initializer=initializers.he_normal(seed=0)))\n",
    "model.add(layers.Dense(100, activation='relu',\n",
    "                      kernel_initializer=initializers.he_normal(seed=0)))\n",
    "model.add(layers.Dropout(0.7))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=\"adam\",)\n",
    "\n",
    "print('start fitting')\n",
    "history = model.fit(dataset_train[feature_names], dataset_train['rank'],\n",
    "                    batch_size=1000,         #1000  #每一个batch的大小\n",
    "                    epochs=5000, #225          #迭代次数\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    #validation_data =        #(测试集的输入特征，测试集的标签），\n",
    "                    #validation_split =       # 从测试集中划分多少比例给训练集，\n",
    "                    #validation_freq = 20        #测试的epoch间隔数                     \n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x21df0e67988>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 718\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(history.history['val_loss'][1:])\n",
    "plt.plot(history.history['loss'][1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lightgbm Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---cf.fit---\n",
      "LGBMRegressor(n_estimators=5000)\n",
      "---cf.score---\n",
      "-0.07231089691815673\n",
      "---predict---\n",
      "[0.51455435 0.44522221 0.53330591 ... 0.35179236 0.39042122 0.44200831]\n"
     ]
    }
   ],
   "source": [
    "##############################################　　　自己加入的　　　##############################################\n",
    "import lightgbm as lgb\n",
    "\n",
    "cf = lgb.LGBMRegressor(n_estimators=5000)\n",
    "\n",
    "train = dataset_train[feature_names] , dataset_train['return'] > 1\n",
    "test = dataset_test[feature_names] , dataset_test['return'] > 1 \n",
    "\n",
    "print('---cf.fit---')\n",
    "print(cf.fit(*train))\n",
    "print('---cf.score---')\n",
    "print(cf.score(*test))\n",
    "print('---predict---')\n",
    "print(cf.predict(test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 參數優化_1110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[67]\tvalid's auc: 0.554579\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.51555\n",
      "Early stopping, best iteration is:\n",
      "[147]\tvalid's auc: 0.518369\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's auc: 0.544087\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.551571\n",
      "Early stopping, best iteration is:\n",
      "[78]\tvalid's auc: 0.553193\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.517361\n",
      "Early stopping, best iteration is:\n",
      "[136]\tvalid's auc: 0.518437\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's auc: 0.541906\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.552421\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.507774\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[46]\tvalid's auc: 0.54178\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.547298\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.516955\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[67]\tvalid's auc: 0.534395\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid's auc: 0.550254\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.518725\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[35]\tvalid's auc: 0.543392\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[49]\tvalid's auc: 0.555397\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.516685\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.541681\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid's auc: 0.556581\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid's auc: 0.51458\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid's auc: 0.541405\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid's auc: 0.54925\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.513842\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalid's auc: 0.536095\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.555441\n",
      "Early stopping, best iteration is:\n",
      "[86]\tvalid's auc: 0.557525\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.506781\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[27]\tvalid's auc: 0.542405\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[47]\tvalid's auc: 0.555559\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.507011\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[44]\tvalid's auc: 0.54261\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.555885\n",
      "Early stopping, best iteration is:\n",
      "[94]\tvalid's auc: 0.55649\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.513427\n",
      "Early stopping, best iteration is:\n",
      "[88]\tvalid's auc: 0.515041\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[10]\tvalid's auc: 0.542988\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.55594\n",
      "Early stopping, best iteration is:\n",
      "[90]\tvalid's auc: 0.557523\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.512139\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid's auc: 0.541477\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid's auc: 0.559351\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.509334\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid's auc: 0.542674\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.557532\n",
      "Early stopping, best iteration is:\n",
      "[89]\tvalid's auc: 0.557925\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.515478\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.540646\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.54939\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.516463\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid's auc: 0.540092\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.553783\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.526543\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's auc: 0.542436\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.552433\n",
      "Early stopping, best iteration is:\n",
      "[89]\tvalid's auc: 0.5537\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.499548\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.54618\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.554582\n",
      "Early stopping, best iteration is:\n",
      "[114]\tvalid's auc: 0.556176\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.502753\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's auc: 0.540816\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's auc: 0.550304\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.528814\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.533327\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.563194\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.522146\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[53]\tvalid's auc: 0.539511\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid's auc: 0.54817\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.519901\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[52]\tvalid's auc: 0.531185\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid's auc: 0.55149\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.512617\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid's auc: 0.544445\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.549976\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.512238\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[24]\tvalid's auc: 0.538008\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.554439\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.505695\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid's auc: 0.544525\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.555706\n",
      "Early stopping, best iteration is:\n",
      "[112]\tvalid's auc: 0.557142\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.509219\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid's auc: 0.537481\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.550533\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.507647\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid's auc: 0.544386\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.552519\n",
      "Early stopping, best iteration is:\n",
      "[149]\tvalid's auc: 0.556563\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.511934\n",
      "Early stopping, best iteration is:\n",
      "[107]\tvalid's auc: 0.513118\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.550347\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid's auc: 0.54888\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.501683\n",
      "Early stopping, best iteration is:\n",
      "[139]\tvalid's auc: 0.506023\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[61]\tvalid's auc: 0.532284\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid's auc: 0.557472\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5122\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalid's auc: 0.542584\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.55776\n",
      "Early stopping, best iteration is:\n",
      "[123]\tvalid's auc: 0.557894\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.500465\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[46]\tvalid's auc: 0.538735\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid's auc: 0.550391\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.510537\n",
      "[200]\tvalid's auc: 0.515558\n",
      "Early stopping, best iteration is:\n",
      "[228]\tvalid's auc: 0.51711\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.543469\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.555766\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.516654\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[27]\tvalid's auc: 0.537174\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.557278\n",
      "Early stopping, best iteration is:\n",
      "[143]\tvalid's auc: 0.558795\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.505852\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[28]\tvalid's auc: 0.543049\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.553788\n",
      "Early stopping, best iteration is:\n",
      "[82]\tvalid's auc: 0.555078\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.512913\n",
      "Early stopping, best iteration is:\n",
      "[134]\tvalid's auc: 0.51763\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid's auc: 0.539606\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.554959\n",
      "Early stopping, best iteration is:\n",
      "[125]\tvalid's auc: 0.556847\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.514261\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[47]\tvalid's auc: 0.542363\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.555662\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.508366\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[42]\tvalid's auc: 0.537427\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid's auc: 0.547667\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.51681\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.530353\n",
      "Early stopping, best iteration is:\n",
      "[78]\tvalid's auc: 0.531777\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[58]\tvalid's auc: 0.557868\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.511673\n",
      "Early stopping, best iteration is:\n",
      "[96]\tvalid's auc: 0.512637\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid's auc: 0.538962\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid's auc: 0.549075\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.515324\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[42]\tvalid's auc: 0.539686\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.552393\n",
      "Early stopping, best iteration is:\n",
      "[108]\tvalid's auc: 0.553851\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.50508\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[42]\tvalid's auc: 0.538395\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid's auc: 0.550653\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.508692\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.541179\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.548648\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.501845\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.538729\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.55134\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.507572\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[44]\tvalid's auc: 0.542056\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid's auc: 0.552494\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.510874\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.535208\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.55604\n",
      "Early stopping, best iteration is:\n",
      "[88]\tvalid's auc: 0.55789\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid's auc: 0.513842\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.536011\n",
      "Early stopping, best iteration is:\n",
      "[90]\tvalid's auc: 0.537913\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalid's auc: 0.548476\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.504415\n",
      "[200]\tvalid's auc: 0.514125\n",
      "[300]\tvalid's auc: 0.51761\n",
      "Early stopping, best iteration is:\n",
      "[320]\tvalid's auc: 0.518158\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[60]\tvalid's auc: 0.535013\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[68]\tvalid's auc: 0.556236\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.515495\n",
      "Early stopping, best iteration is:\n",
      "[129]\tvalid's auc: 0.516431\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[42]\tvalid's auc: 0.538025\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.550619\n",
      "Early stopping, best iteration is:\n",
      "[138]\tvalid's auc: 0.556352\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.510874\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid's auc: 0.540017\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.549061\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.523693\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's auc: 0.538494\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.55499\n",
      "Early stopping, best iteration is:\n",
      "[139]\tvalid's auc: 0.558965\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.517534\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid's auc: 0.547701\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid's auc: 0.556045\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[66]\tvalid's auc: 0.514464\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid's auc: 0.541939\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.559177\n",
      "Early stopping, best iteration is:\n",
      "[109]\tvalid's auc: 0.559925\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.515759\n",
      "[200]\tvalid's auc: 0.518689\n",
      "Early stopping, best iteration is:\n",
      "[200]\tvalid's auc: 0.518689\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.542128\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.555553\n",
      "Early stopping, best iteration is:\n",
      "[80]\tvalid's auc: 0.55737\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.512355\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[38]\tvalid's auc: 0.536236\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.554917\n",
      "Early stopping, best iteration is:\n",
      "[142]\tvalid's auc: 0.556714\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.51658\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.544113\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[49]\tvalid's auc: 0.556271\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.515636\n",
      "Early stopping, best iteration is:\n",
      "[120]\tvalid's auc: 0.516398\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalid's auc: 0.541861\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.561132\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.519936\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[32]\tvalid's auc: 0.540015\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid's auc: 0.54992\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.533817\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid's auc: 0.545854\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.549419\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.520595\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[57]\tvalid's auc: 0.537947\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.551406\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.518386\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid's auc: 0.5372\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid's auc: 0.552619\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.514116\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[44]\tvalid's auc: 0.541692\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.55523\n",
      "Early stopping, best iteration is:\n",
      "[88]\tvalid's auc: 0.556409\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.513058\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[44]\tvalid's auc: 0.542914\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's auc: 0.54926\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5287\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[47]\tvalid's auc: 0.543482\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's auc: 0.553774\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.527487\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid's auc: 0.543711\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.554137\n",
      "Early stopping, best iteration is:\n",
      "[90]\tvalid's auc: 0.554935\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.508028\n",
      "[200]\tvalid's auc: 0.515345\n",
      "Early stopping, best iteration is:\n",
      "[247]\tvalid's auc: 0.517626\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.543543\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.553695\n",
      "[200]\tvalid's auc: 0.555838\n",
      "Early stopping, best iteration is:\n",
      "[183]\tvalid's auc: 0.556896\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.506634\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.539128\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.552729\n",
      "Early stopping, best iteration is:\n",
      "[95]\tvalid's auc: 0.554415\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.511751\n",
      "[200]\tvalid's auc: 0.516206\n",
      "Early stopping, best iteration is:\n",
      "[218]\tvalid's auc: 0.51659\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.542972\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.551232\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.513721\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid's auc: 0.540625\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.555897\n",
      "Early stopping, best iteration is:\n",
      "[110]\tvalid's auc: 0.555956\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.514938\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.548413\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.553824\n",
      "Early stopping, best iteration is:\n",
      "[95]\tvalid's auc: 0.554263\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.509764\n",
      "Early stopping, best iteration is:\n",
      "[119]\tvalid's auc: 0.513938\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.545274\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid's auc: 0.549615\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.51996\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[54]\tvalid's auc: 0.536735\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3]\tvalid's auc: 0.551493\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.512918\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[57]\tvalid's auc: 0.538544\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's auc: 0.550045\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.510803\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[34]\tvalid's auc: 0.538411\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.553633\n",
      "Early stopping, best iteration is:\n",
      "[122]\tvalid's auc: 0.555785\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.514474\n",
      "Early stopping, best iteration is:\n",
      "[120]\tvalid's auc: 0.514937\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[31]\tvalid's auc: 0.54248\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.554945\n",
      "Early stopping, best iteration is:\n",
      "[104]\tvalid's auc: 0.555686\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.51494\n",
      "Early stopping, best iteration is:\n",
      "[91]\tvalid's auc: 0.515866\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid's auc: 0.53817\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.547266\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.511718\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid's auc: 0.536917\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid's auc: 0.546387\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.510615\n",
      "[200]\tvalid's auc: 0.514261\n",
      "Early stopping, best iteration is:\n",
      "[176]\tvalid's auc: 0.515052\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[41]\tvalid's auc: 0.533842\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid's auc: 0.56121\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.514088\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.537105\n",
      "Early stopping, best iteration is:\n",
      "[102]\tvalid's auc: 0.537244\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.549902\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.512831\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's auc: 0.539285\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.554112\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.51192\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[30]\tvalid's auc: 0.540674\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.553593\n",
      "Early stopping, best iteration is:\n",
      "[112]\tvalid's auc: 0.555148\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.516083\n",
      "Early stopping, best iteration is:\n",
      "[120]\tvalid's auc: 0.519451\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid's auc: 0.538443\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.554459\n",
      "Early stopping, best iteration is:\n",
      "[99]\tvalid's auc: 0.554934\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.507476\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid's auc: 0.540041\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.552726\n",
      "Early stopping, best iteration is:\n",
      "[85]\tvalid's auc: 0.55441\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.509787\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.539134\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.55095\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.511393\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[53]\tvalid's auc: 0.540178\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[43]\tvalid's auc: 0.554952\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.519235\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid's auc: 0.544606\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.556594\n",
      "Early stopping, best iteration is:\n",
      "[108]\tvalid's auc: 0.557658\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.514438\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid's auc: 0.541869\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's auc: 0.548021\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.526556\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid's auc: 0.543081\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.5\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.546378\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.517741\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[26]\tvalid's auc: 0.544698\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalid's auc: 0.553713\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.505897\n",
      "Early stopping, best iteration is:\n",
      "[141]\tvalid's auc: 0.509475\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[56]\tvalid's auc: 0.542356\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.552337\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.509545\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[33]\tvalid's auc: 0.53726\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid's auc: 0.553903\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.514533\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[22]\tvalid's auc: 0.539473\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid's auc: 0.53085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=LGBMClassifier(metric='None', n_estimators=5000,\n",
       "                                            n_jobs=4, random_state=314),\n",
       "                   n_iter=100,\n",
       "                   param_distributions={'colsample_bytree': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000021DF25ED088>,\n",
       "                                        'min_child_samples': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000021DF25E8B88>,\n",
       "                                        'min_child_weight': [1e-05, 0.001, 0.01,\n",
       "                                                             0.1, 1, 10.0,\n",
       "                                                             100.0, 1000.0,\n",
       "                                                             10000.0],\n",
       "                                        'num_leaves': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000021DCBDEF408>,\n",
       "                                        'reg_alpha': [0, 0.1, 1, 2, 5, 7, 10,\n",
       "                                                      50, 100],\n",
       "                                        'reg_lambda': [0, 0.1, 1, 5, 10, 20, 50,\n",
       "                                                       100],\n",
       "                                        'subsample': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000021DF25E8C88>},\n",
       "                   random_state=314, scoring='roc_auc', verbose=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import lightgbm\n",
    "\n",
    "fit_params={\"early_stopping_rounds\":30, \n",
    "            \"eval_metric\" : 'auc', \n",
    "            \"eval_set\" : [test],\n",
    "            'eval_names': ['valid'],\n",
    "            'verbose': 100,\n",
    "            'categorical_feature': 'auto'}\n",
    "\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "param_test ={'num_leaves': sp_randint(6, 50), \n",
    "             'min_child_samples': sp_randint(100, 500), \n",
    "             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "             'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
    "             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
    "             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n",
    "\n",
    "#This parameter defines the number of HP points to be tested\n",
    "n_HP_points_to_test = 100\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "#n_estimators is set to a \"large value\". The actual number of trees build will depend on early stopping and 5000 define only the absolute maximum\n",
    "clf = lgb.LGBMClassifier(max_depth=-1, random_state=314, silent=True, metric='None', n_jobs=4, n_estimators=5000)\n",
    "gs = RandomizedSearchCV(\n",
    "    estimator=clf, param_distributions=param_test, \n",
    "    n_iter=n_HP_points_to_test,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    refit=True,\n",
    "    random_state=314,\n",
    "    verbose=True)\n",
    "\n",
    "gs.fit(*train, **fit_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(colsample_bytree=0.5295549531948086, metric='None',\n",
       "               min_child_samples=372, n_estimators=5000, n_jobs=4,\n",
       "               num_leaves=43, random_state=314, reg_alpha=2, reg_lambda=20,\n",
       "               subsample=0.2633722218193496)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#gs.best_estimator_\n",
    "#LGBMClassifier(colsample_bytree=0.6433117836032942, metric='None',\n",
    "#               min_child_samples=224, min_child_weight=1e-05, n_estimators=5000,\n",
    "#               n_jobs=4, num_leaves=20, random_state=314, reg_alpha=10,\n",
    "#               reg_lambda=10, subsample=0.8945613420997809)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 30 rounds.\n",
      "[100]\tvalid's auc: 0.541944\n",
      "Early stopping, best iteration is:\n",
      "[126]\tvalid's auc: 0.543059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.008445614318477235"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf = lgb.LGBMRegressor(colsample_bytree=0.4893610123694421, metric='None',\n",
    "               min_child_samples=325, n_estimators=5000, n_jobs=4,\n",
    "               num_leaves=44, random_state=314, reg_alpha=7, reg_lambda=20,\n",
    "               subsample=0.3338474163716765)\n",
    "\n",
    "cf.fit(dataset_train[feature_names],dataset_train['return'] > 1, **fit_params)\n",
    "cf.score(dataset_test[feature_names],dataset_test['return'] > 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Training until validation scores don't improve for 30 rounds.\n",
    "[100]\tvalid's auc: 0.551389\n",
    "Early stopping, best iteration is:\n",
    "[89]\tvalid's auc: 0.553699\n",
    "0.003857956228475623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(cf.fit(*train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import lightgbm as lgb\n",
    "#gbm = lgb.LGBMClassifier(n_estimators=100, random_state=5, learning_rate=0.01)\n",
    "#gbm.fit(dataset_train[feature_names], dataset_train['return'] > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tqdm\n",
    "#\n",
    "#n = 3\n",
    "#\n",
    "#X = []\n",
    "#y = []\n",
    "#indexes = []\n",
    "#dataset_scaled_x = dataset_scaled[feature_names]\n",
    "#\n",
    "#for i in tqdm.tqdm_notebook(range(0, len(dataset_scaled)-n)):\n",
    "#    X.append(dataset_scaled_x.iloc[i:i+n].values)\n",
    "#    y.append(dataset_scaled['return'].iloc[i+n-1])\n",
    "#    indexes.append(dataset_scaled.index[i+n-1])\n",
    "##dataset_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#X = np.array(X)\n",
    "#y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import lightgbm as lgb\n",
    "#cf = lgb.LGBMRegressor(colsample_bytree=0.7740467183023685, metric='None',\n",
    "#               min_child_samples=395, min_child_weight=0.01, n_estimators=5000,\n",
    "#               n_jobs=4, num_leaves=9, random_state=314, reg_alpha=5,\n",
    "#               reg_lambda=10, subsample=0.4643892520208455)\n",
    "#    \n",
    "#cf.fit(dataset_train[feature_names].astype(float), dataset_train['rank'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "cf2 = RandomForestRegressor(n_estimators=100)\n",
    "cf2.fit(dataset_train[feature_names].astype(float), dataset_train['rank'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 參數優化_1110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint \n",
    "from sklearn.model_selection import RandomizedSearchCV \n",
    "# build a classifier \n",
    "clf = RandomForestRegressor(n_estimators=100) \n",
    "# specify parameters and distributions to sample from \n",
    "param_dist = {\"max_depth\": [3, None], \n",
    "              \"max_features\": sp_randint(1, 11), \n",
    "              \"min_samples_split\": sp_randint(2, 11), \n",
    "              \"min_samples_leaf\": sp_randint(1, 11), \n",
    "              \"bootstrap\": [True, False], \n",
    "              \"criterion\": [\"mse\", \"mae\"]} \n",
    "# run randomized search \n",
    "n_iter_search = 20 \n",
    "rs = RandomizedSearchCV(clf, param_distributions=param_dist, \n",
    "                                   n_iter=n_iter_search) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "##rs.fit(dataset_train[features], dataset_train['return'] > 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split Train Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Value', ylabel='Feature'>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_imp = pd.DataFrame(zip(cf.feature_importances_, feature_names), \n",
    "                           columns=['Value','Feature']).sort_values('Value', ascending=False)\n",
    "feature_imp\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = dataset.index.get_level_values('date') < '2021'\n",
    "dataset_train = dataset[select]\n",
    "dataset_test = dataset[~select]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_drop = dataset.dropna(subset=feature_names+['return'])\n",
    "\n",
    "vals = model.predict(dataset_drop[feature_names].astype(float))\n",
    "dataset_drop['result1'] = pd.Series(vals.swapaxes(0,1)[0], dataset_drop.index)\n",
    "\n",
    "vals = cf.predict(dataset_drop[feature_names].astype(float))\n",
    "dataset_drop['result2'] = pd.Series(vals, dataset_drop.index)\n",
    "\n",
    "vals = cf2.predict(dataset_drop[feature_names].astype(float))\n",
    "dataset_drop['result3'] = pd.Series(vals, dataset_drop.index)\n",
    "\n",
    "dataset_drop = dataset_drop.reset_index().set_index(\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把量加進來做篩選\n",
    " * https://hahow.in/courses/5b9d3a6dca498a001e917383/shapeussions/60c96f5b018697e8a6131cbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>stock_id</th>\n",
       "      <th>0015</th>\n",
       "      <th>0050</th>\n",
       "      <th>0051</th>\n",
       "      <th>0052</th>\n",
       "      <th>0053</th>\n",
       "      <th>0054</th>\n",
       "      <th>0055</th>\n",
       "      <th>0056</th>\n",
       "      <th>0057</th>\n",
       "      <th>0058</th>\n",
       "      <th>...</th>\n",
       "      <th>9944</th>\n",
       "      <th>9945</th>\n",
       "      <th>9946</th>\n",
       "      <th>9949</th>\n",
       "      <th>9950</th>\n",
       "      <th>9951</th>\n",
       "      <th>9955</th>\n",
       "      <th>9958</th>\n",
       "      <th>9960</th>\n",
       "      <th>9962</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-04-23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-04-24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-04-25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-04-26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-04-27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-08-25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3787 rows × 1998 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "stock_id    0015  0050  0051  0052  0053  0054  0055  0056  0057  0058  ...  \\\n",
       "date                                                                    ...   \n",
       "2007-04-23   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2007-04-24   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2007-04-25   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2007-04-26   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2007-04-27   0.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "...          ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "2022-08-19   0.0   1.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0  ...   \n",
       "2022-08-22   0.0   1.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0  ...   \n",
       "2022-08-23   0.0   1.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0  ...   \n",
       "2022-08-24   0.0   1.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0  ...   \n",
       "2022-08-25   0.0   1.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0  ...   \n",
       "\n",
       "stock_id    9944  9945  9946  9949  9950  9951  9955  9958  9960  9962  \n",
       "date                                                                    \n",
       "2007-04-23   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2007-04-24   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2007-04-25   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2007-04-26   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2007-04-27   1.0   1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0  \n",
       "...          ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "2022-08-19   0.0   1.0   0.0   0.0   0.0   0.0   1.0   1.0   0.0   0.0  \n",
       "2022-08-22   0.0   1.0   0.0   0.0   0.0   0.0   0.0   1.0   0.0   0.0  \n",
       "2022-08-23   0.0   1.0   0.0   0.0   0.0   0.0   1.0   1.0   0.0   0.0  \n",
       "2022-08-24   0.0   1.0   0.0   0.0   0.0   0.0   1.0   1.0   0.0   0.0  \n",
       "2022-08-25   0.0   1.0   0.0   0.0   0.0   0.0   1.0   1.0   0.0   0.0  \n",
       "\n",
       "[3787 rows x 1998 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#把量加進來\n",
    "vol=data.get('成交股數')/1000\n",
    "vol_ma5=vol.rolling(5).mean()\n",
    "\n",
    "vol_filter=vol_ma5>1000\n",
    "vol_filter=vol_filter[vol_filter].fillna(0).astype(float)\n",
    "vol_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#t1 = vol_ma5.iloc[-1].dropna()\n",
    "#t1.to_csv('./tmp/132.csv')\n",
    "#t1.hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='date'>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "dates = sorted(list(set(dataset_drop.index)))\n",
    "\n",
    "rs = []\n",
    "for d in dates:\n",
    "    \n",
    "    dataset_time = dataset_drop.loc[d]\n",
    "    \n",
    "    dataset_time = drop_extreme_case(dataset_time , list1 , thresh=0.01)\n",
    "    \n",
    "    rank = (dataset_time['result1'] + dataset_time['result2'] + dataset_time['result3'] ) #* vol_filter.loc[d]\n",
    "\n",
    "\n",
    "    condition = (rank >= rank.nlargest(20).iloc[-1]) \n",
    "    \n",
    "    r = dataset_time['return'][condition].mean()\n",
    "\n",
    "    rs.append(r * (1-3/1000-1.425/1000*2*0.6))\n",
    "\n",
    "rs = pd.Series(rs, index=dates)['2021':].cumprod()\n",
    "\n",
    "s0050 = close['0050']['2021':]\n",
    "\n",
    "pd.DataFrame({'nn strategy return':rs.reindex(s0050.index, method='ffill'), '0050 return':s0050/s0050[0]}).plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#\n",
    "#return_history_1026 = pd.Series(rs, index=dates)['2021':].cumprod()\n",
    "##eq = (gain[hold == 1].mean(axis=1)).fillna(1).cumprod()\n",
    "#\n",
    "#pickle.dump(rs, open('return_history_1026.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyfolio as pf\n",
    "#import pandas as pd\n",
    "#\n",
    "#close.index = close.index.tz_localize(\"Asia/Taipei\")\n",
    "##pf.create_returns_tear_sheet(close['0050'].pct_change())\n",
    "#\n",
    "## 得到 上一個單元的 回測結果\n",
    "#ret = pickle.load(open(\"return_history_1026.pkl\", \"rb\"))\n",
    "#\n",
    "## 將回測報酬率取出來\n",
    "#ret = ret.pct_change().dropna()\n",
    "#ret.index = pd.to_datetime(ret.index).tz_localize('Asia/Taipei')\n",
    "#\n",
    "## 利用pyfolio 比較報酬率\n",
    "#pf.create_returns_tear_sheet(ret, benchmark_rets=close['0050'].pct_change())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 當月持股狀況"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.index.levels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the latest dataset\n",
    "last_date = dataset.index.levels[1].max()#\"2022-10-15\"\n",
    "is_last_date = dataset.index.get_level_values('date') == last_date\n",
    "last_dataset = dataset[is_last_date].copy()\n",
    "\n",
    "\n",
    "last_dataset = drop_extreme_case(last_dataset, list1 , thresh=0.01)\n",
    "\n",
    "\n",
    "# remove NaN testcases\n",
    "last_dataset = last_dataset.dropna(subset=feature_names)\n",
    "\n",
    "# predict\n",
    "\n",
    "vals = model.predict(last_dataset[feature_names].astype(float))\n",
    "last_dataset['result1'] = pd.Series(vals.swapaxes(0,1)[0], last_dataset.index)\n",
    "\n",
    "vals = cf.predict(last_dataset[feature_names].astype(float))\n",
    "last_dataset['result2'] = pd.Series(vals, last_dataset.index)\n",
    "\n",
    "vals = cf2.predict(last_dataset[feature_names].astype(float))\n",
    "last_dataset['result3'] = pd.Series(vals, last_dataset.index)\n",
    "\n",
    "\n",
    "# calculate score\n",
    "\n",
    "rank = last_dataset['result1'] + last_dataset['result2'] + last_dataset['result3']\n",
    "\n",
    "#\n",
    "rank = rank * vol_filter.iloc[-1] #******加上量的濾網\n",
    "\n",
    "condition = (rank >= rank.nlargest(20).iloc[-1])\n",
    "#vol_filter\n",
    "\n",
    "# plot rank distribution\n",
    "rank.hist(bins=20)\n",
    "\n",
    "\n",
    "# show the best 20 stocks\n",
    "slist1 = rank[condition].reset_index()['stock_id']\n",
    "\n",
    "#https://keras-cn.readthedocs.io/en/latest/models/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stock_id\n",
       "0015    0.0\n",
       "0050    1.0\n",
       "0051    0.0\n",
       "0052    0.0\n",
       "0053    0.0\n",
       "       ... \n",
       "9951    0.0\n",
       "9955    1.0\n",
       "9958    1.0\n",
       "9960    0.0\n",
       "9962    0.0\n",
       "Name: 2022-08-25 00:00:00, Length: 1998, dtype: float64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vol_filter.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date = dataset.index.levels[1].max()#\"2022-10-15\"\n",
    "is_last_date = dataset.index.get_level_values('date') == last_date\n",
    "last_dataset = dataset[is_last_date].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank.sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 平均分配資產於股票之中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "股票平分張數:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "stock_id\n",
       "1305    0.123457\n",
       "1514    0.082988\n",
       "1526    0.054250\n",
       "1701    0.133630\n",
       "2049    0.013761\n",
       "2368    0.035800\n",
       "2606    0.075094\n",
       "2915    0.046154\n",
       "3006    0.036452\n",
       "3038    0.130152\n",
       "3189    0.023256\n",
       "4736    0.014118\n",
       "4763    0.024390\n",
       "4927    0.050847\n",
       "6113    0.112994\n",
       "6142    0.301508\n",
       "6187    0.036408\n",
       "6213    0.044510\n",
       "6274    0.052817\n",
       "8415    0.084626\n",
       "Name: 2022-08-25 00:00:00, dtype: float64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close = data.get(\"收盤價\")\n",
    "\n",
    "money = 60000\n",
    "stock_prices = close[slist1].iloc[-1]\n",
    "\n",
    "\n",
    "print(\"股票平分張數:\")\n",
    "money / len(stock_prices) / stock_prices / 1000\n"
   ]
  }
 ],
 "metadata": {
  "@deathbeds/ipydrawio": {
   "xml": "<mxfile host=\"17-0536659-02\" modified=\"2022-10-27T03:01:05.740Z\" agent=\"5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\" etag=\"8bODyUWCdQaexky56D9k\" version=\"20.2.8\" type=\"embed\"><diagram id=\"9nsO6hMlLNMTvIbReD-d\" name=\"第1頁\"><mxGraphModel dx=\"1458\" dy=\"721\" grid=\"1\" gridSize=\"10\" guides=\"1\" tooltips=\"1\" connect=\"1\" arrows=\"1\" fold=\"1\" page=\"1\" pageScale=\"1\" pageWidth=\"827\" pageHeight=\"1169\" math=\"0\" shadow=\"0\"><root><mxCell id=\"0\"/><mxCell id=\"1\" parent=\"0\"/><UserObject label=\"Tree Root\" treeRoot=\"1\" id=\"2\"><mxCell style=\"align=center;collapsible=0;container=1;recursiveResize=0;\" parent=\"1\" vertex=\"1\"><mxGeometry x=\"40\" y=\"40\" width=\"120\" height=\"60\" as=\"geometry\"/></mxCell></UserObject></root></mxGraphModel></diagram></mxfile>"
  },
  "kernelspec": {
   "display_name": "finlab",
   "language": "python",
   "name": "finlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
