{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[版本連結](http://localhost:8888/doc/tree/!%25E9%2581%25B8%25E8%2582%25A1%25E5%259F%25B7%25E8%25A1%258C_%25E7%2589%2588%25E6%259C%25AC%25E6%25BC%2594%25E9%2580%25B2.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 移除不必要的警告\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 獲取歷史資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from finlab.data import Data\n",
    "#from finlab.ml import fundamental_features\n",
    "#fdf = fundamental_features()\n",
    "\n",
    "data = Data()\n",
    "\n",
    "close = data.get(\"收盤價\")\n",
    "open_ = data.get(\"開盤價\")\n",
    "high = data.get(\"最高價\")\n",
    "low = data.get(\"最低價\")\n",
    "vol = data.get(\"成交股數\")\n",
    "\n",
    "PB = data.get(\"股價淨值比\")\n",
    "pe = data.get(\"本益比\")\n",
    "\n",
    "#bi = data.get(\"營業利益\")\n",
    "\n",
    "\n",
    "#close = data.get_adj(\"收盤價\").round(2)\n",
    "\n",
    "#財務指標\n",
    "rev = data.get(\"當月營收\")\n",
    "l_rev = data.get(\"去年當月營收\")\n",
    "\n",
    "#t123 = data.get('土地')\n",
    "\n",
    "bargin_i=data.get(\"投信買賣超股數\")/data.get(\"成交股數\")\n",
    "bargin_f=data.get(\"外資自營商買賣超股數\")/data.get(\"成交股數\")\n",
    "bargin_s=data.get(\"自營商買賣超股數(自行買賣)\")/data.get(\"成交股數\")\n",
    "#\n",
    "\n",
    "vol=data.get('成交股數')/1000\n",
    "vol_ma5=vol.rolling(5).mean()\n",
    "\n",
    "rev.index = rev.index.shift(5, \"d\")         #每月頻率\n",
    "#周頻率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MFI  = data.talib(\"MFI\")\n",
    "##MFI.tail()\n",
    "#ub,mb,lb =data.talib(\"BBANDS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 營收相關"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################　　　自己加入的　　　##############################################\n",
    "import pandas as pd\n",
    "from finlab.__init__ import talib_all_stock\n",
    "from talib import abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias(n):\n",
    "    return close / close.rolling(n, min_periods=1).mean()\n",
    "\n",
    "def acc(n):\n",
    "    return close.shift(n) / (close.shift(2*n) + close) * 2\n",
    "\n",
    "def rsv(n):\n",
    "    l = close.rolling(n, min_periods=1).min() \n",
    "    h = close.rolling(n, min_periods=1).max()\n",
    "    \n",
    "    return (close - l) / (h - l)\n",
    "\n",
    "def mom(n):\n",
    "    return (rev / rev.shift(1)).shift(n)\n",
    "\n",
    "def yoy(n):\n",
    "    return (rev.shift(n) / rev.shift(12+n)) -1\n",
    "\n",
    "def bi(n):\n",
    "    return bargin_i/bargin_i.rolling(n).mean()\n",
    "\n",
    "def bf(n):\n",
    "    return bargin_f/bargin_f.rolling(n).mean()\n",
    "    \n",
    "def bs(n):\n",
    "    return bargin_s/bargin_s.rolling(n).mean()\n",
    "\n",
    "#-------------------------------------------\n",
    "\n",
    "features = {\n",
    "    'mom1': mom(1),\n",
    "    'mom2': mom(2),\n",
    "    'mom3': mom(3),\n",
    "    'mom4': mom(4),\n",
    "    'mom5': mom(5),\n",
    "    'mom6': mom(6),\n",
    "    'mom7': mom(7),\n",
    "    'mom8': mom(8),\n",
    "    'mom9': mom(9),\n",
    "\n",
    "    \n",
    "    'bias5': bias(5),\n",
    "    'bias10': bias(10),\n",
    "    'bias20': bias(20),\n",
    "    'bias60': bias(60),\n",
    "    'bias120': bias(120),\n",
    "    'bias240': bias(240),\n",
    "    \n",
    "    'acc5': acc(5),\n",
    "    'acc10': acc(10),\n",
    "    'acc20': acc(20),\n",
    "    'acc60': acc(60),\n",
    "    'acc120': acc(120),\n",
    "    'acc240': acc(240),\n",
    "    \n",
    "    'rsv5': rsv(5),\n",
    "    'rsv10': rsv(10),\n",
    "    'rsv20': rsv(20),\n",
    "    'rsv60': rsv(60),\n",
    "    'rsv120': rsv(120),\n",
    "    'rsv240': rsv(240),\n",
    "###############################################\n",
    "    'yoy': yoy(1),\n",
    "    'delta_yoy':yoy(1)-yoy(2),\n",
    "    \n",
    "    'PB':PB,\n",
    "    'PE':pe,\n",
    "    \n",
    "   #'bi5' : bi(5),\n",
    "   #'bi10' : bi(10),\n",
    "   #'bi20' : bi(20),\n",
    "   #'bi60' : bi(60),\n",
    "   # \n",
    "   #'bf5' : bf(5),\n",
    "   # #'bf10' : bf(10),\n",
    "   # #'bf20' : bf(20),\n",
    "   # #'bf60' : bf(60),\n",
    "   # \n",
    "   # 'bs5' : bs(5),\n",
    "   # 'bs10' : bs(10),\n",
    "   # 'bs20' : bs(20),\n",
    "   # 'bs60' : bs(60),\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bi(60).dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 財報指標"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "兩個feature結合[[連結網址]](https://hahow.in/courses/5b9d3a6dca498a001e917383/discussions/5d18b63eac23d80020ae4ce7)\n",
    "\n",
    "---\n",
    "```python\n",
    "from finlab import ml\n",
    "from finlab.data import Data\n",
    "\n",
    "data = Data()\n",
    "rsi = data.talib(\"RSI\")\n",
    "\n",
    "dataset = ml.fundamental_features()\n",
    "ml.add_feature(dataset, 'RSI', rsi)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finlab.ml import fundamental_features\n",
    "dataset_fundamental = fundamental_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_fundamental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 技術指標"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# https://zhuanlan.zhihu.com/p/342075180 talib函数功能一览表\n",
    "\n",
    "def bias(n):\n",
    "    return close / close.rolling(n, min_periods=1).mean()\n",
    "\n",
    "def acc(n):\n",
    "    return close.shift(n) / (close.shift(2*n) + close) * 2\n",
    "\n",
    "def rsv(n):\n",
    "    l = close.rolling(n, min_periods=1).min()\n",
    "    h = close.rolling(n, min_periods=1).max()\n",
    "    \n",
    "    return (close - l) / (h - l)\n",
    "\n",
    "def mom(n):\n",
    "    return (rev / rev.shift(1)).shift(n)\n",
    "\n",
    "\n",
    "def bi_(n):\n",
    "    return (bargin_i / vol.shift(1)).shift(n)\n",
    "\n",
    "def bf(n):\n",
    "    return (bargin_f / vol.shift(1)).shift(n)\n",
    "    \n",
    "def bs(n):\n",
    "    return (bargin_s / vol.shift(1)).shift(n)\n",
    "\n",
    "def rsi(n):\n",
    "    #return talib_all_stock(ndays=10000, func=abstract.RSI, timeperiod=n)\n",
    "    return data.talib(\"RSI\",timeperiod=n)\n",
    "\n",
    "def MFI(n):\n",
    "    return data.talib(\"MFI\",timeperiod=n)\n",
    "\n",
    "def obv(n):\n",
    "    return data.talib(\"OBV\",timeperiod=n)\n",
    "\n",
    "\n",
    "\n",
    "features = {\n",
    "    \n",
    "    #'ATR14':data.talib(\"ATR\",timeperiod=14),\n",
    "    #'NATR14':data.talib('NATR',timeperiod=14),\n",
    "    #'TRANGE':data.talib('TRANGE'),\n",
    "    #'Adosc3':data.talib('ADOSC',timeperiod=3),\n",
    "    \n",
    "    #\"MFI5\":MFI(5),\n",
    "    #\"MFI10\":MFI(10),\n",
    "    \n",
    "    #'rsi6': rsi(6),  #DataFrame\n",
    "    #'rsi10': rsi(10),  #DataFrame\n",
    "    #'rsi14': rsi(14),  #DataFrame\n",
    "    #'rsi20': rsi(20),  #DataFrame\n",
    "    #'rsi50': rsi(50),  #DataFrame\n",
    "   \n",
    "    'mom1': mom(1),\n",
    "    'mom2': mom(2),\n",
    "    'mom3': mom(3),\n",
    "    'mom4': mom(4),\n",
    "    'mom5': mom(5),\n",
    "    'mom6': mom(6),\n",
    "    'mom7': mom(7),\n",
    "    'mom8': mom(8),\n",
    "    'mom9': mom(9),\n",
    "    \n",
    "    'yoy': yoy(1),\n",
    "    'delta_yoy':yoy(1)-yoy(2),\n",
    "    \n",
    "#    'ff':ff,\n",
    "    'PB':PB,\n",
    "    'PE':pe,   \n",
    "#  \n",
    "    'bias5': bias(5),\n",
    "    'bias10': bias(10),\n",
    "    'bias20': bias(20),\n",
    "    'bias60': bias(60),\n",
    "    'bias120': bias(120),\n",
    "    'bias240': bias(240),\n",
    "    \n",
    "    'acc5': acc(5),\n",
    "    'acc10': acc(10),\n",
    "    'acc20': acc(20),\n",
    "    'acc60': acc(60),\n",
    "    'acc120': acc(120),\n",
    "    'acc240': acc(240),\n",
    "    \n",
    "    #'rsv5': rsv(5),\n",
    "    #'rsv10': rsv(10),\n",
    "    #'rsv20': rsv(20),\n",
    "    #'rsv60': rsv(60),\n",
    "    #'rsv120': rsv(120),\n",
    "    #'rsv240': rsv(240),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "```\n",
    "https://www.twblogs.net/a/5d3f3173bd9eee517422735f\n",
    "W-WED\n",
    "https://docs.python.org/zh-tw/3/library/calendar.html\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加入其他features\n",
    "* http://finlabcourse.imotor.com/viewthread.php?tid=660&extra=page%3D1\n",
    "\n",
    "```python\n",
    "from finlab import ml\n",
    "from finlab.data import Data\n",
    "\n",
    "data = Data()\n",
    "rsi = data.talib(\"RSI\")\n",
    "\n",
    "dataset = ml.fundamental_features()\n",
    "ml.add_feature(dataset, 'RSI', rsi)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 組合dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確認各指標清單"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t1 = data.talib(\"NATR\",timeperiod=14)\n",
    "#t1.to_csv('myfile.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 製作dataset\n",
    "\n",
    "##### 設定買賣頻率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2005-02-15', '2005-03-15', '2005-04-15', '2005-05-15',\n",
       "               '2005-06-15', '2005-07-15', '2005-08-15', '2005-09-15',\n",
       "               '2005-10-15', '2005-11-15',\n",
       "               ...\n",
       "               '2022-06-15', '2022-07-15', '2022-08-15', '2022-09-15',\n",
       "               '2022-10-15', '2022-11-15', '2022-12-15', '2023-01-15',\n",
       "               '2023-02-15', '2023-03-15'],\n",
       "              dtype='datetime64[ns]', name='date', length=218, freq=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rev.index = rev.index.tz_localize(\"Asia/Taipei\")\n",
    "every_month = rev.index\n",
    "every_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 將dataframe 組裝起來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features['bias20'].reindex(every_month, method='ffill')\n",
    "\n",
    "for name, f in features.items():\n",
    "    features[name] = f.reindex(every_month, method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, f in features.items():\n",
    "    features[name] = f.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(dataset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 組裝自己要的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finlab import ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "股本 = data.get('股本合計').reindex(close.index, method='ffill')\n",
    "市值 = 股本 * close / 10 * 1000\n",
    "\n",
    "#t1 = 股本.reindex(close.index, method='ffill')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.add_feature(dataset, '市值', 市值)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mom1</th>\n",
       "      <th>mom2</th>\n",
       "      <th>mom3</th>\n",
       "      <th>mom4</th>\n",
       "      <th>mom5</th>\n",
       "      <th>mom6</th>\n",
       "      <th>mom7</th>\n",
       "      <th>mom8</th>\n",
       "      <th>mom9</th>\n",
       "      <th>bias5</th>\n",
       "      <th>...</th>\n",
       "      <th>rsv20</th>\n",
       "      <th>rsv60</th>\n",
       "      <th>rsv120</th>\n",
       "      <th>rsv240</th>\n",
       "      <th>yoy</th>\n",
       "      <th>delta_yoy</th>\n",
       "      <th>PB</th>\n",
       "      <th>PE</th>\n",
       "      <th>市值</th>\n",
       "      <th>vol_ma5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stock_id</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0015</th>\n",
       "      <th>2005-02-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-03-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-04-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-05-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-06-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">9962</th>\n",
       "      <th>2022-11-15</th>\n",
       "      <td>0.830797</td>\n",
       "      <td>1.280940</td>\n",
       "      <td>0.641536</td>\n",
       "      <td>1.694666</td>\n",
       "      <td>0.917472</td>\n",
       "      <td>0.678559</td>\n",
       "      <td>1.435387</td>\n",
       "      <td>1.209077</td>\n",
       "      <td>0.859901</td>\n",
       "      <td>1.024242</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.933962</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.003516</td>\n",
       "      <td>-0.722271</td>\n",
       "      <td>1.33</td>\n",
       "      <td>7.01</td>\n",
       "      <td>1.524723e+09</td>\n",
       "      <td>462.1834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-15</th>\n",
       "      <td>1.163303</td>\n",
       "      <td>0.830797</td>\n",
       "      <td>1.280940</td>\n",
       "      <td>0.641536</td>\n",
       "      <td>1.694666</td>\n",
       "      <td>0.917472</td>\n",
       "      <td>0.678559</td>\n",
       "      <td>1.435387</td>\n",
       "      <td>1.209077</td>\n",
       "      <td>1.024939</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.98750</td>\n",
       "      <td>0.989899</td>\n",
       "      <td>0.435556</td>\n",
       "      <td>0.129923</td>\n",
       "      <td>0.126406</td>\n",
       "      <td>1.32</td>\n",
       "      <td>6.99</td>\n",
       "      <td>1.520212e+09</td>\n",
       "      <td>632.1574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-15</th>\n",
       "      <td>1.004024</td>\n",
       "      <td>1.163303</td>\n",
       "      <td>0.830797</td>\n",
       "      <td>1.280940</td>\n",
       "      <td>0.641536</td>\n",
       "      <td>1.694666</td>\n",
       "      <td>0.917472</td>\n",
       "      <td>0.678559</td>\n",
       "      <td>1.435387</td>\n",
       "      <td>1.004331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.94382</td>\n",
       "      <td>0.957627</td>\n",
       "      <td>0.586667</td>\n",
       "      <td>-0.084119</td>\n",
       "      <td>-0.214041</td>\n",
       "      <td>1.46</td>\n",
       "      <td>7.70</td>\n",
       "      <td>1.673587e+09</td>\n",
       "      <td>3007.9848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-15</th>\n",
       "      <td>1.249495</td>\n",
       "      <td>1.004024</td>\n",
       "      <td>1.163303</td>\n",
       "      <td>0.830797</td>\n",
       "      <td>1.280940</td>\n",
       "      <td>0.641536</td>\n",
       "      <td>1.694666</td>\n",
       "      <td>0.917472</td>\n",
       "      <td>0.678559</td>\n",
       "      <td>1.005587</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.697778</td>\n",
       "      <td>0.568746</td>\n",
       "      <td>0.652864</td>\n",
       "      <td>1.55</td>\n",
       "      <td>8.22</td>\n",
       "      <td>1.786362e+09</td>\n",
       "      <td>932.8926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-15</th>\n",
       "      <td>0.729045</td>\n",
       "      <td>1.249495</td>\n",
       "      <td>1.004024</td>\n",
       "      <td>1.163303</td>\n",
       "      <td>0.830797</td>\n",
       "      <td>1.280940</td>\n",
       "      <td>0.641536</td>\n",
       "      <td>1.694666</td>\n",
       "      <td>0.917472</td>\n",
       "      <td>0.991399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.93750</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>0.884444</td>\n",
       "      <td>0.330021</td>\n",
       "      <td>-0.238725</td>\n",
       "      <td>1.72</td>\n",
       "      <td>9.09</td>\n",
       "      <td>1.975825e+09</td>\n",
       "      <td>1083.8936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>447990 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         mom1      mom2      mom3      mom4      mom5  \\\n",
       "stock_id date                                                           \n",
       "0015     2005-02-15       NaN       NaN       NaN       NaN       NaN   \n",
       "         2005-03-15       NaN       NaN       NaN       NaN       NaN   \n",
       "         2005-04-15       NaN       NaN       NaN       NaN       NaN   \n",
       "         2005-05-15       NaN       NaN       NaN       NaN       NaN   \n",
       "         2005-06-15       NaN       NaN       NaN       NaN       NaN   \n",
       "...                       ...       ...       ...       ...       ...   \n",
       "9962     2022-11-15  0.830797  1.280940  0.641536  1.694666  0.917472   \n",
       "         2022-12-15  1.163303  0.830797  1.280940  0.641536  1.694666   \n",
       "         2023-01-15  1.004024  1.163303  0.830797  1.280940  0.641536   \n",
       "         2023-02-15  1.249495  1.004024  1.163303  0.830797  1.280940   \n",
       "         2023-03-15  0.729045  1.249495  1.004024  1.163303  0.830797   \n",
       "\n",
       "                         mom6      mom7      mom8      mom9     bias5  ...  \\\n",
       "stock_id date                                                          ...   \n",
       "0015     2005-02-15       NaN       NaN       NaN       NaN       NaN  ...   \n",
       "         2005-03-15       NaN       NaN       NaN       NaN       NaN  ...   \n",
       "         2005-04-15       NaN       NaN       NaN       NaN       NaN  ...   \n",
       "         2005-05-15       NaN       NaN       NaN       NaN       NaN  ...   \n",
       "         2005-06-15       NaN       NaN       NaN       NaN       NaN  ...   \n",
       "...                       ...       ...       ...       ...       ...  ...   \n",
       "9962     2022-11-15  0.678559  1.435387  1.209077  0.859901  1.024242  ...   \n",
       "         2022-12-15  0.917472  0.678559  1.435387  1.209077  1.024939  ...   \n",
       "         2023-01-15  1.694666  0.917472  0.678559  1.435387  1.004331  ...   \n",
       "         2023-02-15  0.641536  1.694666  0.917472  0.678559  1.005587  ...   \n",
       "         2023-03-15  1.280940  0.641536  1.694666  0.917472  0.991399  ...   \n",
       "\n",
       "                        rsv20    rsv60    rsv120    rsv240       yoy  \\\n",
       "stock_id date                                                          \n",
       "0015     2005-02-15       NaN      NaN       NaN       NaN       NaN   \n",
       "         2005-03-15       NaN      NaN       NaN       NaN       NaN   \n",
       "         2005-04-15       NaN      NaN       NaN       NaN       NaN   \n",
       "         2005-05-15       NaN      NaN       NaN       NaN       NaN   \n",
       "         2005-06-15       NaN      NaN       NaN       NaN       NaN   \n",
       "...                       ...      ...       ...       ...       ...   \n",
       "9962     2022-11-15  1.000000  1.00000  0.933962  0.440000  0.003516   \n",
       "         2022-12-15  1.000000  0.98750  0.989899  0.435556  0.129923   \n",
       "         2023-01-15  0.895833  0.94382  0.957627  0.586667 -0.084119   \n",
       "         2023-02-15  1.000000  1.00000  1.000000  0.697778  0.568746   \n",
       "         2023-03-15  0.846154  0.93750  0.957447  0.884444  0.330021   \n",
       "\n",
       "                     delta_yoy    PB    PE            市值    vol_ma5  \n",
       "stock_id date                                                        \n",
       "0015     2005-02-15        NaN   NaN   NaN           NaN        NaN  \n",
       "         2005-03-15        NaN   NaN   NaN           NaN        NaN  \n",
       "         2005-04-15        NaN   NaN   NaN           NaN        NaN  \n",
       "         2005-05-15        NaN   NaN   NaN           NaN        NaN  \n",
       "         2005-06-15        NaN   NaN   NaN           NaN        NaN  \n",
       "...                        ...   ...   ...           ...        ...  \n",
       "9962     2022-11-15  -0.722271  1.33  7.01  1.524723e+09   462.1834  \n",
       "         2022-12-15   0.126406  1.32  6.99  1.520212e+09   632.1574  \n",
       "         2023-01-15  -0.214041  1.46  7.70  1.673587e+09  3007.9848  \n",
       "         2023-02-15   0.652864  1.55  8.22  1.786362e+09   932.8926  \n",
       "         2023-03-15  -0.238725  1.72  9.09  1.975825e+09  1083.8936  \n",
       "\n",
       "[447990 rows x 33 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml.add_feature(dataset, 'vol_ma5', vol_ma5)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################　　　自己加入的　　　##############################################\n",
    "dataset.index = dataset.index.set_names(['stock_id','date'], level=[0,1])\n",
    "\n",
    "\n",
    "#dataset.index.levels[1].name = 'date'\n",
    "#dataset.index.levels[0].name = 'stock_id'\n",
    "\n",
    "#因為你pandas更新到新版了\n",
    "## profit.index.levels[0].name = 'year'\n",
    "## profit.index.levels[1].name = 'month'\n",
    "#這兩行的語法被棄用，請改成\n",
    "#profit.index=profit.index.set_names('year', level=0)\n",
    "#profit.index=profit.index.set_names('month', level=1)\n",
    "#or profit.index=profit.index.set_names(['year','month'], level=[0,1])\n",
    "#直接一行\n",
    "#就可以了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#list(dataset_fundamental.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataset_fundamental.reindex(dataset.index).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data[組合](https://hahow.in/courses/5b9d3a6dca498a001e917383/discussions/5d18b63eac23d80020ae4ce7)\n",
    "```python\n",
    "new_df = pd.concat([dataset_fundamental['R402_營業毛利成長率'],dataset],axis=1).dropna(how='any')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df = pd.concat([dataset_fundamental,dataset],axis=1).dropna(how='all').fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df = pd.concat([dataset_fundamental,dataset],axis=1).dropna(how='any')\n",
    "#dataset1 = new_df.fillna(method='ffill')#[(new_df.index.get_level_values('stock_id')=='2330')]\n",
    "##dataset = dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#feature_names=list(dataset1.columns)\n",
    "#feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 新增 label(績效/排名)\n",
    " - 定義一下要比績效還是要比排名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finlab import ml\n",
    "\n",
    "ml.add_profit_prediction(dataset)\n",
    "ml.add_rank_prediction(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profit(return) rank\n",
    "predi_target = 'rank'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 刪除太大太小的歷史資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(447990, 35)\n",
      "(387667, 35)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "\n",
    "def drop_extreme_case(dataset, feature_names, thresh=0.01):\n",
    "    \n",
    "    extreme_cases = pd.Series(False, index=dataset.index)\n",
    "    for f in feature_names:\n",
    "        tf = dataset[f]\n",
    "        extreme_cases = extreme_cases | (tf < tf.quantile(thresh)) | (tf > tf.quantile(1-thresh))\n",
    "    dataset = dataset[~extreme_cases]\n",
    "    return dataset\n",
    "\n",
    "dataset_drop_extreme_case = drop_extreme_case(dataset , feature_names , thresh=0.01)\n",
    "\n",
    "print(dataset_drop_extreme_case.shape)\n",
    "\n",
    "##(436988, 34)\n",
    "##(377968, 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dropna = dataset_drop_extreme_case.dropna(how='any')\n",
    "dataset_dropna = dataset_dropna.reset_index().set_index(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_drop_extreme_case.index.get_level_values(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################\n",
    "##############################################　　　自己加入的　　　##############################################\n",
    "##################################################################################################################\n",
    "\n",
    "dataset_dropna.index = pd.to_datetime(dataset_dropna.index)\n",
    "dataset_dropna = dataset_dropna.sort_index()\n",
    "\n",
    "#修復＜class ‘numpy.ndarray‘＞　https://blog.csdn.net/lxbin/article/details/114005757"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset_dropna.loc[:'2015']\n",
    "dataset_test = dataset_dropna.loc['2016':]\n",
    "\n",
    "#date_arr = dataset.index.get_level_values('date') < '2020'\n",
    "#dataset_train = dataset[date_arr]\n",
    "#dataset_test = dataset[~date_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset_train[feature_names] , dataset_train['return'] > 1\n",
    "test = dataset_test[feature_names] , dataset_test['return'] > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 機器學習\n",
    " - 目前只有三個，技術指標也要再增加一下feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_names = feature_names1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2013-05-15', '2013-05-15', '2013-05-15', '2013-05-15',\n",
       "               '2013-05-15', '2013-05-15', '2013-05-15', '2013-05-15',\n",
       "               '2013-05-15', '2013-05-15',\n",
       "               ...\n",
       "               '2015-12-15', '2015-12-15', '2015-12-15', '2015-12-15',\n",
       "               '2015-12-15', '2015-12-15', '2015-12-15', '2015-12-15',\n",
       "               '2015-12-15', '2015-12-15'],\n",
       "              dtype='datetime64[ns]', name='date', length=26431, freq=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Dense(100, activation='relu',\n",
    "                      input_shape=(len(feature_names),),\n",
    "                      kernel_initializer=initializers.he_normal(seed=0)))\n",
    "model.add(layers.Dense(100, activation='relu',\n",
    "                      kernel_initializer=initializers.he_normal(seed=0)))\n",
    "model.add(layers.Dropout(0.7))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=\"adam\",)\n",
    "\n",
    "print('start fitting')\n",
    "history = model.fit(dataset_train[feature_names], dataset_train[predi_target],\n",
    "                    batch_size=1000,         #1000  #每一个batch的大小\n",
    "                    epochs=1000, #225          #迭代次数\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    #validation_data =        #(测试集的输入特征，测试集的标签），\n",
    "                    #validation_split =       # 从测试集中划分多少比例给训练集，\n",
    "                    #validation_freq = 20        #测试的epoch间隔数                     \n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(history.history['val_loss'][1:])\n",
    "plt.plot(history.history['loss'][1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lightgbm Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##############################################　　　自己加入的　　　##############################################\n",
    "import lightgbm as lgb\n",
    "\n",
    "cf = lgb.LGBMRegressor(n_estimators=5000)\n",
    "\n",
    "print('---cf.fit---')\n",
    "print(cf.fit(*train))\n",
    "print('---cf.score---')\n",
    "print(cf.score(*test))\n",
    "print('---predict---')\n",
    "print(cf.predict(test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 參數優化_1110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import lightgbm\n",
    "\n",
    "fit_params={\"early_stopping_rounds\":30, \n",
    "            \"eval_metric\" : 'auc', \n",
    "            \"eval_set\" : [test],\n",
    "            'eval_names': ['valid'],\n",
    "            'verbose': 100,\n",
    "            'categorical_feature': 'auto'}\n",
    "\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "param_test ={'num_leaves': sp_randint(6, 50), \n",
    "             'min_child_samples': sp_randint(100, 500), \n",
    "             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "             'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
    "             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
    "             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n",
    "\n",
    "#This parameter defines the number of HP points to be tested\n",
    "n_HP_points_to_test = 100\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "#n_estimators is set to a \"large value\". The actual number of trees build will depend on early stopping and 5000 define only the absolute maximum\n",
    "clf = lgb.LGBMClassifier(max_depth=-1, random_state=314, silent=True, metric='None', n_jobs=4, n_estimators=5000)\n",
    "gs = RandomizedSearchCV(\n",
    "    estimator=clf, param_distributions=param_test, \n",
    "    n_iter=n_HP_points_to_test,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    "    refit=True,\n",
    "    random_state=314,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "gs.fit(*train, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#gs.best_estimator_\n",
    "#LGBMClassifier(colsample_bytree=0.6433117836032942, metric='None',\n",
    "#               min_child_samples=224, min_child_weight=1e-05, n_estimators=5000,\n",
    "#               n_jobs=4, num_leaves=20, random_state=314, reg_alpha=10,\n",
    "#               reg_lambda=10, subsample=0.8945613420997809)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cf = lgb.LGBMRegressor(colsample_bytree=0.7756038066515227, metric='None',\n",
    "               min_child_samples=424, min_child_weight=10000.0,\n",
    "               n_estimators=5000, n_jobs=4, num_leaves=30, random_state=314,\n",
    "               reg_alpha=0, reg_lambda=100, subsample=0.9348658101450167)\n",
    "\n",
    "cf.fit(dataset_train[feature_names],dataset_train['return'] > 1, **fit_params)\n",
    "cf.score(dataset_test[feature_names],dataset_test['return'] > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.DataFrame(zip(cf.feature_importances_, feature_names), \n",
    "                           columns=['Value','Feature']).sort_values('Value', ascending=False)\n",
    "feature_imp\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Training until validation scores don't improve for 30 rounds.\n",
    "[100]\tvalid's auc: 0.551389\n",
    "Early stopping, best iteration is:\n",
    "[89]\tvalid's auc: 0.553699\n",
    "0.003857956228475623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(cf.fit(*train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import lightgbm as lgb\n",
    "#gbm = lgb.LGBMClassifier(n_estimators=100, random_state=5, learning_rate=0.01)\n",
    "#gbm.fit(dataset_train[feature_names], dataset_train['return'] > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tqdm\n",
    "#\n",
    "#n = 3\n",
    "#\n",
    "#X = []\n",
    "#y = []\n",
    "#indexes = []\n",
    "#dataset_scaled_x = dataset_scaled[feature_names]\n",
    "#\n",
    "#for i in tqdm.tqdm_notebook(range(0, len(dataset_scaled)-n)):\n",
    "#    X.append(dataset_scaled_x.iloc[i:i+n].values)\n",
    "#    y.append(dataset_scaled['return'].iloc[i+n-1])\n",
    "#    indexes.append(dataset_scaled.index[i+n-1])\n",
    "##dataset_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#X = np.array(X)\n",
    "#y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import lightgbm as lgb\n",
    "#cf = lgb.LGBMRegressor(colsample_bytree=0.7740467183023685, metric='None',\n",
    "#               min_child_samples=395, min_child_weight=0.01, n_estimators=5000,\n",
    "#               n_jobs=4, num_leaves=9, random_state=314, reg_alpha=5,\n",
    "#               reg_lambda=10, subsample=0.4643892520208455)\n",
    "#    \n",
    "#cf.fit(dataset_train[feature_names].astype(float), dataset_train['rank'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "cf2 = RandomForestRegressor(n_estimators=100)\n",
    "cf2.fit(dataset_train[feature_names].astype(float), dataset_train[predi_target])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 參數優化_1110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy.stats import randint as sp_randint \n",
    "#from sklearn.model_selection import RandomizedSearchCV \n",
    "## build a classifier \n",
    "#clf = RandomForestRegressor(n_estimators=100) \n",
    "## specify parameters and distributions to sample from \n",
    "#param_dist = {\"max_depth\": [3, None], \n",
    "#              \"max_features\": sp_randint(1, 11), \n",
    "#              \"min_samples_split\": sp_randint(2, 11), \n",
    "#              \"min_samples_leaf\": sp_randint(1, 11), \n",
    "#              \"bootstrap\": [True, False], \n",
    "#              \"criterion\": [\"mse\", \"mae\"]} \n",
    "## run randomized search \n",
    "#n_iter_search = 20 \n",
    "#rs = RandomizedSearchCV(clf, param_distributions=param_dist, \n",
    "#                                   n_iter=n_iter_search) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rs.fit(dataset_train[features], dataset_train['return'] > 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split Train Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.DataFrame(zip(cf2.feature_importances_, feature_names), \n",
    "                           columns=['Value','Feature']).sort_values('Value', ascending=False)\n",
    "feature_imp\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = dataset.index.get_level_values('date') < '2021'\n",
    "dataset_train = dataset[select]\n",
    "dataset_test = dataset[~select]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_drop = dataset.dropna(subset=feature_names+['return'])\n",
    "\n",
    "vals = model.predict(dataset_drop[feature_names].astype(float))\n",
    "dataset_drop['result1'] = pd.Series(vals.swapaxes(0,1)[0], dataset_drop.index)\n",
    "\n",
    "vals = cf.predict(dataset_drop[feature_names].astype(float))\n",
    "dataset_drop['result2'] = pd.Series(vals, dataset_drop.index)\n",
    "\n",
    "vals = cf2.predict(dataset_drop[feature_names].astype(float))\n",
    "dataset_drop['result3'] = pd.Series(vals, dataset_drop.index)\n",
    "\n",
    "dataset_drop = dataset_drop.reset_index().set_index(\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 把量加進來做篩選\n",
    " * https://hahow.in/courses/5b9d3a6dca498a001e917383/shapeussions/60c96f5b018697e8a6131cbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把量加進來\n",
    "vol=data.get('成交股數')/1000\n",
    "vol_ma5=vol.rolling(5).mean()\n",
    "\n",
    "vol_filter=vol_ma5>1000\n",
    "vol_filter=vol_filter[vol_filter].fillna(0)\n",
    "#vol_filter\n",
    "t1 = vol_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_filter = t1.reindex(every_month, method='ffill')#.loc['2010-02-15']\n",
    "#vol_filter.loc['2010-02-15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#t1 = vol_ma5.iloc[-1].dropna()\n",
    "#t1.to_csv('./tmp/132.csv')\n",
    "#t1.hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "dates = sorted(list(set(dataset_drop.index)))\n",
    "\n",
    "rs = []\n",
    "for d in dates:\n",
    "    #print(d)\n",
    "    dataset_time = dataset_drop.loc[d]\n",
    "    \n",
    "    dataset_time = drop_extreme_case(dataset_time , feature_names , thresh=0.01)\n",
    "    \n",
    "    #print(dataset_time)\n",
    "    \n",
    "    predi_target = dataset_time['result1'] + dataset_time['result2'] + dataset_time['result3'] \n",
    "            ###\n",
    "    predi_target = predi_target * (dataset_time['vol_ma5'] >vol).astype(float)\n",
    "            ###\n",
    "\n",
    "    condition = (predi_target >= predi_target.nlargest(20).iloc[-1]) \n",
    "    \n",
    "    #print(vol_filter.loc[d])\n",
    "    #print(condition)\n",
    "    \n",
    "    r = dataset_time['return'][condition].mean()\n",
    "\n",
    "    rs.append(r * (1-3/1000-1.425/1000*2*0.6) )#+ 0.05)\n",
    "\n",
    "rs = pd.Series(rs, index=dates)['2016':].cumprod()\n",
    "\n",
    "s0050 = close['0050']['2016':]\n",
    "\n",
    "pd.DataFrame({'nn strategy return':rs.reindex(s0050.index, method='ffill'), '0050 return':s0050/s0050[0]}).plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#\n",
    "#return_history_1026 = pd.Series(rs, index=dates)['2021':].cumprod()\n",
    "##eq = (gain[hold == 1].mean(axis=1)).fillna(1).cumprod()\n",
    "#\n",
    "#pickle.dump(rs, open('return_history_1026.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyfolio as pf\n",
    "#import pandas as pd\n",
    "#\n",
    "#close.index = close.index.tz_localize(\"Asia/Taipei\")\n",
    "##pf.create_returns_tear_sheet(close['0050'].pct_change())\n",
    "#\n",
    "## 得到 上一個單元的 回測結果\n",
    "#ret = pickle.load(open(\"return_history_1026.pkl\", \"rb\"))\n",
    "#\n",
    "## 將回測報酬率取出來\n",
    "#ret = ret.pct_change().dropna()\n",
    "#ret.index = pd.to_datetime(ret.index).tz_localize('Asia/Taipei')\n",
    "#\n",
    "## 利用pyfolio 比較報酬率\n",
    "#pf.create_returns_tear_sheet(ret, benchmark_rets=close['0050'].pct_change())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 當月持股狀況"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the latest dataset\n",
    "last_date = dataset.index.levels[1].max()#\"2022-10-15\"\n",
    "is_last_date = dataset.index.get_level_values('date') == last_date\n",
    "last_dataset = dataset[is_last_date].copy()\n",
    "\n",
    "\n",
    "last_dataset = drop_extreme_case(last_dataset, feature_names , thresh=0.01)\n",
    "t1 = last_dataset\n",
    "\n",
    "# remove NaN testcases\n",
    "last_dataset = last_dataset.dropna(subset=feature_names)\n",
    "\n",
    "# predict\n",
    "\n",
    "vals = model.predict(last_dataset[feature_names].astype(float))\n",
    "last_dataset['result1'] = pd.Series(vals.swapaxes(0,1)[0], last_dataset.index)\n",
    "\n",
    "vals = cf.predict(last_dataset[feature_names].astype(float))\n",
    "last_dataset['result2'] = pd.Series(vals, last_dataset.index)\n",
    "\n",
    "vals = cf2.predict(last_dataset[feature_names].astype(float))\n",
    "last_dataset['result3'] = pd.Series(vals, last_dataset.index)\n",
    "\n",
    "\n",
    "# calculate score\n",
    "\n",
    "predi_target = last_dataset['result1'] + last_dataset['result2'] + last_dataset['result3']\n",
    "\n",
    "#\n",
    "##predi_target = predi_target * vol_filter.iloc[-1] #******加上量的濾網\n",
    "predi_target = predi_target * (last_dataset['vol_ma5'] >vol).astype(float)\n",
    "\n",
    "\n",
    "\n",
    "condition = (predi_target >= predi_target.nlargest(20).iloc[-1])\n",
    "#vol_filter\n",
    "\n",
    "# plot rank distribution\n",
    "predi_target[predi_target!=0].hist(bins=20)\n",
    "\n",
    "\n",
    "# show the best 20 stocks\n",
    "slist1 = predi_target[condition].reset_index()['stock_id']\n",
    "\n",
    "#https://keras-cn.readthedocs.io/en/latest/models/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date = dataset.index.levels[1].max()#\"2022-10-15\"\n",
    "is_last_date = dataset.index.get_level_values('date') == last_date\n",
    "last_dataset = dataset[is_last_date].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank.sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 平均分配資產於股票之中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close = data.get(\"收盤價\")\n",
    "\n",
    "money = 60000\n",
    "stock_prices = close[slist1].iloc[-1]\n",
    "\n",
    "\n",
    "print(\"股票平分張數:\")\n",
    "money / len(stock_prices) / stock_prices / 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "[移動窗格最佳化](https://hahow.in/courses/5b9d3a6dca498a001e917383/discussions/61b4c90147843d0006cf2593)\n",
    "***************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def select(df):\n",
    "\n",
    "    rank = df['pre']\n",
    "\n",
    "    condition1 = (rank >= rank.nlargest(1).iloc[-1])\n",
    "\n",
    "    return df['return'][condition1].mean() * (1-3/1000-1.425/1000*2*0.6)\n",
    "\n",
    "end = 5\n",
    "\n",
    "cf = lgb.LGBMRegressor(n_estimators=500)\n",
    "\n",
    "\n",
    "\n",
    "train_time = ['2015','2016','2017','2018','2019']\n",
    "\n",
    "s_time = ['2007','2008','2009','2010','2011']\n",
    "\n",
    "test_time = ['2016','2017','2018','2019','2020']\n",
    "\n",
    "#dataset_copy = dataset_dropna.copy()\n",
    "\n",
    "store_mse = []\n",
    "\n",
    "\n",
    "for time in range(end):\n",
    "\n",
    "    print('%d 次執行中'%(time))\n",
    "\n",
    "    dataset_dropna2_train = dataset_copy.loc[s_time[time]:train_time[time]] #2007~ 2015   2008~2016   2009~2017  2010~2018  ....\n",
    "\n",
    "    dataset_dropna2_test = dataset_copy.loc[test_time[time]:test_time[time]]#            2016                2017               2018              2019  .....\n",
    "\n",
    "    \n",
    " cf.fit(dataset_dropna2_train[feature_names].astype(float), dataset_dropna2_train['rank'])\n",
    "\n",
    "    \n",
    " predict = cf.predict(dataset_dropna2_test[feature_names])\n",
    "\n",
    "    dataset_dropna2_test['pre'] = predict\n",
    "\n",
    "dates = dataset_dropna2_test.index.get_level_values('date')\n",
    "\n",
    "b = dataset_dropna2_test.groupby(dates).apply(select).cumprod()\n",
    "\n",
    "s0050 = close['0050'][test_time[time]:test_time[time]]\n",
    "\n",
    "s0056 = close['0056'][test_time[time]:test_time[time]]\n",
    "\n",
    "pd.DataFrame({'Best 1 stocks return(include handling fee)':b.reindex(s0050.index, method='ffill'), \n",
    "\n",
    "              '0050':s0050/s0050[0],'0056':s0056/s0056[0]}).plot()\n",
    "\n",
    "plt.ylabel('return')"
   ]
  }
 ],
 "metadata": {
  "@deathbeds/ipydrawio": {
   "xml": "<mxfile host=\"17-0536659-02\" modified=\"2022-10-27T03:01:05.740Z\" agent=\"5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\" etag=\"8bODyUWCdQaexky56D9k\" version=\"20.2.8\" type=\"embed\"><diagram id=\"9nsO6hMlLNMTvIbReD-d\" name=\"第1頁\"><mxGraphModel dx=\"1458\" dy=\"721\" grid=\"1\" gridSize=\"10\" guides=\"1\" tooltips=\"1\" connect=\"1\" arrows=\"1\" fold=\"1\" page=\"1\" pageScale=\"1\" pageWidth=\"827\" pageHeight=\"1169\" math=\"0\" shadow=\"0\"><root><mxCell id=\"0\"/><mxCell id=\"1\" parent=\"0\"/><UserObject label=\"Tree Root\" treeRoot=\"1\" id=\"2\"><mxCell style=\"align=center;collapsible=0;container=1;recursiveResize=0;\" parent=\"1\" vertex=\"1\"><mxGeometry x=\"40\" y=\"40\" width=\"120\" height=\"60\" as=\"geometry\"/></mxCell></UserObject></root></mxGraphModel></diagram></mxfile>"
  },
  "kernelspec": {
   "display_name": "finlab",
   "language": "python",
   "name": "finlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
